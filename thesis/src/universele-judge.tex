\chapter{De universele judge}\label{ch:de-universele-judge}

\lettrine{H}{et antwoord} op de onderzoeksvraag uit het vorige hoofdstuk neemt de vorm van een nieuwe judge voor het Dodona-platform aan, de \term{universele judge}, die oplossingen voor een opgave in meerdere programmeertalen kan evalueren.
Dit hoofdstuk licht de werking en implementatie van deze judge toe, beginnend met een algemeen overzicht, waarna elk onderdeel in meer detail besproken wordt.

%TODO: terminologie uitleggen (oplossing, evaluatie, \ldots)
%Een groot deel hiervan zal waarschijnlijk uitgelegd zijn bij de werking van Dodona.
%
%Plan voor dit hoofdstuk:
%
%1. Testplan                                    OK
%1.1 Formaat testplan zelf                      OK
%1.2 Formaat serialisatie                       OK
%1.3 Formaat functie-oproep en assignment       OK
%1.4 Vereiste features                          OK
%2. Uitvoeren van een oplossing
%2.1 Genereren van de code
%2.2 Uitvoeren van de code
%3. Evalueren van een oplossing
%3.1 Ingebouwde evaluatie
%3.2 Aangepaste evaluatie
%3.3 Taalspecifieke evaluatie
%4. Resultaten verwerken


\section{Overzicht}\label{sec:overzicht}

\begin{figure}
    \begin{adjustbox}{width=\textwidth}
        \input{figures/architecture.tikz}
    \end{adjustbox}
    \caption{Schematische voorstelling van de opbouw van de universele judge.}
    \label{fig:universal-judge}
\end{figure}

\Cref{fig:universal-judge} toont de opbouw van de judge op schematische wijze.
In meer detail is het stappenplan voor het evalueren van een oefening als volgt:

\begin{enumerate}
    \item De judge wordt opgestart en het testplan wordt geladen.
    Het gestarte proces noemen we het \term{kernproces}
    \item Het testplan wordt gecontroleerd op vereiste functies, met andere woorden: ondersteunt de opgave de gewenste taal?
    Als de opgave bijvoorbeeld programmeertaalspecifieke code bevat die enkel voor Java gegeven is, zal de opgave niet in Python gemaakt kunnen worden.
    \item De code voor het evalueren van de oplossing wordt gegenereerd en gecompileerd.
    Alle code wordt in een keer gecompileerd voor het eigenlijke uitvoeren.
    Dit gebeurt in het \term{uitvoeringsproces}.
    Na deze stap is alle code beschikbaar om te evalueren, met uitzondering van aangepaste evaluatoren.
    \item Elke context uit het testplan wordt afzonderlijk uitgevoerd in een nieuw proces.
    Aangezien deze contexten onafhankelijk zijn van elkaar, worden ze in parallel uitgevoerd, indien de configuratie van de judge dit toelaat.
    \item De resultaten van een uitvoering van een context worden beoordeeld.
    Hiervoor zijn drie mogelijke manieren:
    \begin{enumerate}
        \item Programmeertaalspecifieke evaluatie.
        Hierbij wordt de evaluatie gedaan na de uitvoering, in hetzelfde proces als de uitvoering, het uitvoeringsproces.
        \item Aangepaste evaluator.
        Hierbij is er evaluatiecode geschreven die los staat van de oplossing.
        De evaluatiecode kan in een andere programmeertaal geschreven zijn dan de oplossing.
        De aangepaste evaluator wordt gegenereerd, gecompileerd en uitgevoerd na het uitvoeren van de oplossing, in een nieuw proces: het \term{evaluatieproces}.
        \item Ingebouwde evaluatie.
        Hierbij is het de judge zelf die evalueert, waardoor dit vooral eenvoudige evaluaties betreft, zoals het vergelijken van geproduceerde uitvoer en verwachte uitvoer.
        Dit gebeurt dan in het kernproces.
    \end{enumerate}
    \item Tot slot verzamelt de judge alle evaluatieresultaten en stuurt ze door naar Dodona, waarna ze getoond worden aan de gebruiker.
\end{enumerate}


\section{Beschrijven van een opgave}\label{sec:testplan}

Elke evaluatie begint met het \term{testplan}, een document dat beschrijft hoe een oplossing voor een opgave geëvalueerd moet worden.
Het vervangt de taalspecifieke testen van de bestaande judges (ie.\ de jUnit-tests of de doctests in respectievelijk Java en Python).
Het bestaat uit verschillende onderdelen, die hierna besproken worden.

\subsection{Het testplan}\label{subsec:het-testplan}

Het eigenlijke testplan beschrijft de structuur van een evaluatie van een oplossing voor een opgave.
Qua structuur lijkt dit sterk op de structuur van de feedback zoals gebruikt door Dodona.
Deze aanpak heeft als voordeel dat eenvoudiger is om een testplan op te stellen: er moet geen mentale afbeelding tussen de structuur van het testplan en dat van Dodona bijgehouden worden.

Bij de keuze voor een formaat voor het testplan (bv.\ json of xml), hebben we vooraf enkele vereisten geformuleerd waaraan het gekozen formaat moet voldoen.
Het moet:

\begin{itemize}
    \item leesbaar zijn voor mensen,
    \item geschreven kunnen worden met minimale inspanning, met andere woorden de syntaxis dient eenvoudig te zijn, en
    \item programmeertaalonafhankelijk zijn.
\end{itemize}

Uiteindelijk is gekozen om het op te stellen in json.
Niet alleen voldoet json aan de vooropgestelde voorwaarden, het wordt ook door veel talen ondersteund.

Toch zijn er ook enkele nadelen aan het gebruik van json.
Zo is json geen beknopte of compacte taal om met de hand te schrijven.
Een oplossing hiervoor gebruikt de eigenschap dat veel talen json kunnen produceren: andere programma's kunnen desgewenst het testplan in het json-formaat genereren, waardoor het niet met de hand geschreven moet worden.
Hiervoor denken we aan een \termen{DSL} (\english{domain specific language}), maar dit valt buiten de thesis en wordt verder besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Een tweede nadeel is dat json geen programmeertaal is.
Terwijl dit de implementatie van de judge bij het interpreteren van het testplan weliswaar eenvoudiger maakt, is het tevens beperkend: beslissen of een testgeval moet uitgevoerd worden op basis van het resultaat van een vorig testgeval is bij wijze als voorbeeld niet mogelijk.
Ook deze beperking wordt uitgebreider besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

De structuur van het testplan vertaalt zich in json naar een reeks json-objecten, die hieronder beschreven worden.

\begin{description}
    \item[Tab] Een testplan bestaat uit verschillende \termen{tab}s of tabbladen.
    Deze komen overeen met de tabbladen in de gebruikersinterface van Dodona.
    Een tabblad kan een naam hebben, die zichtbaar is voor de gebruikers.
    \item[Context] Elk tabblad bestaat uit een of meerdere \termen{context}en.
    Een context is een onafhankelijke uitvoering van een evaluatie.
    De nadruk ligt op de "onafhankelijkheid".
    Elke context wordt in een nieuw proces uitgevoerd, zodat er geen informatie tussen contexten kan uitgewisseld worden.
    \item[Testcase] Een context bestaat uit een of meerdere \termen{testcase}s of testgevallen.
    Een testgeval bestaat uit invoer en een aantal tests.
    Een context bevat twee soorten testgevallen:
    \begin{description}
        \item[Main testcase] of hoofdtestgeval.
        Van deze soort is er maximaal een per context.
        Dit testgeval is voor het uitvoeren van de main-functie (of de code zelf als het gaat om een scripttaal zoals Bash of Python).
        Als invoer voor dit testgeval kunnen enkel het standaardinvoerkanaal en de programma-argumenten meegegeven worden.
        \item[Normal testcase] of normaal testgeval.
        Hiervan kunnen er nul of meer zijn per context.
        Deze testgevallen zijn voor andere aspecten te testen, nadat de code van de gebruiker met success ingeladen is.
        De invoer is dan ook uitgebreider: het kan gaan om het standaardinvoerkanaal, functieoproepen en variabeletoekenningen.
        Een functieoproep of variabeletoekenning is verplicht.
    \end{description}
    \item[Test] Een testcase bestaat uit meerdere \term{test}s, die elk een aspect van een testcase controleren.
    Er zijn dus aparte tests voor het standaarduitvoerkanaal, het standaardfoutkanaal, opgevangen uitzonderingen (\english{exceptions}), de teruggegeven waarden van een functieoproep (returnwaarde) en de inhoud van een bestand.
    Elke test bevat de verwachte uitvoer om mee te vergelijken of de code om het resultaat te evalueren.
\end{description}

\subsection{Dataserialisatie}\label{subsec:dataserialisatie}

In het testplan, zoals beschreven in de paragraaf hierboven, wordt gewag gemaakt van returnwaarden.
Aangezien het testplan programmeertaalonafhankelijk is, moet er dus een manier zijn om data uit de verschillende programmeertalen voor te stellen: het \term{serialisatieformaat}.
Ook hier is een keuze voor een bepaald formaat gemaakt.
Daarvoor zijn er ook enkele voorwaarden vooropgesteld, waaraan het serialisatieformaat moet voldoen.
Het formaat moet:

\begin{itemize}
    \item door mensen geschreven kunnen worden,
    \item niet binair zijn, aangezien het een onderdeel van het json-testplan moet worden,
    \item in meerdere programmeertalen bruikbaar zijn, en
    \item de types ondersteunen die we willen aanbieden in het programmeertaalonafhankelijke deel van het testplan.
\end{itemize}

Een voor de hand liggende oplossing is ook hiervoor json gebruiken, en zelf in json een structuur op te stellen voor de waarden.
In tegenstelling tot het testplan bestaan er al een resem aan dataserialisatieformaten, waardoor het de moeite is om na te gaan of er geen bestaand formaat voldoet aan de vereisten.
Hiervoor is gestart van een overzicht op Wikipedia, zie \autocite{wiki2020}.
Uiteindelijk is niet gekozen voor een bestaand formaat, maar voor de json-oplossing.
De redenen hiervoor zijn samen te vatten als:

\begin{itemize}
    \item Het gaat om een binair formaat.
    Hoewel binaire formaten vaak beter zijn op het vak van geheugengebruik en snelheid, zijn er nadelen aan verbonden voor ons gebruik:
    \begin{itemize}
        \item Het formaat moet in elke ondersteunde taal geïmplementeerd worden, en binaire data is minder eenvoudig te implementeren dan een op tekst gebaseerd formaat.
        \item Het is niet mogelijk om het met de hand te schrijven.
        \item Het inbedden in het json-testplan is niet triviaal: waarschijnlijk is een encodering als base64 nodig.
    \end{itemize}
    \item Het formaat ondersteunt niet alle gewenste types.
    Sommige formaten hebben ondersteuning voor complexere datatypes, maar niet voor alle complexere datatypes die wij nodig hebben.
    Uiteraard kunnen de eigen types samengesteld worden uit basistypes, maar dan biedt de ondersteuning voor de complexere types weinig voordeel, aangezien er toch een eigen encodering van die complexere types opgesteld zal moeten worden.
    \item Sommige formaten zijn omslachtig in gebruik.
    Vaak ondersteunen dit soort formaten meer dan wat wij nodig hebben.
    \item Het formaat is niet eenvoudig te implementeren in een programmeertaal waarvoor geen ondersteuning is.
    Sommige dezer formaten ondersteunen weliswaar veel talen, maar we willen niet dat het serialisatieformaat een beperkende factor wordt in welke talen door de judge ondersteund worden.
\end{itemize}

Een lijst van de overwogen formaten met een korte beschrijving volgt:

\begin{description}
    \item[Apache Avro] Een volledig "systeem voor dataserialisatie".
    De specificatie van het formaat gebeurt in json, terwijl de eigenlijke data binair geëncodeerd wordt.
    Heeft uitbreidbare types, met veel ingebouwde types \autocite{avro}.
    \item[Apache Parquet] Minder relevant, dit is een bestandsformaat voor Hadoop \autocite{parquet}.
    \item[ASN.1] Staat voor \english{Abstract Syntax Notation One}, een ouder formaat uit de telecommunicatie.
    De hoofdstandaard beschrijft enkel de notatie voor een dataformaat.
    Andere standaarden beschrijven dan de serialisatie, in bv.\ binair formaat, json of xml.
    De meerdere serialisatievormen zijn in theorie aantrekkelijk: elke taal moet er slechts een ondersteunen, terwijl de judge ze allemaal kan ondersteunen.
    In de praktijk blijkt echter dat voor veel talen er slechts een serialisatieformaat is, en dat dit vaak het binaire formaat is \autocite{x680}.
    \item[Bencode] Schema gebruikt in BitTorrent.
    Het is gedeeltelijk binair, gedeeltelijk in text \autocite{cohen2017}.
    \item[Binn] Binair dataformaat \autocite{ramos2019}.
    \item[BSON] Een binaire variant op json, geschreven voor en door MongoDB \autocite{bson}.
    \item[CBOR] Een lichtjes op json gebaseerd formaat, ook binair.
    Heeft een goede standaard, ondersteunt redelijk wat talen \autocite{rfc7049}.
    \item[FlatBuffers] Lijkt op ProtocolBuffers, allebei geschreven door Google, maar verschilt wat in implementatie van ProtocolBuffers.
    De encodering is binair \autocite{flatbuffers}.
    \item[Fast Infoset] Is eigenlijk een manier om xml binair te encoderen (te beschouwen als een soort compressie voor xml), waardoor het minder geschikt voor ons gebruik wordt \autocite{x981}.
    \item[Ion] Een superset van json, ontwikkeld door Amazon.
    Het heeft zowel een tekstuele als binaire voorstelling.
    Naast de gebruikelijke json-types, bevat het enkele uitbreidingen. \autocite{ion}.
    \item[MessagePack] Nog een binair formaat dat lichtjes op json gebaseerd is.
    Lijkt qua types sterk op json.
    Heeft implementaties in veel talen \autocite{messagepack}.
    \item[OGDL] Afkorting voor \english{Ordered Graph Data Language}.
    Daar het om een serialisatieformaat voor grafen gaat, is het niet nuttig voor ons doel \autocite{ogdl}.
    \item[OPC Unified Architecture] Een protocol voor intermachinecommunicatie.
    Complex: de specificatie bevat 14 documenten, met ongeveer 1250 pagina's \autocite{tr62541}.
    \item[OpenDLL] Afkorting voor de \english{Open Data Description Language}.
    Een tekstueel formaat, bedoeld om arbitraire data voor te stellen.
    Wordt niet ondersteunt in veel programmeertalen, in vergelijking met bv.\ json \autocite{openddl}.
    \item[ProtocolBuffers] Lijkt zoals vermeld sterk op FlatBuffers, maar heeft nog extra stappen nodig bij het encoderen en decoderen, wat het minder geschikt maakt \autocite{protobuf}.
    \item[Smile] Nog een binaire variant van json \autocite{smile}.
    \item[SOAP] Afkorting voor \english{Simple Object Access Protocol}.
    Niet bedoeld als formaat voor dataserialisatie, maar voor communicatie tussen systemen over een netwerk \autocite{soap}.
    \item[SDXF] Binair formaat voor data-uitwisseling.
    Weinig talen ondersteunen dit formaat \autocite{rfc3072}.
    \item[Thrift] Lijkt sterk op ProtocolBuffers, maar geschreven door Facebook \autocite{slee2007}.
    \item[UBJSON] Nog een binaire variant van json \autocite{ubjson}.

\end{description}

Geen enkel overwogen formaat heeft grote voordelen tegenover een eigen structuur in json.
Meer zelfs, de meeste formaten hebben het nadeel dat ze geen json zijn, waardoor we een nieuwe taal moeten inbedden in het bestaande json-testplan.
Hiervoor viel de keuze uiteindelijk op json.
Zoals bij het testplan definieert het serialisatieformaat een structuur die gevolgd moet worden.
Concreet wordt een waarde voorgesteld als een json-object dat bestaat uit de (geëncodeerde) waarde en het type van die waarde.
Hieronder staat een voorbeeld van lijst van twee getallen in het serialisatieformaat.
Een formelere definitie van het formaat in json-schema is \cref{ch:specificatie-van-het-serialisatieformaat}.

\inputminted{json}{code/format.json}

Het formaat ondersteunt de meeste basistypes die in bijna elke programmeertaal beschikbaar zijn.
Hieronder volgt een korte omschrijving van de ondersteunde types:

\begin{description}
    \item[\texttt{integer}] Gehele getallen.
    \item[\texttt{rational}] Rationale getallen.
    \item[\texttt{text}] Een tekenreeks of string.
    \item[\texttt{literal}] Een tekstuele waarde die rechtstreeks in de taal zelf beschikbaar is.
    Deze waarde wordt gebruikt om het type van functie-argumenten aan te duiden als het gaat om eigen klassen (bv.\  een klasse geïmplementeerd door de student).
    \item[\texttt{unknown}] Dit type wordt gebruikt als er onbekende types zijn bij het encoderen van een waarde.
    Bij het omzetten van een waarde uit het serialisatieformaat naar een taal, worden waarden van dit type genegeerd.
    \item[\texttt{boolean}] Een Boolese waarde (of boolean).
    \item[\texttt{list}] Een wiskundige rij, wat wil zeggen dat de volgorde belangrijk is en dat dubbele elementen toegelaten zijn.
    Merk op dat sommige talen meerdere implementaties hebben voor het concept van lijst.
    Het is de implementatie vrij om te kiezen welk concept gebruikt wordt.
    Zo wordt bijvoorbeeld in de Java-implementatie \texttt{List} in plaats van \texttt{array} gebruikt, om consistent te zijn met de implementatie van \texttt{set} en \texttt{object}.
    \item[\texttt{set}] Een wiskundige verzameling, wat wil zeggen dat de volgorde niet belangrijk is en dat dubbele elementen niet toegelaten zijn.
    \item[\texttt{object}] Een wiskundige afbeelding: elk element wordt afgebeeld op een ander element.
    In Java is dit bijvoorbeeld een \texttt{Map}, in Python een \texttt{dict} en in Javascript een \texttt{object}.
    \item[\texttt{nothing}] Geeft aan dat er geen waarde is, ook wel \texttt{null}, \texttt{None} of \texttt{nil} genoemd.
    \item[\texttt{instance}] Duidt aan dat een waarde van een aangepast type is.
    Dit wordt enkel gebruikt bij de toekenning van variabelen.
\end{description}

Het serialisatieformaat bestaat eigenlijk uit twee delen: een deel om het type van een waarde aan te geven en een tweede deel om een waarde te encoderen als een type.
Zoals duidelijk is uit de beschrijving hierboven, is het deel om types aan te geven uitgebreider dan het deel om waarden te encoderen.
Zo is het niet mogelijk waarden te encoderen met types \texttt{literal} of \texttt{instance}.
Deze types worden gebruikt bij het aangeven van het type van argumenten bij functieoproepen of het type van een variabele waaraan een waarde wordt toegekend.

\subsection{Functieoproepen en assignments}\label{subsec:functieoproepen}

Een ander onderdeel van het testplan verdient ook speciale aandacht: het toekennen van variabelen (\english{assignment}) en de functieoproepen.

In heel wat oefeningen, en zeker in objectgerichte programmeertalen, is het toekennen van een waarde aan een variabele om deze later te gebruiken onmisbaar.
Bijvoorbeeld zou een opgave kunnen bestaan uit het implementeren van een klasse.
Bij de evaluatie dient dan een instantie van die klasse aangemaakt te worden, waarna er methoden kunnen aangeroepen worden, zoals hieronder geïllustreerd in een fictief voorbeeld.

\inputminted{java}{code/assignment.jshell}

Concreet is ervoor gekozen om het testplan niet uit te breiden met generieke statements of expressions, maar de ondersteuning te beperken tot assignments en functieoproepen.
Dit om de implementatie van de vertaling van het testplan naar de ondersteunde programmeertalen niet nodeloos ingewikkeld te maken.
Een functieoproep ziet er als volgt uit:

\inputminted{json}{code/function.json}

Het type van de functie geeft aan welk soort functie het is.
Mogelijke waarden zijn momenteel \texttt{top}, \texttt{object}, \texttt{constructor} en \texttt{identity}.
De laatste soort is een speciaal geval, waarbij geen functienaam moet gegeven worden en exact één argument toegelaten is.
Die functie zal dan dat ene argument teruggeven.
De naam van de functie benoemt eenvoudig welke functie opgeroepen moet worden.
Het object van de functie laat toe om functies op objecten op te roepen.
De lijst van argumenten kan nul of meer waarden bevatten.
Deze waarden moeten in het formaat zijn zoals aangegeven in het vorige deel over de serialisatie van waarden.
Het is niet mogelijk om een functie-oproep als argument mee te geven.
Als dit nodig is, zal moeten gewerkt worden met een assignment als tussenstap.

Aan het resultaat van een functieoproep kan een naam gegeven worden, wat ons bij een assignment brengt:

\inputminted{json}{code/assignment.json}

De \texttt{name} is de naam die aan de variabele gegeven zal worden.
Het veldje \texttt{expression} moet een object zijn dat een functieoproep voorstelt (zie hierna).
Ook is er de mogelijk om een optioneel type mee te geven;
in eenvoudige gevallen kan de judge dit afleiden, maar bij complexere gevallen niet meer.
Dit type moet een van de ondersteunde types zijn uit het serialisatieformaat.

Een gecombineerd voorbeeld staat hieronder.
Hier wordt de string \texttt{'Dodona'} toegekend aan een variabele met naam \texttt{name}.
De judge kan het type afleiden, dus we moeten niet opgeven dat \texttt{name} een \texttt{str} is.

\inputminted{json}{code/assign-variable.json}

\subsection{Vereiste functies}\label{subsec:vereiste-functies}

Voor elk onderdeel van het testplan wordt afgeleid welke functies een taal moet ondersteunen om van het testplan gebruik te kunnen maken.
Bevat het testplan bijvoorbeeld waarden met als type \texttt{set}, dan kunnen enkel programmeertalen die een verzameling ondersteunen gebruikt worden.
Dat zijn bijvoorbeeld Python en Java, maar geen Bash.
Het testplan is zo opgebouwd dat het afleiden van de vereiste functies geen tussenkomst van de persoon die het testplan opstelt vereist.

\section{Uitvoeren van de oplossing}\label{sec:uitvoeren-van-de-oplossing}

Nadat de student een oplossing heeft ingediend en de judge is opgestart, begint de evaluatie van de oplossing.
Eerst wordt de uit te voeren code gegenereerd, waarna de uitvoering van die code volgt.

\subsection{Genereren van code}\label{subsec:genereren-van-code}

Het genereren van de code gebeurt met een sjabloonsysteem, genaamd Mako \autocite{mako}.
Dit sjabloonsysteem wordt traditioneel gebruikt bij webapplicaties (zoals Ruby on Rails met \textsc{erb}, Phoenix met \textsc{eex}, Laravel met Blade, enz.) om een html-pagina te genereren.
In ons geval zijn de sjablonen verantwoordelijk voor de vertaling van programmeertaalonafhankelijke concepten naar implementaties in specifieke talen.
Voorbeelden hiervan zijn functieoproepen, toekennen van variabelen, enz.
Ook zijn de sjablonen verantwoordelijk voor het genereren van de code die de oplossing van de student zal oproepen en evalueren.

Het aantal sjablonen en hoe ze geïmplementeerd worden is redelijk vrij.
De judge vereist wel enkele standaardsjablonen, waaraan vastgelegde parameters meegegeven worden.
Deze sjablonen stellen elk een aparte functie voor, die de judge nodig heeft.
De verplichte sjablonen zijn:
\begin{description}
    \item[\texttt{assignment}] Vertaalt een assignment uit het testplan naar code.
    \item[\texttt{contexts}] Het centrale sjabloon, genereert de code nodig om alle contexten uit een testplan uit te voeren.
    Om performantieredenen (zie ook~\cref{ch:beperkingen-en-toekomstig-werk}) wordt de code van alle contexten uit een testplan in een bestand genereert.
    Aan de hand van een parameter bij uitvoeren (een getal dat de context aangeeft), wordt bij het uitvoeren de code voor de juiste context gekozen.
    \item[\texttt{evaluator\_executor}] Genereert code om een aangepaste evaluator te starten.
    \item[\texttt{evaluators}] Genereert de evaluatiecode voor alle testgevallen.
    \item[\texttt{function}] Vertaalt een functie-oproep naar code.
    \item[\texttt{value}] Vertaalt een waarde uit het serialisatieformaat naar code.
\end{description}

Daarnaast moet het encoderen naar serialisatieformaat ook geïmplementeerd worden in elke taal.
Veel talen hebben dus nog enkele bijkomende bestanden met code.

\subsection{Uitvoeren van de code}\label{subsec:uitvoeren-van-de-code}

Nadat alle code voorhanden is, wordt deze per context uit het testplan uitgevoerd en wordt de uitvoer verzameld.
Het uitvoeren zelf gebeurt op de normale manier dat de programmeertaal uitgevoerd wordt via de commandoregel.
Dit is een voordeel van onze aanpak: er is geen verschil tussen hoe de judge de code van de student uitvoert en hoe de student zijn code zelf uitvoert op zijn eigen computer.
Dit voorkomt dat er subtiele verschillen zouden insluipen.


\section{Evalueren van een oplossing}\label{sec:evalueren-van-een-oplossing2}

Na de uitvoering van elke context heeft de judge alle relevant uitvoer verzamelt, zoals de standaardkanalen.
Deze uitvoer moet vervolgens beoordeeld worden om na te gaan in hoeverre deze uitvoer voldoet aan de verwachte uitvoer.
