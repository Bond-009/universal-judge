\chapter{TESTed}\label{ch:de-universele-judge}

\lettrine{I}{n het kader} van deze masterproef werd een prototype geïmplementeerd van een judge voor Dodona.
Het doel hiervan is een antwoord te bieden aan de onderzoeksvraag uit het vorige hoofdstuk en de beperkingen van deze aanpak in kaart te brengen.
Deze judge heeft de naam \term{TESTed} gekregen.
Bij TESTed is een oefening programmeertaalonafhankelijk en kunnen oplossingen in verschillende programmeertalen beoordeeld worden aan de hand van een en dezelfde specificatie.
Dit hoofdstuk begint met het ontwerp van de judge toe te lichten, waarna elk onderdeel in meer detail besproken wordt.
Tot slot wordt afgesloten met een samenvattend overzicht.

\section{Overzicht}\label{sec:ontwerp}

\subsection{Architecturaal ontwerp}\label{subsec:architecturaal-overzicht}

\Cref{fig:universal-judge} toont het architecturaal ontwerp van TESTed.
De twee stippellijnen geven programmeertaalbarrières aan, en verdelen TESTed in drie logisch omgevingen:

\begin{enumerate}
    \item TESTed zelf is geschreven in Python: in het middelste deel staat de programmeertaal dus vast.
    Dit onderdeel is verantwoordelijk voor de regie van de beoordeling op basis van het testplan.
    \item De ingediende oplossing wordt uitgevoerd in de \term{uitvoeringsomgeving}, waar de programmeertaal overeenkomt met de programmeertaal van de oplossing.
    \item Tot slot is er nog de \term{evaluatieomgeving}, waar door de lesgever geschreven beoordelingscode wordt uitgevoerd.
    Deze moet niet in dezelfde programmeertaal als de oplossing of TESTed geschreven zijn.
\end{enumerate}

\subsection{Stappenplan bij een beoordeling}\label{subsec:stappenplan-van-een-beoordeling}

De rest van het hoofdstuk bespreekt alle onderdelen van en stappen die gebeuren bij een beoordeling van een ingediende oplossing in detail.
In \cref{fig:tested-flow} zijn deze stappen gegeven als een flowchart, en een uitgeschreven versie volgt:

\begin{enumerate}
    \item De Docker-container voor TESTed wordt gestart.
    Dodona stelt de invoer ter beschikking aan de container: het testplan komt uit de oefening, terwijl de ingediende oplossing en de configuratie uit Dodona komen.
    \item Als eerste stap wordt gecontroleerd dat het testplan de programmeertaal van de ingediende oplossing ondersteunt.
    De programmeertaal van de oplossing wordt gegeven via de configuratie uit Dodona.
    Merk op dat de ingediende oplossing zelf hierbij niet nodig is: deze controle zou idealiter gebeuren bij het importeren van de oefening in Dodona, zodat Dodona weet in welke programmeertalen een bepaalde oefening aangeboden kan worden (zie \cref{ch:beperkingen-en-toekomstig-werk}).
    Als het testplan bijvoorbeeld programmeertaalspecifieke code bevat die enkel in Java geschreven is, zal een oplossing in Python niet beoordeeld kunnen worden.
    Bevat het testplan bijvoorbeeld een functie die een verzameling moet teruggeven, dan zullen talen als Bash niet in aanmerking komen.
    \item De testcode wordt gegenereerd voor elke context uit het testplan, wat resulteert in de testcode voor elke context.
    Deze stap is de overgang naar de \term{uitvoeringsomgeving}.
    \item De code wordt optioneel gecompileerd.
    Dit kan op twee manieren gebeuren (meer details in \cref{subsec:testcode-genereren}):
    \begin{enumerate}
        \item Batchcompilatie: hierbij wordt de testcode van alle contexten verzameld en gecompileerd tot één uitvoerbaar bestand (executable).
        Dit heeft als voordeel dat er slechts een keer een compilatie nodig is, wat goed is voor de performantie.
        \item Contextcompilatie: hierbij wordt de testcode voor elke context afzonderlijk gecompileerd tot een uitvoerbaar bestand.
    \end{enumerate}
    In talen die geen compilatie nodig hebben of ondersteunen, wordt deze stap overgeslagen.
    \item Nu kan het uitvoeren van de beoordeling zelf beginnen: de gegenereerde code wordt uitgevoerd (nog steeds in de uitvoeringsomgeving).
    Elke context uit he testplan wordt in een afzonderlijk subproces uitgevoerd, teneinde het delen van informatie tegen te gaan.
    De onafhankelijkheid van de contexten laat ons ook toe om contexten in parallel uit te voeren.
    Toch moeten we opmerken dat het parallel uitvoeren vooral een theoretische mogelijkheid is: in werkelijkheid gebruiken er meerdere gebruikers tegelijk Dodona, waardoor er ook meerdere oefeningen tegelijk beoordeeld worden, wat op zijn beurt ervoor zorgt dat er geen grote ruimte voor snelheidswinst door parallellisatie binnen een judge is.
    \item De uitvoering van de executable in de vorige stap resulteert in resultaten (voor elke context), zoals de standaarduitvoerstroom, de standaardfoutstroom, returnwaardes, exceptions of exitcodes.
    Deze bundel resultaten wordt nu geëvalueerd op juistheid.
    Hiervoor zijn drie mogelijke manieren:
    \begin{enumerate}
        \item Programmeertaalspecifieke evaluatie (afgekort tot SE in de flowchart).
        De code voor de evaluatie is opgenomen in de executable en wordt onmiddellijk uitgevoerd in hetzelfde proces.
        Via deze mogelijkheid kunnen taalspecifieke aspecten gecontroleerd worden.
        Daar de evaluatie in hetzelfde proces gebeurt, blijft dit in de uitvoeringsomgeving.
        \item Geprogrammeerde evaluatie (afgekort tot PE in de flowchart).
        Hierbij is er evaluatiecode geschreven die los staat van de oplossing, waardoor deze evaluatiecode ook in een andere programmeertaal geschreven kan zijn.
        De code ter uitvoering van de geprogrammeerde evaluatiecode wordt gegenereerd en dan uitgevoerd.
        Het doel van deze modus is om complexe evaluaties toe te laten op een programmeertaalonafhankelijke manier.
        Deze stap vindt plaats in de evaluatieomgeving.
        \item Generieke evaluatie.
        Hierbij evalueert TESTed zelf het resultaat.
        Deze modus is bedoeld voor gestandaardiseerde evaluaties, zoals het vergelijken van geproduceerde uitvoer en verwachte uitvoer.
        Hier gebeurt de evaluatie binnen TESTed zelf.
    \end{enumerate}
    \item Tot slot verzamelt TESTed alle evaluatieresultaten en stuurt ze gebundeld door naar Dodona, waarna ze getoond worden aan de gebruiker.
\end{enumerate}

\begin{figure}
    \centering
    \input{figures/architecture.tex}
    \caption{Schematische voorstelling van het architecturale ontwerp van de TESTed.}
    \label{fig:universal-judge}
\end{figure}

\begin{figure}
    \centering
    \input{figures/flow.tex}
    \caption{
        Flowchart van een beoordeling door TESTed.
        In het schema worden kleuren gebruikt als er een keuze gemaakt moet worden, en slechts een van de mogelijkheden gebruikt wordt.
        De afkortingen PE, GE en SE staan respectievelijk voor geprogrammeerde evaluatie, generieke evaluatie en (programmeertaal)specifieke evaluatie.
    }
    \label{fig:tested-flow}
\end{figure}


\section{Beschrijven van een oefening}\label{sec:testplan}

De beoordeling van een ingediende oplossing van een oefening begint bij de invoer die TESTed krijgt.
Centraal in deze invoer is een \term{testplan}, een specificatie die op een programmeertaalonafhankelijke manier beschrijft hoe een oplossing voor een oefening beoordeeld moet worden.
Het vervangt de taalspecifieke testen van de bestaande judges (ie.\ de jUnit-tests of de doctests in respectievelijk Java en Python).
Het testplan \latin{sensu lato} wordt opgedeeld in verschillende onderdelen, die hierna besproken worden.

\subsection{Het testplan}\label{subsec:het-testplan}

Het testplan \latin{sensu stricto} beschrijft de structuur van de beoordeling van een ingediende oplossing voor een oefening.
Deze structuur lijkt qua opbouw sterk op de structuur van de feedback zoals gebruikt door Dodona.
Dat de structuur van de oplossing in Dodona en van het testplan op elkaar lijken, heeft als voordeel dat er geen mentale afbeelding moet gemaakt worden tussen de structuur van het testplan en dat van Dodona.
Concreet is de structuur een hiërarchie met volgende elementen:

\begin{description}
    \item[Plan] Het top-level object van het testplan.
    Dit object bevat twee belangrijke objecten: de tabbladen en de configuratie.
    Deze configuratie is de plaats om opties aan TESTed mee te geven, zowel voor de TESTed zelf als voor programmeertaalspecifieke dingen.
    \item[Tab] Een testplan bestaat uit verschillende \termen{tab}s of tabbladen.
    Deze komen overeen met de tabbladen in de gebruikersinterface van Dodona.
    Een tabblad kan een naam hebben, die zichtbaar is voor de gebruikers.
    \item[Context] Elk tabblad bestaat uit een of meerdere \termen{context}en.
    Een context is een onafhankelijke uitvoering van een evaluatie.
    De nadruk ligt op de "onafhankelijkheid", zoals al vermeld.
    Elke context wordt in een nieuw proces en in een eigen map (directory) uitgevoerd, zodat de kans op het delen van informatie klein is.
    Hierbij willen we vooral onbedoeld delen van informatie (zoals statische variabelen of het overschrijven van bestanden) vermijden.
    De gemotiveerde student zal nog steeds informatie kunnen delen tussen de uitvoeringen, door bv.\ in een andere locatie een bestand aan te maken en later te lezen.
    \item[Testcase] Een context bestaat uit een of meerdere \termen{testcase}s of testgevallen.
    Een testgeval bestaat uit invoer en een aantal tests.
    De testgevallen kunnen onderverdeeld worden in twee soorten:
    \begin{description}
        \item[Main testcase] of hoofdtestgeval.
        Van deze soort is er maximaal één per context (dus geen hoofdtestgeval is ook mogelijk).
        Dit testgeval heeft als doel het uitvoeren van de main-functie (of de code zelf als het gaat om een scripttaal zoals Bash of Python).
        Als invoer voor dit testgeval kunnen enkel het standaardinvoerkanaal en de programma-argumenten meegegeven worden.
        De exitcode van een uitvoering kan ook enkel in het hoofdtestgeval gecontroleerd worden.
        \item[Normal testcase] of normaal testgeval.
        Hiervan kunnen er nul of meer zijn per context.
        Deze testgevallen dienen om andere aspecten van de ingediende oplossing te testen, nadat de code van de gebruiker met success ingeladen is.
        De invoer is dan ook uitgebreider: het kan gaan om het standaardinvoerkanaal, functieoproepen en variabeletoekenningen.
        Een functieoproep of variabeletoekenning is verplicht (zonder functieoproep of toekenning aan een variabele is er geen code om te testen).
    \end{description}
    Het hoofdtestgeval wordt altijd als eerste uitgevoerd.
    Dit is verplicht omdat bepaalde programmeertalen (zoals Python en andere scripttalen) de code onmiddellijk uitvoeren bij het inladen.
    Om te vermijden dat de volgorde van de testgevallen zou verschillen tussen de programmeertalen, wordt het hoofdtestgeval altijd eerst uitgevoerd.
    \item[Test] De beoordeling van een testgeval bestaat uit meerdere \term{test}s, die elk één aspect van het testgeval controleren.
    Met aspect bedoelen we het standaarduitvoerkanaal, het standaardfoutkanaal, opgevangen uitzonderingen (\english{exceptions}), de teruggegeven waarden van een functieoproep (returnwaarde) of de inhoud van een bestand.
    De exitcode is ook mogelijk, maar enkel in het hoofdtestgeval.
\end{description}

Bij de keuze voor een formaat voor het testplan (json, xml, \ldots), hebben we vooraf enkele vereisten geformuleerd waaraan het gekozen formaat moet voldoen.
Het moet:

\begin{itemize}
    \item leesbaar zijn voor mensen,
    \item geschreven kunnen worden met minimale inspanning, met andere woorden de syntaxis dient eenvoudig te zijn, en
    \item programmeertaalonafhankelijk zijn.
\end{itemize}

Uiteindelijk is gekozen om het op te stellen in json.
Niet alleen voldoet json aan de vooropgestelde voorwaarden, het wordt ook door veel talen ondersteund.

Toch zijn er ook enkele nadelen aan het gebruik van json.
Zo is json geen beknopte of compacte taal om met de hand te schrijven.
Een oplossing hiervoor gebruikt de eigenschap dat veel talen json kunnen produceren: andere programma's kunnen desgewenst het testplan in het json-formaat genereren, waardoor het niet met de hand geschreven moet worden.
Hiervoor denken we aan een \termen{DSL} (\english{domain specific language}), maar dit valt buiten de thesis en wordt verder besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Een tweede nadeel is dat json geen programmeertaal is.
Terwijl dit de implementatie van de judge bij het interpreteren van het testplan weliswaar eenvoudiger maakt, is het tevens beperkend: beslissen of een testgeval moet uitgevoerd worden op basis van het resultaat van een vorig testgeval is bijvoorbeeld niet mogelijk.
Ook deze beperking wordt uitgebreider besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Tot slot bevat \cref{lst:testplan} een testplan met één context voor de voorbeeldoefening Lotto uit \cref{ch:dodona}.

\begin{listing}
    \inputminted{python}{code/testplan.json}
    \caption{
        Een ingekorte versie van het testplan voor de voorbeeldoefening Lotto.
        Het testplan bevat maar één context.
    }
    \label{lst:testplan}
\end{listing}

\subsection{Dataserialisatie}\label{subsec:dataserialisatie}

Bij de beschrijving van het testplan wordt gewag gemaakt van returnwaarden en variabeletoekenningen.
Aangezien het testplan programmeertaalonafhankelijk is, moet er dus een manier zijn om data uit de verschillende programmeertalen voor te stellen en te vertalen: het \term{serialisatieformaat}.

\subsubsection{Keuze van het formaat}

Zoals bij het testplan, werd voor de voorstelling van waarden ook een keuze voor een bepaald formaat gemaakt.
Daarvoor werden opnieuw enkele voorwaarden vooropgesteld, waaraan het serialisatieformaat moet voldoen.
Het formaat moet:

\begin{itemize}
    \item door mensen geschreven kunnen worden (\english{human writable}),
    \item onderdeel van het testplan kunnen zijn,
    \item in meerdere programmeertalen bruikbaar zijn, en
    \item de basisgegevenstypes ondersteunen die we willen aanbieden in het programmeertaalonafhankelijke deel van het testplan.
    Deze gegevenstypes zijn:
    \begin{itemize}
        \item Primitieven: gehele getallen, reële getallen, Boolese waarden en tekenreeksen.
        \item Collecties: rijen (eindige, geordende reeks; \texttt{list} of \texttt{array}), verzamelingen (eindige, ongeordende reeks zonder herhaling; \texttt{set}) en afbeeldingen (elk element wordt afgebeeld op een ander element; \texttt{map}, \texttt{dict} of \texttt{object}).
    \end{itemize}
\end{itemize}

Een voor de hand liggende oplossing is om ook hiervoor json te gebruiken, en zelf in json een structuur op te stellen voor de waarden.
In tegenstelling tot het testplan bestaan er al een resem aan dataserialisatieformaten, waardoor het de moeite loont om na te gaan of er geen bestaand formaat voldoet aan de vereisten.
Hiervoor is gestart van een overzicht op Wikipedia, \autocite{wiki2020}.
Uiteindelijk is niet gekozen voor een bestaand formaat, maar voor de json-oplossing.
De redenen hiervoor zijn samen te vatten als:

\begin{itemize}
    \item Het gaat om een binair formaat.
    Binaire formaten zijn uitgesloten op basis van de eerste twee voorwaarden die we opgesteld hebben: mensen kunnen het niet schrijven zonder hulp van bijkomende tools en het is moeilijk in te bedden in een json-bestand (zonder gebruik te maken van encoderingen zoals base64).
    Bovendien zijn binaire formaten moeilijker te implementeren in sommige talen.
    \item Het formaat ondersteunt niet alle gewenste types.
    Sommige formaten hebben ondersteuning voor complexere datatypes, maar niet voor alle complexere datatypes die wij nodig hebben.
    Uiteraard kunnen de eigen types samengesteld worden uit basistypes, maar dan biedt de ondersteuning voor de complexere types weinig voordeel, aangezien er toch een eigen dataschema voor die complexere types opgesteld zal moeten worden.
    \item Sommige formaten zijn omslachtig in gebruik.
    Vaak ondersteunen dit soort formaten meer dan wat wij nodig hebben.
    \item Het formaat is niet eenvoudig te implementeren in een programmeertaal waarvoor geen ondersteuning is.
    Sommige dezer formaten ondersteunen weliswaar veel talen, maar we willen niet dat het serialisatieformaat een beperkende factor wordt in welke talen door de judge ondersteund worden.
    Het mag niet de bedoeling zijn dat het implementeren van het serialisatieformaat het meeste tijd in beslag neemt.
\end{itemize}

Een lijst van de overwogen formaten met een korte beschrijving:

\begin{description}
    \item[Apache Avro] Een volledig "systeem voor dataserialisatie".
    De specificatie van het formaat gebeurt in json (vergelijkbaar met JSON Schema), terwijl de eigenlijke data binair geëncodeerd wordt.
    Heeft uitbreidbare types, met veel ingebouwde types \autocite{avro}.
    \item[Apache Parquet] Minder relevant, dit is een bestandsformaat voor Hadoop \autocite{parquet}.
    \item[ASN.1] Staat voor \english{Abstract Syntax Notation One}, een formaat uit de telecommunicatie.
    De hoofdstandaard beschrijft enkel de notatie voor een dataformaat.
    Andere standaarden beschrijven dan de serialisatie, bv.\ een binair formaat, json of xml.
    De meerdere serialisatievormen zijn in theorie aantrekkelijk: elke taal moet er slechts een ondersteunen, terwijl de judge ze allemaal kan ondersteunen.
    In de praktijk blijkt echter dat voor veel talen er slechts één serialisatieformaat is, en dat dit vaak het binaire formaat is \autocite{x680}.
    \item[Bencode] Schema gebruikt in BitTorrent.
    Het is gedeeltelijk binair, gedeeltelijk in text \autocite{cohen2017}.
    \item[Binn] Binair dataformaat \autocite{ramos2019}.
    \item[BSON] Een binaire variant op json, geschreven voor en door MongoDB \autocite{bson}.
    \item[CBOR] Een lichtjes op json gebaseerd formaat, ook binair.
    Heeft een goede standaard, ondersteunt redelijk wat talen \autocite{rfc7049}.
    \item[FlatBuffers] Lijkt op ProtocolBuffers, allebei geschreven door Google, maar verschilt wat in implementatie van ProtocolBuffers.
    De encodering is binair \autocite{flatbuffers}.
    \item[Fast Infoset] Is eigenlijk een manier om xml binair te encoderen (te beschouwen als een soort compressie voor xml), waardoor het minder geschikt voor ons gebruik wordt \autocite{x981}.
    \item[Ion] Een superset van json, ontwikkeld door Amazon.
    Het heeft zowel een tekstuele als binaire voorstelling.
    Naast de gebruikelijke json-types, bevat het enkele uitbreidingen. \autocite{ion}.
    \item[MessagePack] Nog een binair formaat dat lichtjes op json gebaseerd is.
    Lijkt qua types sterk op json.
    Heeft implementaties in veel talen \autocite{messagepack}.
    \item[OGDL] Afkorting voor \english{Ordered Graph Data Language}.
    Daar het om een serialisatieformaat voor grafen gaat, is het niet nuttig voor ons doel \autocite{ogdl}.
    \item[OPC Unified Architecture] Een protocol voor intermachinecommunicatie.
    Complex: de specificatie bevat 14 documenten, met ongeveer 1250 pagina's \autocite{tr62541}.
    \item[OpenDLL] Afkorting voor de \english{Open Data Description Language}.
    Een tekstueel formaat, bedoeld om arbitraire data voor te stellen.
    Wordt niet ondersteund in veel programmeertalen, in vergelijking met bv.\ json \autocite{openddl}.
    \item[ProtocolBuffers] Lijkt zoals vermeld sterk op FlatBuffers, maar heeft nog extra stappen nodig bij het encoderen en decoderen, wat het minder geschikt maakt \autocite{protobuf}.
    \item[Smile] Nog een binaire variant van json \autocite{smile}.
    \item[SOAP] Afkorting voor \english{Simple Object Access Protocol}.
    Niet bedoeld als formaat voor dataserialisatie, maar voor communicatie tussen systemen over een netwerk \autocite{soap}.
    \item[SDXF] Binair formaat voor data-uitwisseling.
    Weinig talen ondersteunen dit formaat \autocite{rfc3072}.
    \item[Thrift] Lijkt sterk op ProtocolBuffers, maar geschreven door Facebook \autocite{slee2007}.
    \item[UBJSON] Nog een binaire variant van json \autocite{ubjson}.

\end{description}

Geen enkel overwogen formaat heeft grote voordelen tegenover een eigen structuur in json.
Daarenboven hebben veel talen het nadeel dat ze geen json zijn, waardoor we een nieuwe taal moeten inbedden in het bestaande json-testplan.
Dit nadeel, gekoppeld met het ontbreken van voordelen, heeft geleid tot de keuze voor json.

\subsubsection{Dataschema}

Json is echter slechts een formaat en geeft geen semantische betekenis aan json-elementen.
Hiervoor stellen we een dataschema op, dat uit twee onderdelen bestaat:

\begin{itemize}
    \item Het encoderen van waarden.
    \item Het beschrijven van de gegevenstypes van deze waarden.
\end{itemize}

Elke waarde wordt in het serialisatieformaat voorgesteld als een object met twee elementen: de geëncodeerde waarde en het bijhorende gegevenstype.
Een concreet voorbeeld is \cref{lst:serialisation}.

\begin{listing}
    \inputminted{json}{code/format.json}
    \caption{Een lijst bestaande uit twee getallen, geëncodeerd in het serialisatieformaat.}
    \label{lst:serialisation}
\end{listing}


Het encoderen van waarden slaat op het voorstellen van waarden als json-waarden.
Json heeft slechts een beperkt aantal gegevenstypes, dus worden alle waarden voorgesteld als een van deze types.
Zo worden bijvoorbeeld zowel \texttt{array}s en \texttt{set}s voorgesteld als een json-lijst.

Het verschil tussen beiden wordt dan duidelijk gemaakt door het bijhorende gegevenstype.
Er is dus nood aan een systeem om aan te geven wat het gegevenstype van een waarde is.

Enerzijds vervult dit dataschema een gelijkaardige functie als JSON Schema (of XML Schema voor xml-bestanden): het dataschema legt de structuur van het serialisatieformaat vast.
Anderzijds is er een belangrijk verschil: het dataschema schrijft geen verwachte gegevenstypes voor, maar beschrijft bestaande data.
Het dataschema is dus altijd gekoppeld aan concrete data.
Dit laatste zorgt ervoor dat het dataschema eenvoudig kan blijven, wat als voordeel heeft dat implementaties eenvoudiger zijn.
Complexe types als \texttt{union[string, int]} zijn bijvoorbeeld niet mogelijk, maar ook niet nodig: concrete data kan niet zowel een \texttt{string} als een \texttt{int} zijn.

\begin{listing}
    \inputminted{json}{code/value-schema.json}
    \caption{
        Het schema voor een waarde in het serialisatieformaat, in een vereenvoudigde versie van JSON Schema.
        Hierbij is \texttt{<type>} een van de gegevenstypes die besproken worden in subparagraaf \emph{Gegevenstypes} van \cref{subsec:dataserialisatie}.
    }
    \label{lst:value-schema}
\end{listing}

Tot slot bevat \cref{lst:value-schema} het schema van een waarde in het serialisatieformaat, in een vereenvoudigde versie van JSON Schema.
Hierbij staat \texttt{<type>} voor een van de gegevenstypes die hierna besproken worden.

\subsubsection{Gegevenstypes}

Het systeem om de gegevenstypes aan te duiden vervult een dubbele functie:

\begin{itemize}
    \item Het wordt gebruikt om het gegevenstype van concrete data aan te duiden (beschrijvende modus).
    \item Het wordt gebruikt om te beschrijven welke gegevenstype verwacht wordt (voorschrijvende modus).
\end{itemize}

Bij het ontwerp van het systeem voor de gegevenstypes zorgen deze twee functies soms voor tegenstrijdige belangen: voor het beschrijven van een waarde moet het systeem zo eenvoudig mogelijk zijn.
Een waarde met bijhorend gegevenstype \texttt{union[string, int]} is niet bijster nuttig: een waarde kan nooit tegelijk een \texttt{string} en een \texttt{int} zijn.
Aan de andere kant zijn dit soort complexe gegevenstypes wel nuttig bij het beschrijven van het verwachte gegevenstype van bijvoorbeeld een variabele.
Daarnaast moet ook rekening gehouden worden met het feit dat deze gegevenstypes in veel programmeertalen implementeerbaar moeten zijn.
Een gegevenstype als \texttt{union[string, int]} is eenvoudig te implementeren in Python, maar dat is niet het geval in bijvoorbeeld Java of C\@.
Ook heeft elke taal een eigen niveau van details bij gegevenstypes.
Python heeft bijvoorbeeld enkel \texttt{integer} voor gehele getallen, terwijl C beschikt over \texttt{int}, \texttt{unsigned}, \texttt{long}, enz.

Om deze redenen zijn de gegevenstypes opgedeeld in vier categorieën:

\begin{enumerate}
    \item De basistypes.
    Deze gegevenstypes zijn bruikbaar in zowel beschrijvende als voorschrijvende modus.
    De lijst van basistypes omvat:
    \begin{description}
        \item[\texttt{integer}] Gehele getallen, zowel positief als negatief.
        \item[\texttt{rational}] Rationale getallen.
        \item[\texttt{text}] Een tekenreeks of string (alle vormen).
        \item[\texttt{boolean}] Een Boolese waarde (of boolean).
        \item[\texttt{sequence}] Een wiskundige rij, wat wil zeggen dat de volgorde belangrijk is en dat dubbele elementen toegelaten zijn.
        \item[\texttt{set}] Een wiskundige verzameling, wat wil zeggen dat de volgorde niet belangrijk is en dat dubbele elementen niet toegelaten zijn.
        \item[\texttt{map}] Een wiskundige afbeelding: elk element wordt afgebeeld op een ander element.
        In Java is dit bijvoorbeeld een \texttt{Map}, in Python een \texttt{dict} en in Javascript een \texttt{object}.
        \item[\texttt{nothing}] Geeft aan dat er geen waarde is, ook wel \texttt{null}, \texttt{None} of \texttt{nil} genoemd.
    \end{description}
    Elke implementatie van een programmeertaal moet een keuze maken wat de standaardimplementatie van deze types is.
    Zo implementeert de Java-implementatie het gegevenstype \texttt{sequence} als een \texttt{List<>}, niet als een \texttt{array}.
    Een implementatie in een programmeertaal kan ook aangeven dat een bepaald type niet ondersteund wordt, waardoor testplannen met dat type niet zullen werken.
    \item De uitgebreide types.
    Naast de basistypes van hierboven, bevat TESTed een hele reeks bijkomende types.
    Deze gegevenstypes staan toe om meer details te gebruiken.
    Een voorbeeld is de lijst van types in \cref{tab:vertaling}, dat voor een reeks gegevenstypes voor gehele getallen de concrete types in verschillende programmeertalen geeft.
    Het grote verschil is dat deze uitgebreide types standaard vertaald worden naar een van de basistypes.
    Voor talen die bijvoorbeeld geen \texttt{tuple} uit Python ondersteunen, zal het vanzelf omgezet worden naar een \texttt{list}.
    Er is ook de mogelijk dat implementaties voor programmeertalen expliciet een bepaald type niet ondersteunen.
    Zo zal de Java-implementatie geen \texttt{uint64} (een unsigned 64-bit integer) ondersteunen, omdat er geen equivalent bestaat in de taal.
    \item De gegevenstypes worden ook gebruikt om de types van functie-argumenten aan te duiden.
    Hier is er nood aan een bijkomend type:
    \begin{description}
        \item[identifier] Een verwijzing naar een eerder benoemd element (zoals variabelen of functies).
        Bij het gebruik van dit gegevenstype stel de data de naam van de identifier voor.
    \end{description}
    Het is logisch dat dit type niet gebruikt kan worden voor het encoderen van data, daar dit bijvoorbeeld onmogelijk als returnwaarde kan voorkomen.
    \item Voorschrijvende types.
    Gegevenstypes in deze categorie kunnen enkel gebruikt worden bij het aangeven welk gegevenstype verwacht wordt, niet bij de eigenlijke encodering van waarden.
    In de praktijk gaat het om het type van variabelen.
    In deze categorie zouden gegevenstypes als \texttt{union[str, int]} komen.
    Er is echt expliciet gekozen om dit soort types niet te ondersteunen, door de moeilijkheid om dit te implementeren in statisch getypeerde talen, zoals Java of C\@.
    Een type dat wel ondersteund wordt is:
    \begin{description}
        \item[any] Het \texttt{any}-type geeft aan dat het type van een variabele onbekend is.
        Merk op dat dit in sommige talen tot moeilijkheden zal leiden: zo zal dit in C-code als \texttt{long} beschouwd worden (want C heeft geen equivalent van een \texttt{any}-type).
    \end{description}
\end{enumerate}

\begin{table}[h]
    \centering
    \caption{Voorbeeld van de implementatie van types voor gehele getallen, met als basistype \texttt{integer}.}
    \label{tab:vertaling}
    \begin{threeparttable}
        \begin{tabular}{|l|llllllll|}
            \hline
                       & \texttt{int8} & \texttt{uint8} & \texttt{int16} & \texttt{uint16} & \texttt{int32} & \texttt{uint32} & \texttt{int64} & \texttt{uint64} \\
            \hline
            Python     & \texttt{int}  & \texttt{int}   & \texttt{int}   & \texttt{int}    & \texttt{int}   & \texttt{int}    & \texttt{int}   & \texttt{int}    \\
            Java       & \texttt{byte} & \texttt{short} & \texttt{short} & \texttt{int}    & \texttt{int}   & \texttt{long}   & \texttt{long}  & -               \\
            C\tnote{1} & \texttt{int8\_t} & \texttt{uint8\_t} & \texttt{int16\_t} & \texttt{uint16\_t} & \texttt{int32\_t} & \texttt{uint32\_t} & \texttt{int64\_t} & \texttt{uint64\_t} \\
            Haskell    & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} & \texttt{Integer} \\
            \hline
        \end{tabular}
    \begin{tablenotes}
        \item[1] Uiteraard met de gebruikelijke aliassen van \texttt{short}, \texttt{unsigned}, \ldots
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsection{Functieoproepen en assignments}\label{subsec:functieoproepen}

Een ander onderdeel van het testplan verdient ook speciale aandacht: toekennen van waarden aan variabelen (\english{assignments}) en functieoproepen.

In heel wat oefeningen, en zeker bij objectgerichte en imperatieve programmeertalen, is het toekennen van een waarde aan een variabele, om deze later te gebruiken, onmisbaar.
Bijvoorbeeld zou een opgave kunnen bestaan uit het implementeren van een klasse.
Bij de evaluatie dient dan een instantie van die klasse aangemaakt te worden, waarna er methoden kunnen aangeroepen worden, zoals hieronder geïllustreerd in een fictief voorbeeld.

\inputminted{java}{code/assignment.jshell}

Concreet is ervoor gekozen om het testplan niet uit te breiden met generieke statements of expressies, maar de ondersteuning te beperken tot assignments en functieoproepen.
Dit om de implementatie van de vertaling van het testplan naar de ondersteunde programmeertalen niet nodeloos ingewikkeld te maken.

\begin{listing}
    \inputminted{json}{code/value-schema.json}
    \caption{
        Het schema voor een functieoproep in het testplan.
        Hierbij is \texttt{<waarde>} een waarde, zoals besproken in subparagraaf \emph{Dataschema} van \cref{subsec:dataserialisatie}.
    }
    \label{lst:function-schema}
\end{listing}

\Cref{lst:function-schema} toont de structuur van een functieoproep in een vereenvoudigde versie van JSON Schema.
Het bestaat uit volgende onderdelen:
\begin{description}
    \item[\texttt{type}] Het soort functie.
    Kan een van deze waarden zijn:
    \begin{description}
        \item[\texttt{function}] Een \english{top-level} functie.
        Afhankelijk van de programmeertaal zal deze functie toch omgezet worden naar een functie op een object (bv.\ naar een statische functie in Java).
        \item[\texttt{namespace}] Een methode (functie van een object) of een functie in een namespace.
        De invulling hiervan is gedeeltelijk programmeertaalafhankelijk: in Java gaat het om methodes, terwijl het in Haskell om functies van een module gaat.
        Bij dit soort functies moet de \texttt{namespace} gegeven worden.
        \item[\texttt{constructor}] Deze soort heeft dezelfde semantiek als een top-level functie, met dien verstande dat het om een constructor gaat.
        In Java zal bijvoorbeeld het keyword \texttt{new} vanzelf toegevoegd worden.
        De functienaam doet dienst als naam van de klasse.
        \item[\texttt{identity}] Dit is een speciaal geval: er mag geen functienaam gegeven worden en er moet exact één argument gegeven worden.
        Dat ene argument zal de returnwaarde van de functie zijn.
        In de implementaties wordt dit vaak ook niet vertaald als een functie, maar wordt de waarde rechtstreeks gebruikt.
        De bestaansreden van dit soort functies is het toekennen van waarden aan variabelen, om redenen die we later zullen bespreken.
    \end{description}
    \item[\texttt{namespace}] De namespace voor functies van het type \texttt{namespace}.
    \item[\texttt{name}] De naam van de functie.
    \item[\texttt{arguments}] De argumenten van de functie.
    Dit is een lijst van waarden, waarbij de waarden zijn zoals beschreven bij het serialisatieformaat in \cref{subsec:dataserialisatie}.
\end{description}

% TODO: herdenk heel deze zever.
Een beperking is dat het niet mogelijk is om rechtstreeks een functieoproep te doen als argument voor een andere functie,
of toch niet op een programmeertaalonafhankelijke manier.
Een oproep als \texttt{oproep(hallo(), 5)} is niet mogelijk.
Bij dergelijke dingen zal de functieoproep eerst aan variabele moeten toegekend worden, bv.\ \texttt{var param = hallo()}, waarna deze variabele als argument met type \texttt{literal} kan gegeven worden aan de oorspronkelijke functie: \texttt{oproep(param, 5)}.
De aandachtige lezer zal opmerken dat met die functieargumenten van het type \texttt{literal} rond deze beperking kan gewerkt worden, aangezien de tekstuele waarde van een dergelijk argument letterlijk in de taal komt.
We raden deze omweg echter ten sterkste af: dit maakt het testplan taalafhankelijk, want niet elke programmeertaal implementeert functieoproepen op eenzelfde wijze.

Dit brengt ons bij de variabeletoekenning of \english{assignment}.
In ons testplan beperkt dit zich tot het toekennen van een naam aan het resultaat van een functieoproep.
Dit is ook meteen de reden voor het bestaan van de functiesoort \texttt{identity}: via deze weg blijft het testplan eenvoudiger (de waarde van een assignment is altijd een functieoproep), maar toch kunnen gewoon waarden toegekend worden aan een variabelen.
Concreet ziet een variabeletoekenning er als volgt uit:

\inputminted{json}{code/assignment.json}

De \texttt{name} is de naam die aan de variabele gegeven zal worden.
Het veldje \texttt{expression} moet een object zijn dat een functieoproep voorstelt, in het formaat zoals hiervoor besproken.
In een beperkt aantal gevallen kan de judge het type van de variabele afleiden uit de functieoproep, maar in veel gevallen is het nodig om zelf het type mee te geven.
Dit type moet een van de ondersteunde types zijn uit het serialisatieformaat, zij het dat er ondersteuning is voor eigen types (zoals een klasse die geïmplementeerd moest worden door de student).

Een gecombineerd voorbeeld staat hieronder.
Hier wordt de string \texttt{'Dodona'} toegekend aan een variabele met naam \texttt{name}.
De judge kan het type afleiden, dus we moeten niet opgeven dat \texttt{name} een \texttt{text} is.

\inputminted{json}{code/assign-variable.json}

\subsection{Controle ondersteuning voor programmeertalen}\label{subsec:vereiste-functies}

In het stappenplan uit \cref{sec:ontwerp} is al vermeld dat vóór een beoordeling start, een controle plaatsvindt om zeker te zijn dat het testplan uitgevoerd kan worden in de programmeertaal van de ingediende oplossing.
Concreet gebeurt dit door voor elk item in het testplan af te leiden wat de programmeertaal dient te ondersteunen om met dat item uit het testplan te kunnen werken.
Bevat een testplan bijvoorbeeld waarden met als type \texttt{set} (verzamelingen), dan kunnen enkel programmeertalen die verzamelingen ondersteunen gebruikt worden.
Dat zijn bijvoorbeeld Python en Java, maar geen Bash.
Het afleiden van wat de programmeertaal moet ondersteunen gebeurt volledig automatisch aan de hand van het testplan.

\section{Oplossingen uitvoeren}\label{sec:oplossingen-uitvoeren}

De eerste stap die wordt uitgevoerd bij de beoordeling van een ingediende oplossing is het genereren van de testcode, die de ingediende oplossing zal beoordelen.

\subsection{Testcode genereren}\label{subsec:testcode-genereren}

Het genereren van de testcode gebeurt met een sjabloonsysteem genaamd Mako \autocite{mako}.
Dit soort systemen wordt traditioneel gebruikt bij webapplicaties (zoals Ruby on Rails met \textsc{erb}, Phoenix met \textsc{eex}, Laravel met Blade, enz.) om bijvoorbeeld html-pagina's te genereren.
In ons geval zijn de sjablonen verantwoordelijk voor de vertaling van programmeertaalonafhankelijke specificaties in het testplan naar concrete testcode in de programmeertaal van de ingediende oplossing.
Hierbij denken we aan de functieoproepen, assignments, enz.
Ook zijn de sjablonen verantwoordelijk voor het genereren van de code die de oplossing van de student zal oproepen en evalueren.

\subsubsection{Sjablonen}

TESTed heeft een aantal standaardsjablonen nodig, waaraan vastgelegde parameters meegegeven worden en die een vaste functie moeten uitvoeren.
Deze verplichte sjablonen zijn:
\begin{description}
    \item[\texttt{assignment}] Vertaalt een toekenningsopdracht uit het testplan naar code.
    \item[\texttt{context}] Een sjabloon dat code genereert om een context te beoordelen.
    Deze code moet uitvoerbaar zijn (dat wil zeggen een main-functie bevatten of een script zijn).
    \item[\texttt{selector}] Een sjabloon dat code genereert om een bepaalde context uit te voeren.
    Om performantieredenen (hierover later meer) wordt de code van alle contexten soms uit een keer gegenereerd en gecompileerd.
    Aan de hand van een parameter (de naam van de context), wordt bij het uitvoeren van deze selectiecode de testcode voor de juiste context gekozen.
    Dit sjabloon is enkel nodig indien batchcompilatie ondersteund wordt.
    \item[\texttt{evaluator\_executor}] Een sjabloon dat code genereert om een geprogrammeerde evaluatie te starten.
    \item[\texttt{function}] Vertaalt een functie-oproep naar testcode.
\end{description}

Daarnaast moet het encoderen naar het serialisatieformaat ook geïmplementeerd worden in elke programmeertaal.
Veel programmeertalen hebben dus nog enkele bijkomende bestanden met code.
In alle bestaande configuraties van programmeertalen is dit geïmplementeerd als een module of een klasse met naam \texttt{Value}.
Dit wordt geïllustreerd in \cref{ch:nieuwe-taal}, dat het toevoegen van een nieuwe programmeertaal aan TESTed volledig uitwerkt.

\subsubsection{Testcode compileren}

TESTed ondersteunt twee modi waarin de code gecompileerd kan worden (bij programmeertalen die geen compilatie ondersteunen wordt deze stap overgeslagen):

\begin{description}
    \item[Batchcompilatie] In deze modus wordt de code voor alle contexten in een keer gecompileerd.
    Dit wordt gedaan om performantieredenen.
    In talen die resulteren in een uitvoerbaar bestand (zoals Haskell, C/C++), resulteert deze modus in één uitvoerbaar bestand voor alle contexten.
    Bij het uitvoeren wordt dan aan de hand van een parameter de juiste context uitgevoerd (met het \texttt{selector}-sjabloon van hierboven).
    \item[Contextcompilatie] Hierbij wordt elke context afzonderlijk gecompileerd.
\end{description}

Dit wordt getoond in \cref{fig:tested-flow} uit \cref{sec:ontwerp} door twee kleuren te gebruiken: de stappen die enkel gebeuren bij batchcompilatie zijn in het \textcolor{ugent-ps}{groen}, terwijl stappen die enkel bij contextcompilatie gebeuren in het \textcolor{ugent-we}{blauw} staan.
Stappen die altijd gebeuren staan in de flowchart in het zwart.

Dit gedrag is configureerbaar in het testplan, maar standaard wordt de batchcompilatie gebruikt.
Als er een compilatiefout optreed bij de compilatie in batchcompilatie, wordt valt TESTed terug op contextcompilatie.
Deze terugval is handig voor programmeertalen waar de compilatie veel fouten ontdekt (vaak de meer statische programmeertalen).
Een voorbeeldscenario is als volgt: stel een oefening waarbij de student twee functies moet implementeren.
De student implementeert de eerste functie en dient een oplossing in om al feedback te krijgen.
Bij programmeertalen als Java of Haskell zal dit niet lukken: daar alle contexten in één keer gecompileerd worden, zal de ontbrekende tweede functie ervoor zorgen dat de volledige compilatie faalt.
In individuele modus is dit geen probleem: de contexten die de eerste functie testen zullen compileren en kunnen uitgevoerd worden.
De individuele modus brengt wel een niet te verwaarlozen kost qua uitvoeringstijd met zich mee (zie ook \cref{ch:beperkingen-en-toekomstig-werk}).
% TODO: referentie naar deel over performantie.

\Cref{lst:generated-context-python,lst:generated-context-java} bevatten de testcode gegenereerd voor een context uit de voorbeeldoefening Lotto (het gaat om dezelfde context uit het voorbeeld van het testplan in \cref{lst:testplan}), in respectievelijk Python en Java.
Daarnaast bevat Z de code voor de \texttt{selector} in Java.
Hiervan is geen versie in Python, daar Python selector nodig heeft in batchcompilatie (in Python kunnen meerdere onafhankelijke bestanden tegelijk gecompileerd worden).
De selector bevat twee contexten om de werking duidelijk te maken.

\begin{listing}
    \inputminted{python}{code/generated-context-1.py}
    \caption{
        Gegenereerde testcode in Python voor de eerste context uit het testplan van de voorbeeldoefening Lotto.
    }
    \label{lst:generated-context-python}
\end{listing}

\begin{listing}
    \inputminted{java}{code/generated-context-1.java}
    \caption{
        Gegenereerde testcode in Java voor de eerste context uit het testplan van de voorbeeldoefening Lotto.
        Enkele hulpfuncties en imports zijn verwijderd om de code korter te maken.
    }
    \label{lst:generated-context-java}
\end{listing}

\begin{listing}
    \inputminted{java}{code/Selector.java}
    \caption{
        Gegenereerde selector in Java voor twee contexten uit het testplan van de voorbeeldoefening Lotto.
    }
    \label{lst:selector-java}
\end{listing}

\subsection{Testcode uitvoeren}\label{subsec:testcode-uitvoeren}

Vervolgens wordt de (gecompileerde) testcode voor elke context uit het testplan afzonderlijk uitgevoerd en worden de resultaten (het gedrag en de neveneffecten) verzameld.
Het uitvoeren zelf gebeurt op de normale manier waarop code voor de programmeertaal uitgevoerd wordt: via de commandoregel.
Deze aanpak heeft als voordeel dat er geen verschil is tussen hoe TESTed de ingediende code uitvoert en hoe de student zijn code zelf uitvoert op zijn eigen computer.
Dit voorkomt dat er subtiele verschillen in de resultaten sluipen.

Indien de configuratie het toelaat, worden de contexten parallel uitgevoerd.
Om te vermijden dat bestanden of uitvoer overschreven worden, wordt alle relevante gecompileerde code voor een context gekopieerd naar een aparte map waar het uitvoeren gebeurt.
\Cref{lst:mapstructuur} illustreert dit met een voorbeeld voor een ingediende oplossing in de programmeertaal Python.
Deze mapstructuur stelt de toestand van de werkmap van TESTed voor na het uitvoeren van de code.
In de map \texttt{common} zit alle testcode en de gecompileerde bestanden voor alle contexten.
Voor elke context worden de gecompileerde bestanden gekopieerd naar een andere map, bv.\ \texttt{context\_0\_1}, wat de map is voor context \texttt{1} van tabblad \texttt{0} van het testplan.

\begin{listing}
    \inputminted{text}{code/dir-listing.txt}
    \caption{Mapstructuur na het uitvoeren van de testcode van een oplossing in Python.
    \texttt{context\_0\_0} staat voor de eerste context van het eerste tabblad.
    }
    \label{lst:mapstructuur}
\end{listing}

\subsection{Beoordelen van gedrag}\label{subsec:beoordelen-van-gedrag}

Het uitvoeren van de testcode genereert resultaten (gedrag en neveneffecten) die door TESTed beoordeeld moeten worden.
Er zijn verschillende soorten gedragingen en neveneffecten die interessant zijn.
Elke soort gedrag of neveneffect wordt een \term{uitvoerkanaal} genoemd.
TESTed verzamelt volgende uitvoerkanalen:
\begin{itemize}
    \item De standaarduitvoerstroom.
    Dit wordt verzameld als tekstuele uitvoer.
    \item De standaardfoutstroom.
    Ook dit wordt als tekst verzameld.
    \item Fatale uitzonderingen.
    Hiermee bedoelen we uitzonderingen die tot aan de testcode geraken.
    Een uitzondering die afgehandeld wordt door de ingediende oplossing wordt niet verzameld.
    De uitzonderingen worden verzameld in een bestand.
    \item Returnwaarden.
    Deze waarden worden geëncodeerd en ook verzameld in een bestand.
    \item Exitcode.
    Het gaat om de exitcode van de testcode voor een context.
    Daar de code per context wordt uitgevoerd, wordt de exitcode ook verzameld per context (en niet per testcase, zoals de andere uitvoerkanalen).
    \item Bestanden.
    Tijdens het beoordelen van de verzamelde resultaten is het mogelijk de door de ingediende oplossing gemaakte bestanden te bekijken.
\end{itemize}

De standaarduitvoer- en standaardfoutstroom worden rechtstreeks opgevangen door TESTed.
De andere uitvoerkanalen (uitzonderingen en returnwaarden) worden naar een bestand geschreven.
De reden dat deze niet naar een andere \term{file descriptor} geschreven worden is eenvoudig: niet alle talen (zoals Java) ondersteunen het openen van bijkomende file descriptors.

Alle uitvoerkanalen (met uitzondering van de exitcode en de bestanden) worden per testcase verzameld.
Aangezien de uitvoerkanalen pas verzameld worden na het uitvoeren van de context, moet er een manier zijn om de uitvoer van de verschillende testgevallen te onderscheiden.
De testcode is hier verantwoordelijk voor, en schrijft een \english{separator} naar alle uitvoerkanalen tussen elk testgeval, zoals te zien is in \cref{lst:uitvoer}.

\begin{listing}
    \begin{minted}{text}
    {"data":"1 - 3 - 6 - 8 - 10 - 15","type":"text"}--gL9koJNv3-- SEP
    \end{minted}
    \caption{Voorbeeld van het uitvoerkanaal voor returnwaarden na het uitvoeren van de eerste context uit de voorbeeldoefening Lotto.}
    \label{lst:uitvoer}
\end{listing}

Tijdens het genereren van de code krijgen de sjablonen een reeks willekeurige tekens mee, de \english{secret}.
Deze secret wordt gebruikt voor verschillende dingen, zoals:
\begin{itemize}
    \item De separator.
    Door het gebruik van de willekeurige tekens is de kans dat de separator overeenkomt met een echt waarde praktisch onbestaand.
    \item Bestandsnamen.
    De testcode is verantwoordelijk voor het openen van de bestanden voor de uitvoerkanalen die naar een bestand geschreven worden.
    Bij het openen zal de testcode de secret in de bestandsnaam gebruiken.
    Dit is om het per abuis overschrijven van deze bestanden door de ingediende oplossing tegen te gaan.
\end{itemize}

\section{Oplossingen beoordelen}\label{sec:evalueren-van-een-oplossing2}

Na het uitvoeren van de testcode voor elke context heeft TESTed alle relevante uitvoer gemeten en verzameld.
Deze uitvoer moet vervolgens beoordeeld worden om na te gaan in hoeverre deze uitvoer voldoet aan de verwachte uitvoer.
Dit kan op drie manieren:
\begin{enumerate}
    \item Generieke evaluatie: de uitvoer wordt beoordeeld door TESTed zelf.
    \item Geprogrammeerde evaluatie: de uitvoer wordt beoordeeld door programmacode geschreven door degene die de oefening opgesteld heeft, in een aparte omgeving (de evaluatieomgeving).
    \item Programmeertaalspecifieke evaluatie: de uitvoer wordt onmiddellijk na het uitvoeren van de testcode beoordeeld in het hetzelfde proces.
\end{enumerate}

\subsection{Generieke evaluatie}\label{subsec:ingebouwde-evaluator}

Voor eenvoudige beoordelingen (bijvoorbeeld tussen twee waarden) volstaat de generieke evaluatie binnen TESTed.
Het is mogelijk om de verwachte resultaten in het testplan op te nemen.
TESTed zal deze resultaten uit het testplan dan vergelijken met de resultaten geproduceerd door het uitvoeren van de testcode.
Als \english{proof of concept} zijn drie eenvoudige evaluatiemethoden ingebouwd in TESTed, die hieronder besproken worden.

\subsubsection{Tekstevaluatie}

Deze evaluator vergelijkt de verkregen uitvoer van een uitvoerkanaal (standaarduitvoer, standaardfout, \ldots) met de verwachte uitvoer uit het testplan.
Deze evaluator biedt enkele opties om het gedrag aan te passen:

\begin{description}
    \item[\texttt{ignoreWhitespace}]
    Witruimte voor en na het resultaat wordt genegeerd.
    Dit gebeurt op de volledige tekst, niet regel per regel.
    \item[\texttt{caseInsensitive}] Er wordt geen rekening gehouden met het verschil tussen hoofdletters en kleine letters.
    \item[\texttt{tryFloatingPoint}]
    De tekst zal geïnterpreteerd worden als een zwevendekommagetal (\english{floating point}).
    Bij het vergelijken met de verwachte waarde zal de functie \mintinline{python}{math.isclose()}\footnote{Documentatie is hier te vinden: \url{https://docs.python.org/3/library/math.html\#math.isclose}} uit de standaardbibliotheek van Python gebruikt worden.
    Deze functie controleert of twee zwevendekommagetallen "dicht bij elkaar" liggen.
    De standaardfoutmarges van Python worden gebruikt.
    Een punt voor de toekomst is het configureerbaar maken van deze foutmarges.
    \item[\texttt{applyRounding}] Of zwevendekommagetallen afgrond moeten worden tijdens het vergelijken.
    Indien wel wordt het aantal cijfers genomen van de optie \texttt{roundTo}.
    Na de afronding worden ze ook vergeleken met de functie \mintinline{python}{math.isclose()}.
    Deze afronding is enkel van toepassing op het vergelijken, niet op de uitvoer.
    \item[\texttt{roundTo}] Het aantal cijfers na de komma.
    Enkel nuttig als \texttt{applyRounding} waar is.
\end{description}

Deze configuratieopties worden op het niveau van de testen meegegeven.
Dit laat toe om voor elke test (zelfs binnen eenzelfde testgeval) andere opties mee te geven.
Een nadeel is wel dat dezelfde opties mogelijk veel herhaald moeten worden, bijvoorbeeld als een bepaalde oefening een optie voor elke test wil instellen.
Echter wordt er verwacht dat dit soort zaken opgelost kunnen worden door een DSL of door het testplan te genereren.

Dit is de standaardevaluatievorm in het testplan als niets anders gegeven wordt.
\Cref{lst:testplan-text} toont een fragment uit een testplan: de uitvoerspecificatie van een testgeval waarbij de tekstevaluatie gebruikt wordt.

\begin{listing}
    \inputminted{json}{code/testplan-text.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van de standaarduitvoerstroom voor een testgeval toont, waarbij de tekstevaluatie gebruikt wordt.}
    \label{lst:testplan-text}
\end{listing}

\subsubsection{Bestandsevaluatie}

In deze evaluatievorm worden twee bestanden vergeleken met elkaar.
Hiervoor bevat het testplan enerzijds een pad naar een bestand die met de oefening gegeven wordt met de verwachte inhoud en anderzijds de naam (of pad) van de locatie waar het verwachte bestand zich moet bevinden.
De bestandsevaluatie ondersteunt enkel tekstuele bestanden, geen binaire bestanden.
Het vergelijken van de bestanden gebeurt op één dezer manieren:

\begin{description}
    \item[\texttt{exact}] Beide bestanden moet exact hetzelfde zijn, inclusief regeleindes.
    \item[\texttt{lines}] Elke regel wordt vergeleken met overeenkomstige regel in het andere bestand.
    De evaluatie van de lijnen is exact, maar zonder de regeleindes.
    Dit betekent dat de witruimte bijvoorbeeld ook moet overeenkomen.
    \item[\texttt{values}] Elke regel in het bestand wordt afzonderlijk vergeleken met de tekstevaluatie.
    Indien deze modus gebruikt wordt, kunnen ook alle opties van de tekstevaluatie meegegeven worden.
\end{description}

Een voorbeeld van hoe dit eruitziet is \cref{lst:testplan-file}.
In dit fragment wordt de modus \texttt{values} gebruikt, en worden de opties van de tekstevaluatie ook meegegeven.
Het bestand met de verwachte inhoud heeft als naam \texttt{bestand-uit-de-oefening.txt} gekregen, terwijl de ingediende oplossing een bestand moet schrijven naar \texttt{waar-het-verwachte-bestand-komt.txt}.
Beide paden zijn relatief, maar ten opzichte van andere mappen: het bestand met verwachte inhoud is relatief tegenover de map van de oefening, terwijl het pad waar de ingediende oplossing naar moet schrijven relatief is ten opzichte van de werkmap van de context waarin de oplossing wordt uitgevoerd (zie \cref{lst:mapstructuur} voor een overzicht van de structuur).

\begin{listing}
    \inputminted{json}{code/testplan-file.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van een bestand voor een testgeval toont, waarbij de bestandsevaluatie gebruikt wordt.}
    \label{lst:testplan-file}
\end{listing}

\subsubsection{Waarde-evaluatie}

Voor uitvoerkanalen zoals de returnwaarden moet meer dan alleen tekst met elkaar vergeleken kunnen worden.
Staat er in het testplan welke waarde verwacht wordt (geëncodeerd in het serialisatieformaat), dan kan TESTed dit vergelijken met de eigenlijke waarde die geproduceerd werd door de ingediende oplossing.

Het vergelijken van een waarde bestaat uit twee stappen:
\begin{enumerate}
    \item Het gegevenstype wordt vergeleken, waarbij beide waarden (de verwachte waarde uit het testplan en de geproduceerde waarde uit de ingediende oplossing) hetzelfde type moeten hebben.
    Hierbij wordt rekening gehouden met de vertalingen tussen de verschillende programmeertalen, waarbij twee gevallen onderscheiden kunnen worden:
    \begin{enumerate}
        \item Specifieert het testplan een basistype, dan zullen alle types die tot dit basistype herleid kunnen worden als hetzelfde beschouwd worden.
        Is de verwachte waarde bijvoorbeeld \texttt{sequence}, zullen ook \texttt{array}s uit Java en \texttt{tuple}s uit Python goedgekeurd worden.
        \item Specifieert het testplan een uitgebreid type, dan zal het uitgebreid type gebruikt worden voor talen die dat type ondersteunen, terwijl voor andere talen het basistype gebruikt zal worden.
        Stel dat het testplan bijvoorbeeld een waarde met als gegevenstype \texttt{tuple} heeft.
        In Python en Haskell (twee talen die dat gegevenstype ondersteunen) zullen enkel \texttt{tuple}s goedgekeurd worden.
        Voor andere talen, zoals Java, worden alle gegevenstypes goedgekeurd die herleidbaar zijn tot het basistype.
        Concreet zullen dus \texttt{List}s en \texttt{array}s goedgekeurd worden.
        Merk op dat momenteel bij collecties (\texttt{sequence}s, \texttt{set}s en \texttt{map}s) enkel het type van de collectie gecontroleerd wordt.
    \end{enumerate}
    \item De twee waarden worden vergeleken op inhoud (indien de vergelijking van de gegevenstypes uit de vorige stap positief is).
    Hierbij maakt TESTed gebruik van de ingebouwde vergelijking van Python om twee waarden te evalueren.
    Dit betekent dat de regels voor \english{value comparisons} uit Python\footnote{Zie \url{https://docs.python.org/3/reference/expressions.html?highlight=comparison\#value-comparisons}} gevolgd worden.
    Eén uitzondering is zwevendekommagetallen, waarvoor opnieuw \mintinline{python}{math.isclose()} gebruikt wordt in plaats van \mintinline{python}{==}.
\end{enumerate}

Bij deze evaluatievorm zijn geen configuratieopties.
Een voorbeeld van het gebruik binnen een testplan is \cref{lst:testplan-value}.
Hier wordt als returnwaarde een verzameling met drie elementen (5, 10 en 15) verwacht.

\begin{listing}
    \inputminted{json}{code/testplan-value.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van de returnwaarde voor een testgeval toont, waarbij de waarde-evaluatie gebruikt wordt.}
    \label{lst:testplan-value}
\end{listing}

\subsection{Geprogrammeerde evaluatie}\label{subsec:geprogrammeerde-evaluatie}

Bij oefeningen, zoals de voorbeeldoefening Lotto, met niet-deterministische resultaten, kunnen de verwachte waarden niet in het testplan komen.
Ook andere oefeningen waar geen directe vergelijking kan gemaakt worden, zoals het uitlijnen van sequenties (\english{sequence alignment}) uit de bio-informatica, volstaat een vergelijking met een verwachte waarde uit het testplan niet.

Toch is deze evaluatie niet programmeertaalafhankelijk: de logica om een sequentie uit te lijnen is dezelfde ongeacht de programmeertaal waarin dit gebeurt.
Voor dergelijke scenario's is geprogrammeerde evaluatie een oplossing: hierbij wordt code geschreven om de evaluatie te doen, maar deze evaluatiecode staat los van de ingediende oplossing en moet ook niet in dezelfde programmeertaal geschreven zijn.
Binnen TESTed wordt dit mogelijk gemaakt door geproduceerde waarden uit de ingediende oplossing te serialiseren bij het uitvoeren van de testcode, en terug te deserialiseren bij het uitvoeren van de evaluatiecode.

Deze evaluatiecode kan geschreven worden in een programmeertaal naar keuze, al moet de programmeertaal wel ondersteund worden door TESTed.
De implementatie volgt in alle programmeertalen hetzelfde stramien, maar de implementatiedetails kunnen verschillen.
In Python bestaat de evaluatiecode uit een module (een \texttt{.py}-bestand) met een functie die voldoet aan de definitie, zoals gegeven in \cref{lst:evaluation-python-custom}.
TESTed stelt ook een module \texttt{evaluation\_utils} ter beschikking.
De functie van hierboven moet dan één oproep doen naar de functie \texttt{evaluated()}.
Deze module is redelijk eenvoudig, zoals te zien in \cref{lst:evaluation-util-python}.

\begin{listing}
    \inputminted{python}{code/custom_signature.py}
    \caption{De definitie van de functie die aanwezig moet zijn in de evaluatiecode voor een geprogrammeerde evaluatie geschreven in Python.}
    \label{lst:evaluation-python-custom}
\end{listing}

\begin{listing}
    \inputminted{python}{../../judge/runners/templates/python/evaluation_utils.py}
    \caption{De implementatie van de module \texttt{evaluation\_utils}}
    \label{lst:evaluation-util-python}
\end{listing}

In de Java-implementatie is de situatie gelijkaardig: het gaat om het implementeren van een abstracte klasse, die ook dienst doet als de module van Python.
Deze klassen en haar ouder staan in \cref{lst:evaluation-util-java,lst:evaluation-java-custom}.

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractEvaluator}.}
    \label{lst:evaluation-util-java}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractCustomEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractCustomEvaluator}.}
    \label{lst:evaluation-java-custom}
\end{listing}


Een voorbeeld van een geprogrammeerde evaluatie is de voorbeeldoefening Lotto.
De evaluatiecode is gegeven in \cref{lst:evaluation-lotto}.

\begin{listing}
    \inputminted{python}{../../exercise/lotto/evaluation/evaluator.py}
    \caption{De evaluatiecode voor de geprogrammeerde evaluatie van de voorbeeldoefening Lotto.}
    \label{lst:evaluation-lotto}
\end{listing}

\subsection{Programmeertaalspecifieke evaluatie}\label{subsec:programmeertaalspecifieke-evaluatie}

In sommige scenario's moeten programmeertaalspecifieke concepten beoordeeld worden.
Een mogelijkheid is deze oefeningen niet aanbieden in TESTed, maar in de programmeertaalspecifieke judges.
Toch zijn er nog voordelen om ook deze oefeningen in TESTed aan te bieden:
\begin{itemize}
    \item Het bijkomende werk om meer programmeertalen te ondersteunen beperkt zich tot een minimum.
    \item Het werk om een nieuwe programmeertaal toe te voegen aan TESTed is kleiner dan een volledig nieuwe judge te implementeren.
\end{itemize}
Het is desalniettemin het vermelden waard dat het niet zeker is of deze evaluatiemethode (en dit scenario meer algemeen) veel zal voorkomen.
Oefeningen, die programmeertaalspecifieke aspecten moeten beoordelen, zijn, net door hun programmeertaalspecifieke aard, moeilijker aan te bieden in meerdere programmeertalen.
Een oefening in de programmeertaal C die bijvoorbeeld beoordeelt op juist gebruik van pointers zal weinig nut hebben in Python.

In gebruik lijkt de programmeertaalspecifieke evaluatie sterk op de geprogrammeerde evaluatie, met dat verschil dat het testplan niet evaluatiecode in één programmeertaal bevat, maar evaluatiecode in alle programmeertalen waarin de oefening aangeboden wordt, zoals geïllustreerd in X.
Als de programmeertaalspecifieke evaluatie gebruikt wordt en er wordt geen evaluatiecode voor een bepaalde programmeertaal, zal de oefening niet opgelost kunnen worden in die programmeertaal.

Ook de implementatie lijkt op de geprogrammeerde evaluatie, zij het dat de te implementeren functie afwijkt.
In Python wordt dit \cref{lst:evaluation-python-specific}, in Java \cref{lst:evaluation-java-specific}.
Om het resultaat van de evaluatie aan de judge te geven, wordt dezelfde \texttt{evaluated}-functie als bij de aangepaste evaluator gebruikt (zie \cref{lst:evaluation-util-python,lst:evaluation-util-java})

\begin{listing}
    \inputminted{python}{code/specific_signature.py}
    \caption{De definitie van de functie die aanwezig moet zijn in de evaluatiecode voor een programmeertaalspecifieke evaluatie geschreven in Python.}
    \label{lst:evaluation-python-specific}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractSpecificEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractSpecificEvaluator}.}
    \label{lst:evaluation-java-specific}
\end{listing}
