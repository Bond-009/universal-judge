\chapter{TESTed}\label{ch:tested}

\lettrine{I}{n het kader} van deze masterproef werd een prototype geïmplementeerd van een judge voor Dodona.
Het doel hiervan is een antwoord te bieden aan de onderzoeksvraag uit het vorige hoofdstuk en de beperkingen van deze aanpak in kaart te brengen.
Deze judge heeft de naam \term{TESTed} gekregen.
Bij TESTed is een oefening programmeertaalonafhankelijk en kunnen oplossingen in verschillende programmeertalen beoordeeld worden aan de hand van een en dezelfde specificatie.
Dit hoofdstuk begint met het ontwerp en de algemene werking van de judge toe te lichten, waarna elk onderdeel in meer detail besproken wordt.

\section{Overzicht}\label{sec:ontwerp}

\subsection{Architecturaal ontwerp}\label{subsec:architecturaal-overzicht}

\Cref{fig:universal-judge} toont het architecturaal ontwerp van TESTed.
De twee stippellijnen geven programmeertaalbarrières aan, en verdelen TESTed in drie logisch omgevingen:

\begin{enumerate}
    \item TESTed zelf is geschreven in Python: in het middelste deel staat de programmeertaal dus vast.
    Dit onderdeel is verantwoordelijk voor de regie van de beoordeling op basis van het testplan.
    \item De ingediende oplossing wordt uitgevoerd in de \term{uitvoeringsomgeving}, waar de programmeertaal overeenkomt met de programmeertaal van de oplossing.
    \item Tot slot is er nog de \term{evaluatieomgeving}, waar door de lesgever geschreven evaluatiecode wordt uitgevoerd.
    Deze moet niet in dezelfde programmeertaal als de oplossing of TESTed geschreven zijn.
\end{enumerate}

\subsection{Stappenplan van een beoordeling}\label{subsec:stappenplan-van-een-beoordeling}

De rest van het hoofdstuk bespreekt alle onderdelen van en stappen die gebeuren bij een beoordeling van een ingediende oplossing in detail.
In \cref{fig:tested-flow} zijn deze stappen gegeven als een flowchart, en een uitgeschreven versie volgt:

\begin{enumerate}
    \item De Docker-container voor TESTed wordt gestart.
    Dodona stelt de invoer ter beschikking aan de container: het testplan komt uit de oefening, terwijl de ingediende oplossing en de configuratie uit Dodona komen.
    \item Als eerste stap wordt gecontroleerd dat het testplan de programmeertaal van de ingediende oplossing ondersteunt.
    De programmeertaal van de oplossing wordt gegeven via de configuratie uit Dodona.
    Merk op dat de ingediende oplossing zelf hierbij niet nodig is: deze controle zou idealiter gebeuren bij het importeren van de oefening in Dodona, zodat Dodona weet in welke programmeertalen een bepaalde oefening aangeboden kan worden (zie \cref{ch:beperkingen-en-toekomstig-werk}).
    Als het testplan bijvoorbeeld programmeertaalspecifieke code bevat die enkel in Java geschreven is, zal een oplossing in Python niet beoordeeld kunnen worden.
    Bevat het testplan bijvoorbeeld een functie die een verzameling moet teruggeven, dan zullen talen als Bash niet in aanmerking komen.
    \item Het testplan (details in \cref{subsec:het-testplan}) bestaat uit verschillende contexten.
    Elke context is een onafhankelijke uitvoering van de ingediende oplossing en kan verschillende aspecten van die uitvoering beoordelen.
    Voor elk van die contexten wordt in deze stap de testcode gegenereerd.
    Deze stap is de overgang naar de \term{uitvoeringsomgeving}.
    \item De testcode wordt optioneel gecompileerd.
    Dit kan op twee manieren gebeuren (details in \cref{subsec:testcode-genereren}):
    \begin{enumerate}
        \item Batchcompilatie: hierbij wordt de testcode van alle contexten verzameld en gecompileerd tot één uitvoerbaar bestand (executable).
        Dit heeft als voordeel dat er slechts een keer een compilatie nodig is, wat voor een betere performantie zorgt.
        Bij deze manier resulteert de compilatiestap in één uitvoerbaar bestand.
        \item Contextcompilatie: hierbij wordt de testcode voor elke context afzonderlijk gecompileerd tot een uitvoerbaar bestand.
        Bij deze manier worden er $n$ uitvoerbare bestanden geproduceerd tijdens de compilatiestap.
    \end{enumerate}
    In talen die geen compilatie nodig hebben of ondersteunen, wordt deze stap overgeslagen.
    \item Nu kan het uitvoeren van de beoordeling zelf beginnen: de gegenereerde code wordt uitgevoerd (nog steeds in de uitvoeringsomgeving).
    Elke context uit het testplan wordt in een afzonderlijk subproces uitgevoerd, teneinde het delen van informatie tegen te gaan.
    \item De uitvoering van de executable in de vorige stap produceert resultaten (voor elke context), zoals de standaarduitvoerstroom, de standaardfoutstroom, returnwaardes, exceptions of exitcodes.
    Deze bundel resultaten wordt nu geëvalueerd op juistheid.
    Hiervoor zijn drie mogelijke manieren:
    \begin{enumerate}
        \item Programmeertaalspecifieke evaluatie (afgekort tot SE in de flowchart).
        De code voor de evaluatie is opgenomen in de executable en wordt onmiddellijk uitgevoerd in hetzelfde proces.
        Via deze mogelijkheid kunnen taalspecifieke aspecten gecontroleerd worden.
        Daar de evaluatie in hetzelfde proces gebeurt, blijft dit in de uitvoeringsomgeving.
        \item Geprogrammeerde evaluatie (afgekort tot PE in de flowchart).
        Hierbij is er evaluatiecode geschreven die los staat van de oplossing, waardoor deze evaluatiecode ook in een andere programmeertaal geschreven kan zijn.
        De code ter uitvoering van de geprogrammeerde evaluatiecode wordt gegenereerd en dan uitgevoerd.
        Het doel van deze modus is om complexe evaluaties toe te laten op een programmeertaalonafhankelijke manier.
        Deze stap vindt plaats in de evaluatieomgeving.
        \item Generieke evaluatie.
        Hierbij evalueert TESTed zelf het resultaat.
        Deze modus is bedoeld voor gestandaardiseerde evaluaties, zoals het vergelijken van geproduceerde uitvoer en verwachte uitvoer.
        Hier gebeurt de evaluatie binnen TESTed zelf.
    \end{enumerate}
    \item Tot slot verzamelt TESTed alle evaluatieresultaten en stuurt ze gebundeld door naar Dodona, waarna ze getoond worden aan de gebruiker.
\end{enumerate}

\begin{figure}
    \centering
    \input{figures/architecture.tex}
    \caption{Schematische voorstelling van het architecturale ontwerp van de TESTed.}
    \label{fig:universal-judge}
\end{figure}

\begin{figure}
    \centering
    \input{figures/flow.tex}
    \caption{
        Flowchart van een beoordeling door TESTed.
        In het schema worden kleuren gebruikt als er een keuze gemaakt moet worden voor een volgende stap.
        Er kan steeds slechts één mogelijkheid gekozen worden.
        De afkortingen PE, GE en SE staan respectievelijk voor geprogrammeerde evaluatie, generieke evaluatie en (programmeertaal)specifieke evaluatie.
    }
    \label{fig:tested-flow}
\end{figure}


\section{Beschrijven van een oefening}\label{sec:testplan}

De beoordeling van een ingediende oplossing van een oefening begint bij de invoer die TESTed krijgt.
Centraal in deze invoer is een \term{testplan}, een specificatie die op een programmeertaalonafhankelijke manier beschrijft hoe een oplossing voor een oefening beoordeeld moet worden.
Het vervangt de taalspecifieke testen van de bestaande judges (ie.\ de jUnit-tests of de doctests in respectievelijk Java en Python).
Het testplan \latin{sensu lato} wordt opgedeeld in verschillende onderdelen, die hierna besproken worden.

\subsection{Het testplan}\label{subsec:het-testplan}

Het testplan \latin{sensu stricto} beschrijft de structuur van de beoordeling van een ingediende oplossing voor een oefening.
Deze structuur lijkt qua opbouw sterk op de structuur van de feedback zoals gebruikt door Dodona.
Dat de structuur van de oplossing in Dodona en van het testplan op elkaar lijken, heeft als voordeel dat er geen mentale afbeelding moet gemaakt worden tussen de structuur van het testplan en dat van Dodona.
Concreet is de structuur een hiërarchie met volgende elementen:

\begin{description}
    \item[Plan] Het top-level object van het testplan.
    Dit object bevat twee belangrijke objecten: de tabbladen en de configuratie.
    Deze configuratie is de plaats om opties aan TESTed mee te geven.
    \item[Tab] Een testplan bestaat uit verschillende \termen{tab}s of tabbladen.
    Deze komen overeen met de tabbladen in de gebruikersinterface van Dodona.
    Een tabblad kan een naam hebben, die zichtbaar is voor de gebruikers.
    \item[Context] Elk tabblad bestaat uit een of meerdere \termen{context}en.
    Een context is een onafhankelijke uitvoering van een evaluatie.
    De nadruk ligt op de "onafhankelijkheid", zoals al vermeld.
    Elke context wordt in een nieuw proces en in een eigen map (directory) uitgevoerd, zodat de kans op het delen van informatie klein is.
    Hierbij willen we vooral onbedoeld delen van informatie (zoals statische variabelen of het overschrijven van bestanden) vermijden.
    De gemotiveerde student zal nog steeds informatie kunnen delen tussen de uitvoeringen, door bv.\ in een andere locatie een bestand aan te maken en later te lezen.
    \item[Testcase] Een context bestaat uit een of meerdere \termen{testcase}s of testgevallen.
    Een testgeval bestaat uit invoer en een aantal tests.
    De testgevallen kunnen onderverdeeld worden in twee soorten:
    \begin{description}
        \item[Main testcase] of hoofdtestgeval.
        Van deze soort is er maximaal één per context (geen hoofdtestgeval is ook mogelijk).
        Dit testgeval heeft als doel het uitvoeren van de main-functie (of de code zelf als het gaat om een scripttaal zoals Bash of Python).
        Als invoer voor dit testgeval kunnen enkel de standaardinvoerstroom en de programma-argumenten meegegeven worden.
        De exitcode van een uitvoering kan ook enkel in het hoofdtestgeval gecontroleerd worden.
        \item[Normal testcase] of normaal testgeval.
        Hiervan kunnen er nul of meer zijn per context.
        Deze testgevallen dienen om andere aspecten van de ingediende oplossing te testen, nadat de code van de gebruiker met success ingeladen is.
        De invoer is dan ook uitgebreider: het kan gaan om het standaardinvoerkanaal, functieoproepen en variabeletoekenningen.
        Een functieoproep of variabeletoekenning is verplicht (zonder functieoproep of toekenning aan een variabele is er geen code om te testen).
    \end{description}
    Het hoofdtestgeval wordt altijd als eerste uitgevoerd.
    Dit is verplicht omdat bepaalde programmeertalen (zoals Python en andere scripttalen) de code onmiddellijk uitvoeren bij het inladen.
    Om te vermijden dat de volgorde van de testgevallen zou verschillen tussen de programmeertalen, wordt het hoofdtestgeval altijd eerst uitgevoerd.
    \item[Test] De beoordeling van een testgeval bestaat uit meerdere \term{test}s, die elk één aspect van het testgeval controleren.
    Met aspect bedoelen we de standaarduitvoerstroom, de standaardfoutstroom, opgevangen uitzonderingen (\english{exceptions}), de teruggegeven waarden van een functieoproep (returnwaarden) of de inhoud van een bestand.
    De exitcode is ook mogelijk, maar enkel in het hoofdtestgeval.
    Het beoordelen van de verschillende aspecten wordt in meer detail beschreven in \cref{sec:oplossingen-beoordelen}
\end{description}

Bij de keuze voor een formaat voor het testplan (\acronym{JSON}, \acronym{XML}, \ldots), zijn vooraf enkele vereisten geformuleerd waaraan het gekozen formaat moet voldoen.
Het moet:

\begin{itemize}
    \item leesbaar zijn voor mensen,
    \item geschreven kunnen worden met minimale inspanning, met andere woorden de syntaxis dient eenvoudig te zijn, en
    \item programmeertaalonafhankelijk zijn.
\end{itemize}

Uiteindelijk is gekozen om het testplan op te stellen in \acronym{JSON}.
Niet alleen voldoet \acronym{JSON} aan de vooropgestelde voorwaarden, het wordt ook door veel talen ondersteund.

Toch zijn er ook enkele nadelen aan het gebruik van \acronym{JSON}.
Zo is \acronym{JSON} geen beknopte of compacte taal om met de hand te schrijven.
Een oplossing hiervoor gebruikt de eigenschap dat veel talen \acronym{JSON} kunnen produceren: andere programma's kunnen desgewenst het testplan in het json-formaat genereren, waardoor het niet met de hand geschreven moet worden.
Hiervoor denken we aan een \acronym{DSL} (\english{domain specific language}), maar dit valt buiten de thesis en wordt verder besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Een tweede nadeel is dat \acronym{JSON} geen programmeertaal is.
Terwijl dit de implementatie van de judge bij het interpreteren van het testplan weliswaar eenvoudiger maakt, is het tevens beperkend: beslissen of een testgeval moet uitgevoerd worden op basis van het resultaat van een vorig testgeval is bijvoorbeeld niet mogelijk.
Ook deze beperking wordt uitgebreider besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Tot slot bevat \cref{lst:testplan} een testplan met één context voor de voorbeeldoefening Lotto uit \cref{ch:dodona}.

\begin{listing}
    \inputminted{python}{code/testplan.json}
    \caption{
        Een ingekorte versie van het testplan voor de voorbeeldoefening Lotto.
        Het testplan bevat maar één context.
    }
    \label{lst:testplan}
\end{listing}

\subsection{Dataserialisatie}\label{subsec:dataserialisatie}

Bij de beschrijving van het testplan wordt gewag gemaakt van returnwaarden en variabeletoekenningen.
Aangezien het testplan programmeertaalonafhankelijk is, moet er dus een manier zijn om data uit de verschillende programmeertalen voor te stellen en te vertalen: het \term{serialisatieformaat}.

\subsubsection{Keuze van het formaat}

Zoals bij het testplan, werd voor de voorstelling van waarden ook een keuze voor een bepaald formaat gemaakt.
Daarvoor werden opnieuw enkele voorwaarden vooropgesteld, waaraan het serialisatieformaat moet voldoen.
Het formaat moet:

\begin{itemize}
    \item door mensen geschreven kunnen worden (\english{human writable}),
    \item onderdeel van het testplan kunnen zijn,
    \item in meerdere programmeertalen bruikbaar zijn, en
    \item de basisgegevenstypes ondersteunen die we willen aanbieden in het programmeertaalonafhankelijke deel van het testplan.
    Deze gegevenstypes zijn:
    \begin{itemize}
        \item Primitieven: gehele getallen, reële getallen, Boolese waarden en tekenreeksen.
        \item Collecties: rijen (eindige, geordende reeks; \texttt{list} of \texttt{array}), verzamelingen (eindige, ongeordende reeks zonder herhaling; \texttt{set}) en afbeeldingen (elk element wordt afgebeeld op een ander element; \texttt{map}, \texttt{dict} of \texttt{object}).
    \end{itemize}
\end{itemize}

Een voor de hand liggende oplossing is om ook hiervoor \acronym{JSON} te gebruiken, en zelf in \acronym{JSON} een structuur op te stellen voor de waarden.
In tegenstelling tot de situatie bij het testplan bestaan er al een resem aan dataserialisatieformaten, waardoor het de moeite loont om na te gaan of er geen bestaand formaat voldoet aan de vereisten.
Hiervoor is gestart van een overzicht op Wikipedia \autocite{wiki2020}.
Uiteindelijk is niet gekozen voor een bestaand formaat, maar voor de \acronym{JSON}-oplossing.
De redenen hiervoor zijn samen te vatten als:

\begin{itemize}
    \item Het gaat om een binair formaat.
    Binaire formaten zijn uitgesloten op basis van de eerste twee voorwaarden die we opgesteld hebben: mensen kunnen het niet schrijven zonder hulp van bijkomende tools en het is moeilijk in te bedden in een \acronym{JSON}-bestand (zonder gebruik te maken van encoderingen zoals base64).
    Bovendien zijn binaire formaten moeilijker te implementeren in sommige talen.
    \item Het formaat ondersteunt niet alle gewenste types.
    Sommige formaten hebben ondersteuning voor complexere datatypes, maar niet voor alle complexere datatypes die wij nodig hebben.
    Uiteraard kunnen de eigen types samengesteld worden uit basistypes, maar dan biedt de ondersteuning voor de complexere types weinig voordeel, aangezien er toch een eigen dataschema voor die complexere types opgesteld zal moeten worden.
    \item Sommige formaten zijn omslachtig in gebruik.
    Vaak ondersteunen dit soort formaten meer dan wat wij nodig hebben.
    \item Het formaat is niet eenvoudig te implementeren in een programmeertaal waarvoor geen ondersteuning is.
    Sommige dezer formaten ondersteunen weliswaar veel talen, maar we willen niet dat het serialisatieformaat een beperkende factor wordt in welke talen door de judge ondersteund worden.
    Het mag niet de bedoeling zijn dat het implementeren van het serialisatieformaat het meeste tijd in beslag neemt.
\end{itemize}

Een lijst van de overwogen formaten met een korte beschrijving:

\begin{description}
    \item[Apache Avro] Een volledig "systeem voor dataserialisatie".
    De specificatie van het formaat gebeurt in \acronym{JSON} (vergelijkbaar met \acronym{JSON} Schema), terwijl de eigenlijke data binair geëncodeerd wordt.
    Heeft uitbreidbare types, met veel ingebouwde types \autocite{avro}.
    \item[Apache Parquet] Minder relevant, dit is een bestandsformaat voor Hadoop \autocite{parquet}.
    \item[\acronym{ASN}.1] Staat voor \english{Abstract Syntax Notation One}, een formaat uit de telecommunicatie.
    De hoofdstandaard beschrijft enkel de notatie voor een dataformaat.
    Andere standaarden beschrijven dan de serialisatie, bijvoorbeeld een binair formaat, \acronym{JSON} of \acronym{XML}.
    De meerdere serialisatievormen zijn in theorie aantrekkelijk: elke taal moet er slechts een ondersteunen, terwijl de judge ze allemaal kan ondersteunen.
    In de praktijk blijkt echter dat voor veel talen er slechts één serialisatieformaat is, en dat dit vaak het binaire formaat is \autocite{x680}.
    \item[Bencode] Schema gebruikt in BitTorrent.
    Het is gedeeltelijk binair, gedeeltelijk in text \autocite{cohen2017}.
    \item[Binn] Binair dataformaat \autocite{ramos2019}.
    \item[\acronym{BSON}] Een binaire variant op \acronym{JSON}, geschreven voor en door MongoDB \autocite{bson}.
    \item[\acronym{CBOR}] Een lichtjes op \acronym{JSON} gebaseerd formaat, ook binair.
    Heeft een goede standaard, ondersteunt redelijk wat talen \autocite{rfc7049}.
    \item[FlatBuffers] Lijkt op ProtocolBuffers, allebei geschreven door Google, maar verschilt wat in implementatie van ProtocolBuffers.
    De encodering is binair \autocite{flatbuffers}.
    \item[Fast Infoset] Is eigenlijk een manier om \acronym{XML} binair te encoderen (te beschouwen als een soort compressie voor xml), waardoor het minder geschikt voor ons gebruik wordt \autocite{x981}.
    \item[Ion] Een superset van \acronym{JSON}, ontwikkeld door Amazon.
    Het heeft zowel een tekstuele als binaire voorstelling.
    Naast de gebruikelijke \acronym{JSON}-types, bevat het enkele uitbreidingen. \autocite{ion}.
    \item[MessagePack] Nog een binair formaat dat lichtjes op \acronym{JSON} gebaseerd is.
    Lijkt qua types sterk op \acronym{JSON}.
    Heeft implementaties in veel talen \autocite{messagepack}.
    \item[\acronym{OGDL}] Afkorting voor \english{Ordered Graph Data Language}.
    Daar het om een serialisatieformaat voor grafen gaat, is het niet nuttig voor ons doel \autocite{ogdl}.
    \item[\acronym{OPC} Unified Architecture] Een protocol voor intermachinecommunicatie.
    Complex: de specificatie bevat 14 documenten, met ongeveer 1250 pagina's \autocite{tr62541}.
    \item[Open\acronym{DLL}] Afkorting voor de \english{Open Data Description Language}.
    Een tekstueel formaat, bedoeld om arbitraire data voor te stellen.
    Wordt niet ondersteund in veel programmeertalen, in vergelijking met bijvoorbeeld \acronym{JSON} \autocite{openddl}.
    \item[ProtocolBuffers] Lijkt zoals vermeld sterk op FlatBuffers, maar heeft nog extra stappen nodig bij het encoderen en decoderen, wat het minder geschikt maakt \autocite{protobuf}.
    \item[Smile] Nog een binaire variant van \acronym{JSON} \autocite{smile}.
    \item[\acronym{SOAP}] Afkorting voor \english{Simple Object Access Protocol}.
    Niet bedoeld als formaat voor dataserialisatie, maar voor communicatie tussen systemen over een netwerk \autocite{soap}.
    \item[\acronym{SDXF}] Binair formaat voor data-uitwisseling.
    Weinig talen ondersteunen dit formaat \autocite{rfc3072}.
    \item[Thrift] Lijkt sterk op ProtocolBuffers, maar geschreven door Facebook \autocite{slee2007}.
    \item[\acronym{UBJSON}] Nog een binaire variant van \acronym{JSON} \autocite{ubjson}.

\end{description}

Geen enkel overwogen formaat heeft grote voordelen tegenover een eigen structuur in \acronym{JSON}.
Daarenboven hebben veel talen het nadeel dat ze geen \acronym{JSON} zijn, waardoor we een nieuwe taal moeten inbedden in het bestaande \acronym{JSON}-testplan.
Dit nadeel, gekoppeld met het ontbreken van voordelen, heeft geleid tot de keuze voor \acronym{JSON}.

\subsubsection{Dataschema}

Json is slechts een formaat en geeft geen semantische betekenis aan \acronym{JSON}-elementen.
Hiervoor stellen we een dataschema op, dat uit twee onderdelen bestaat:

\begin{itemize}
    \item Het encoderen van waarden.
    \item Het beschrijven van de gegevenstypes van deze waarden.
\end{itemize}

Elke waarde wordt in het serialisatieformaat voorgesteld als een object met twee elementen: de geëncodeerde waarde en het bijhorende gegevenstype.
Een concreet voorbeeld is \cref{lst:serialisation}.

\begin{listing}
    \inputminted{json}{code/format.json}
    \caption{Een lijst bestaande uit twee getallen, geëncodeerd in het serialisatieformaat.}
    \label{lst:serialisation}
\end{listing}

Het encoderen van waarden slaat op het voorstellen van waarden als \acronym{JSON}-waarden.
Json heeft slechts een beperkt aantal gegevenstypes, dus worden alle waarden voorgesteld als een van deze types.
Zo worden bijvoorbeeld zowel \texttt{array}s en \texttt{set}s voorgesteld als een \acronym{JSON}-lijst.

Het verschil tussen beiden wordt dan duidelijk gemaakt door het bijhorende gegevenstype.
Er is dus nood aan een systeem om aan te geven wat het gegevenstype van een waarde is.

\Cref{lst:type-schema} bevat het onder andere de structuur van een waarde in het serialisatieformaat, in een vereenvoudigde versie van \acronym{JSON} Schema.
Hierbij staat \texttt{<types>} voor een van de gegevenstypes die hierna besproken werden.

\begin{listing}
    \inputminted{json}{code/type-schema.json}
    \caption{Het schema voor waarden, expressies en statements, in een vereenvoudigde versie van \acronym{JSON} Schema.}
    \label{lst:type-schema}
\end{listing}

\subsubsection{Gegevenstypes}

Het systeem om de gegevenstypes aan te duiden vervult meerdere functies.
Het wordt gebruikt om:

\begin{itemize}
    \item het gegevenstype van concrete data aan te duiden (beschrijvende modus).
    Dit gaat om de serialisatie van waarden uit de uitvoeringsomgeving naar TESTed, zoals het geval is bij returnwaarden van functies.
    \item te beschrijven welk gegevenstype verwacht wordt (voorschrijvende modus).
    Een voorbeeld hiervan is het aangeven van het gegevenstype van een variabele.
    \item zelf code te schrijven (letterlijke modus).
    Dit gaat om serialisatie vanuit het testplan zelf naar de uitvoeringsomgeving.
    Een voorbeeld hiervan is het opnemen van functieargumenten in het testplan: deze argumenten worden tijdens de serialisatie omgezet naar echte code.
\end{itemize}

Bij het ontwerp van het systeem voor de gegevenstypes zorgen deze verschillende functies soms voor tegenstrijdige belangen: voor het beschrijven van een waarde moet het systeem zo eenvoudig mogelijk zijn.
Een waarde met bijhorend gegevenstype \texttt{union[string, int]} is niet bijster nuttig: een waarde kan nooit tegelijk een \texttt{string} en een \texttt{int} zijn.
Aan de andere kant zijn dit soort complexe gegevenstypes wel nuttig bij het aangeven van het verwachte gegevenstype van bijvoorbeeld een variabele.
Daarnaast moet ook rekening gehouden worden met het feit dat deze gegevenstypes in veel programmeertalen implementeerbaar moeten zijn.
Een gegevenstype als \texttt{union[string, int]} is eenvoudig te implementeren in Python, maar dat is niet het geval in bijvoorbeeld Java of C\@.
Ook heeft elke programmeertaal een eigen niveau van details bij gegevenstypes.
Python heeft bijvoorbeeld enkel \texttt{integer} voor gehele getallen, terwijl C beschikt over \texttt{int}, \texttt{unsigned}, \texttt{long}, enz.
Daarenboven heeft het schrijven van code bijkomende vereisten: als functieargument zijn waarden alleen niet voldoende, ook andere variabelen moeten gerefereerd kunnen worden en het resultaat van andere functieoproepen moeten ook als argument gebruikt kunnen worden.

Om deze redenen zijn de gegevenstypes opgedeeld in drie categorieën:

\begin{enumerate}
    \item De basistypes.
    Deze gegevenstypes zijn bruikbaar in alle modi.
    De lijst van basistypes omvat:
    \begin{description}
        \item[\texttt{integer}] Gehele getallen, zowel positief als negatief.
        \item[\texttt{rational}] Rationale getallen.
        Het gaat hier om \texttt{float}s, die ook vaak gebruikt worden als benadering van gehele getallen.
        \item[\texttt{text}] Een tekenreeks of string (alle vormen).
        \item[\texttt{char}] Een enkel teken.
        \item[\texttt{boolean}] Een Boolese waarde (of boolean).
        \item[\texttt{sequence}] Een wiskundige rij, wat wil zeggen dat de volgorde belangrijk is en dat dubbele elementen toegelaten zijn.
        \item[\texttt{set}] Een wiskundige verzameling, wat wil zeggen dat de volgorde niet belangrijk is en dat dubbele elementen niet toegelaten zijn.
        \item[\texttt{map}] Een wiskundige afbeelding: elk element wordt afgebeeld op een ander element.
        In Java is dit bijvoorbeeld een \texttt{Map}, in Python een \texttt{dict} en in Javascript een \texttt{object}.
        \item[\texttt{nothing}] Geeft aan dat er geen waarde is, ook wel \texttt{null}, \texttt{None} of \texttt{nil} genoemd.
    \end{description}
    Een lijst van de implementaties in de verschillende programmeertalen is \cref{tab:basistypes}.
    Elke implementatie van een programmeertaal moet een keuze maken wat de standaardimplementatie van deze types is.
    Zo implementeert de Java-implementatie het gegevenstype \texttt{sequence} als een \texttt{List<>}, niet als een \texttt{array}.
    Een implementatie in een programmeertaal kan ook aangeven dat een bepaald type niet ondersteund wordt, waardoor testplannen met dat type niet zullen werken.
    \item De uitgebreide types: dit zijn een hele reeks bijkomende types.
    Deze gegevenstypes staan toe om meer details over de types te serialiseren en in het testplan op te nemen.
    Een voorbeeld is de lijst van types in \cref{tab:vertaling}, die voor een reeks gegevenstypes voor gehele getallen de concrete types in verschillende programmeertalen geeft.
    Het grote verschil is dat deze uitgebreide types standaard vertaald worden naar een van de basistypes.
    Voor talen die bijvoorbeeld geen \texttt{tuple} uit Python ondersteunen, zal het type omgezet worden naar \texttt{list}.
    Er is ook de mogelijk dat implementaties voor programmeertalen expliciet een bepaald type niet ondersteunen.
    Zo zal de Java-implementatie geen \texttt{uint64} (een unsigned 64-bit integer) ondersteunen, omdat er geen equivalent bestaat in de taal\footnote{Dit is slechts ter illustratie: in de implementatie van TESTed wordt \texttt{BigInteger} gebruikt.}.
    \item Voorschrijvende types.
    Gegevenstypes in deze categorie kunnen enkel gebruikt worden bij het aangeven welk gegevenstype verwacht wordt, niet bij de eigenlijke encodering van waarden.
    In de praktijk gaat het om het type van variabelen.
    In deze categorie zouden gegevenstypes als \texttt{union[str, int]} komen.
    Er is echter expliciet gekozen om dit soort types niet te ondersteunen, door de moeilijkheid om dit te implementeren in statisch getypeerde talen, zoals Java of C\@.
    Twee types die wel ondersteund worden in deze modus zijn:
    \begin{description}
        \item[any] Het \texttt{any}-type geeft aan dat het type van een variabele onbekend is.
        Merk op dat dit in sommige talen tot moeilijkheden zal leiden: zo zal dit in C-code als \texttt{long} beschouwd worden (want C heeft geen equivalent van een \texttt{any}-type).
        \item[custom] Een eigen type, waarbij de naam van het type gegeven wordt.
        Dit is nuttig om bijvoorbeeld variabelen aan te maken met als gegevenstype een eigen klasse, zoals een klasse die de student moest implementeren.
    \end{description}
\end{enumerate}

\begin{table}
    \centering
    \caption{Implementaties van de basistypes in de verschillende programmeertalen.}
    \label{tab:basistypes}
    \begin{tabular}{|l|lll|}
        \hline
        Type              & Python          & Java              & Haskell           \\
        \hline
        \texttt{integer}  & \texttt{int}    & \texttt{long}     & \texttt{Integer}  \\
        \texttt{rational} & \texttt{float}  & \texttt{double}   & \texttt{Double}   \\
        \texttt{text}     & \texttt{str}    & \texttt{String}   & \texttt{String}   \\
        \texttt{char}     & \texttt{str}    & \texttt{char}     & \texttt{Char}     \\
        \texttt{boolean}  & \texttt{bool}   & \texttt{boolean}  & \texttt{Boolean}  \\
        \texttt{sequence} & \texttt{list}   & \texttt{List<>}   & \texttt{List}     \\
        \texttt{set}      & \texttt{set}    & \texttt{Set<>}    & -        \\
        \texttt{map}      & \texttt{dict}   & \texttt{Map<>}    & -        \\
        \texttt{nothing}  & \texttt{None}   & \texttt{null}     & \texttt{Nothing}  \\
        \hline
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Voorbeeld van de implementatie van types voor gehele getallen, met als basistype \texttt{integer}.}
    \label{tab:vertaling}
    \begin{threeparttable}
        \begin{tabular}{|l|llllllll|}
            \hline
                       & \texttt{int8} & \texttt{uint8} & \texttt{int16} & \texttt{uint16} & \texttt{int32} & \texttt{uint32} & \texttt{int64} & \texttt{uint64} \\
            \hline
            Python     & \texttt{int}  & \texttt{int}   & \texttt{int}   & \texttt{int}    & \texttt{int}   & \texttt{int}    & \texttt{int}   & \texttt{int}    \\
            Java       & \texttt{byte} & \texttt{short} & \texttt{short} & \texttt{int}    & \texttt{int}   & \texttt{long}   & \texttt{long}  & -               \\
            C\tnote{1} & \texttt{int8\_t} & \texttt{uint8\_t} & \texttt{int16\_t} & \texttt{uint16\_t} & \texttt{int32\_t} & \texttt{uint32\_t} & \texttt{int64\_t} & \texttt{uint64\_t} \\
            Haskell    & \texttt{Int8} & \texttt{Word8} & \texttt{Int16} & \texttt{Word16} & \texttt{Int32} & \texttt{Word32} & \texttt{Int64} & \texttt{Word64} \\
            \hline
        \end{tabular}
    \begin{tablenotes}
        \item[1] Uiteraard met de gebruikelijke aliassen van \texttt{short}, \texttt{unsigned}, \ldots
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsection{Expressions en statements}\label{subsec:expressions-and-statements}

Een ander onderdeel van het testplan verdient ook speciale aandacht: toekennen van waarden aan variabelen (\english{assignments}) en functieoproepen.

In heel wat oefeningen, en zeker bij objectgerichte en imperatieve programmeertalen, is het toekennen van een waarde aan een variabele, om deze later te gebruiken, onmisbaar.
Bijvoorbeeld zou een opgave kunnen bestaan uit het implementeren van een klasse.
Bij de evaluatie dient dan een instantie van die klasse aangemaakt te worden, waarna er methoden kunnen aangeroepen worden, zoals hieronder geïllustreerd in een fictief voorbeeld.

\inputminted{java}{code/assignment.jshell}

Om deze reden is het testplan uitgebreid met ondersteuning voor statements en expressies.
Toch moet meteen opgemerkt worden dan deze ondersteuning beperkt is tot wat er nodig is om het scenario van hiervoor te kunnen uitvoeren;
het is zeker niet de bedoeling om een volledige eigen programmeertaal te ontwerpen.

In \cref{lst:type-schema} staat onder andere het formaat van een expressie en een functieoproep in een vereenvoudigde versie van \acronym{JSON} Schema.
Een expressie is een dezer drie dingen:
\begin{enumerate}
    \item Een waarde, zoals hiervoor besproken in subparagraaf \emph{Dataschema} van \cref{subsec:dataserialisatie}.
    \item Een \texttt{identifier}, voorgesteld als een string.
    \item Een functieoproep, die bestaat uit:
    \begin{description}
        \item[\texttt{type}] Het soort functie.
        Kan een van deze waarden zijn:
        \begin{description}
            \item[\texttt{function}] Een \english{top-level} functie.
            Afhankelijk van de programmeertaal zal deze functie toch omgezet worden naar een \texttt{namespace}-functie.
            Zo worden dit soort functies in Java omgezet naar statische functies.
            \item[\texttt{namespace}] Een methode (functie van een object) of een functie in een namespace.
            De invulling hiervan is gedeeltelijk programmeertaalafhankelijk: in Java gaat het om methodes, terwijl het in Haskell om functies van een module gaat.
            Bij dit soort functies moet de \texttt{namespace} gegeven worden.
            \item[\texttt{constructor}] Deze soort functie heeft dezelfde semantiek als een top-level functie, met dien verstande dat het om een constructor gaat.
            In Java zal bijvoorbeeld het keyword \texttt{new} vanzelf toegevoegd worden.
            De functienaam doet dienst als naam van de klasse.
            \item[\texttt{property}] De property van een instantie wordt gelezen.
            Deze soort functie heeft dezelfde semantiek van een namespace-functie, maar heeft geen argumenten.
        \end{description}
        \item[\texttt{namespace}] De namespace van de functie.
        \item[\texttt{name}] De naam van de functie.
        \item[\texttt{arguments}] De argumenten van de functie.
        Dit is een lijst van expressies.
    \end{description}
\end{enumerate}

De ondersteuning voor statements in het testplan beperkt zich tot variabeletoekenningen of \english{assignment}s.
Er is expliciet voor gekozen om expressies geen statements te maken.
De reden hiervoor is dat dit de implementatie ingewikkelder zou maken, zonder noemenswaardig voordeel.
Een assignment kent een naam toe aan het resultaat van een expression.
\Cref{lst:type-schema} toont ook de vereenvoudigde \acronym{JSON} Schema van een statement (en dus van een assignment, daar er maar één soort statement bestaat).
Hier staat \texttt{<datatype>} voor een van de gegevenstypes die hiervoor besproken zijn.

De \texttt{name} is de naam die aan de variabele gegeven zal worden.
Het veldje \texttt{expression} moet een expressie zijn, zoals reeds besproken.
Ook moet het gegevenstype van de variabele gegeven worden.
Hiervoor kunnen types het het serialisatieformaat gebruikt worden, inclusief de types uit de letterlijke modus.

Een gecombineerd voorbeeld staat hieronder.
Hier wordt de string \texttt{'Dodona'} toegekend aan een variabele met naam \texttt{name}.

\inputminted{json}{code/assign-variable.json}

Tot slot is het nog het vermelden waard dat waarden van de gegevenstypes \texttt{sequence} en \texttt{map} als elementen geen andere waarden hebben, maar expressies.
Dit niet het geval in de beschrijvende modus van de gegevenstypes, bijvoorbeeld bij het aangeven wat de verwachte returnwaarde van een functie is.
Het testplan biedt namelijk geen ondersteuning voor het serialiseren van identifiers en functieoproepen, enkel waarden.
Dit betekent dat constructies zoals deze mogelijk zijn in het testplan:

\inputminted{java}{code/advanced.jshell}

\subsection{Controle ondersteuning voor programmeertalen}\label{subsec:vereiste-functies}

In het stappenplan uit \cref{sec:ontwerp} is al vermeld dat vóór een beoordeling start, een controle plaatsvindt om zeker te zijn dat het testplan uitgevoerd kan worden in de programmeertaal van de ingediende oplossing.
Bij de controle worden volgende zaken nagekeken:
\begin{itemize}
    \item Controle of de programmeertaal de nodige gegevenstypes ondersteunt.
    Dit gaat van de basistypes (zoals \texttt{sequence}) tot de geavanceerde types (zoals \texttt{tuple}).
    Voor elke programmeertaal binnen TESTed wordt bijgehouden welke types ondersteund worden en welke niet.
    Bevat een testplan bijvoorbeeld waarden met als type \texttt{set} (verzamelingen), dan kunnen enkel programmeertalen die verzamelingen ondersteunen gebruikt worden.
    Dat zijn bijvoorbeeld Python en Java, maar geen Bash.
    \item Controle of de programmeertaal over de nodige taalconstructies beschikt, zoals \texttt{exceptions} of \texttt{objects}.
    Bij deze controle wordt ook gecontroleerd of dat de programmeertaal optionele functieargumenten of functieargumenten met heterogene gegevenstypes nodig heeft.
    Een voorbeeld van een functie met argumenten met heterogene gegevenstypes komt bijvoorbeeld uit de \acronym{ISBN}-oefening (deze oefening wordt besproken in \cref{ch:nieuwe-oefening}):
    \begin{minted}{pycon}
>>> is_isbn("9789027439642")
True
>>> is_isbn(9789027439642)
False
    \end{minted}
    In talen als Python en Java kan deze functie geïmplementeerd worden, maar in talens als C is dat veel moeilijker.
\end{itemize}

\section{Oplossingen uitvoeren}\label{sec:oplossingen-uitvoeren}

De eerste stap die wordt uitgevoerd bij de beoordeling van een ingediende oplossing is het genereren van de testcode, die de ingediende oplossing zal beoordelen.

\subsection{Testcode genereren}\label{subsec:testcode-genereren}

Het genereren van de testcode gebeurt met een sjabloonsysteem genaamd Mako \autocite{mako}.
Dit soort systemen wordt traditioneel gebruikt bij webapplicaties (zoals Ruby on Rails met \acronym{ERB}, Phoenix met \acronym{EEX}, Laravel met Blade, enz.) om bijvoorbeeld html-pagina's te genereren.
In ons geval zijn de sjablonen verantwoordelijk voor de vertaling van programmeertaalonafhankelijke specificaties in het testplan naar concrete testcode in de programmeertaal van de ingediende oplossing.
Hierbij denken we aan de functieoproepen, assignments, enz.
Ook zijn de sjablonen verantwoordelijk voor het genereren van de code die de oplossing van de student zal oproepen en evalueren.

\subsubsection{Sjablonen}

TESTed heeft een aantal standaardsjablonen nodig, waaraan vastgelegde parameters meegegeven worden en die een vaste functie moeten uitvoeren.
Deze verplichte sjablonen zijn:
\begin{description}
    \item[\texttt{assignment}] Vertaalt een toekenningsopdracht uit het testplan naar code.
    \item[\texttt{context}] Een sjabloon dat code genereert om een context te beoordelen.
    Deze code moet uitvoerbaar zijn (dat wil zeggen een main-functie bevatten of een script zijn).
    \item[\texttt{selector}] Een sjabloon dat code genereert om een bepaalde context uit te voeren.
    Om performantieredenen (hierover later meer) wordt de code van alle contexten soms uit een keer gegenereerd en gecompileerd.
    Aan de hand van een parameter (de naam van de context), wordt bij het uitvoeren van deze selectiecode de testcode voor de juiste context gekozen.
    Dit sjabloon is enkel nodig indien batchcompilatie ondersteund wordt en de programmeertaal dit nodig heeft (bijvoorbeeld niet nodig in Python, maar wel in Java).
    \item[\texttt{evaluator\_executor}] Een sjabloon dat code genereert om een geprogrammeerde evaluatie te starten.
    \item[\texttt{function}] Vertaalt een functie-oproep naar testcode.
\end{description}

Daarnaast moet het encoderen naar het serialisatieformaat ook geïmplementeerd worden in elke programmeertaal.
Veel programmeertalen hebben dus nog enkele bijkomende bestanden met code.
In alle bestaande configuraties van programmeertalen is dit geïmplementeerd als een module of een klasse met naam \texttt{Value}.
Dit wordt geïllustreerd in \cref{ch:nieuwe-taal}, dat het toevoegen van een nieuwe programmeertaal aan TESTed volledig uitwerkt.

\subsubsection{Testcode compileren}

TESTed ondersteunt twee modi waarin de code gecompileerd kan worden (bij programmeertalen die geen compilatie ondersteunen wordt deze stap overgeslagen):

\begin{description}
    \item[Batchcompilatie] In deze modus wordt de code voor alle contexten in een keer gecompileerd.
    Dit wordt gedaan om performantieredenen.
    In talen die resulteren in een uitvoerbaar bestand (zoals Haskell, C/C++), resulteert deze modus in één uitvoerbaar bestand voor alle contexten.
    Bij het uitvoeren wordt dan aan de hand van een parameter de juiste context uitgevoerd (met het \texttt{selector}-sjabloon van hierboven).
    \item[Contextcompilatie] Hierbij wordt elke context afzonderlijk gecompileerd.
\end{description}

Dit wordt getoond in \cref{fig:tested-flow} uit \cref{sec:ontwerp} door twee kleuren te gebruiken: de stappen die enkel gebeuren bij batchcompilatie zijn in het \textcolor{ugent-ps}{groen}, terwijl stappen die enkel bij contextcompilatie gebeuren in het \textcolor{ugent-we}{blauw} staan.
Stappen die altijd gebeuren staan in de flowchart in het zwart.

Dit gedrag is configureerbaar in het testplan, maar standaard wordt de batchcompilatie gebruikt.
Als er een compilatiefout optreed bij de compilatie in batchcompilatie, wordt valt TESTed terug op contextcompilatie.
Deze terugval is handig voor programmeertalen waar de compilatie veel fouten ontdekt (vaak de meer statische programmeertalen).
Een voorbeeldscenario is als volgt: stel een oefening waarbij de student twee functies moet implementeren.
De student implementeert de eerste functie en dient een oplossing in om al feedback te krijgen.
Bij programmeertalen als Java of Haskell zal dit niet lukken: daar alle contexten in één keer gecompileerd worden, zal de ontbrekende tweede functie ervoor zorgen dat de volledige compilatie faalt.
In individuele modus is dit geen probleem: de contexten die de eerste functie testen zullen compileren en kunnen uitgevoerd worden.
De individuele modus brengt wel een niet te verwaarlozen kost qua uitvoeringstijd met zich mee (zie ook \cref{ch:beperkingen-en-toekomstig-werk}).

\Cref{lst:generated-context-python,lst:generated-context-java} bevatten de testcode gegenereerd voor een context uit de voorbeeldoefening Lotto (het gaat om dezelfde context uit het voorbeeld van het testplan in \cref{lst:testplan}), in respectievelijk Python en Java.
Daarnaast bevat Z de code voor de \texttt{selector} in Java.
Hiervan is geen versie in Python, daar Python selector nodig heeft in batchcompilatie (in Python kunnen meerdere onafhankelijke bestanden tegelijk gecompileerd worden).
De selector bevat twee contexten om de werking duidelijk te maken.

\begin{listing}
    \inputminted{python}{code/generated-context-1.py}
    \caption{
        Gegenereerde testcode in Python voor de eerste context uit het testplan van de voorbeeldoefening Lotto.
    }
    \label{lst:generated-context-python}
\end{listing}

\begin{listing}
    \inputminted{java}{code/generated-context-1.java}
    \caption{
        Gegenereerde testcode in Java voor de eerste context uit het testplan van de voorbeeldoefening Lotto.
        Enkele hulpfuncties en imports zijn verwijderd om de code korter te maken.
    }
    \label{lst:generated-context-java}
\end{listing}

\begin{listing}
    \inputminted{java}{code/Selector.java}
    \caption{
        Gegenereerde selectiecode in Java voor twee contexten uit het testplan van de voorbeeldoefening Lotto.
    }
    \label{lst:selector-java}
\end{listing}

\subsection{Testcode uitvoeren}\label{subsec:testcode-uitvoeren}

Vervolgens wordt de (gecompileerde) testcode voor elke context uit het testplan afzonderlijk uitgevoerd en worden de resultaten (het gedrag en de neveneffecten) verzameld.
Het uitvoeren zelf gebeurt op de normale manier waarop code voor de programmeertaal uitgevoerd wordt: via de commandoregel.
Deze aanpak heeft als voordeel dat er geen verschil is tussen hoe TESTed de ingediende code uitvoert en hoe de student zijn code zelf uitvoert op zijn eigen computer.
Dit voorkomt dat er subtiele verschillen in de resultaten sluipen.

\Cref{lst:mapstructuur} illustreert dit met een voorbeeld voor een ingediende oplossing in de programmeertaal Python.
Deze mapstructuur stelt de toestand van de werkmap van TESTed voor na het uitvoeren van de code.
In de map \texttt{common} zit alle testcode en de gecompileerde bestanden voor alle contexten.
Voor elke context worden de gecompileerde bestanden gekopieerd naar een andere map, bv.\ \texttt{context\_0\_1}, wat de map is voor context \texttt{1} van tabblad \texttt{0} van het testplan.

\begin{listing}
    \inputminted{text}{code/dir-listing.txt}
    \caption{Mapstructuur na het uitvoeren van de testcode van een oplossing in Python.
    \texttt{context\_0\_0} staat voor de eerste context van het eerste tabblad.
    }
    \label{lst:mapstructuur}
\end{listing}

\subsection{Beoordelen van gedrag}\label{subsec:beoordelen-van-gedrag}

Het uitvoeren van de testcode genereert resultaten (gedrag en neveneffecten) die door TESTed beoordeeld moeten worden.
Er zijn verschillende soorten gedragingen en neveneffecten die interessant zijn.
Elke soort gedrag of neveneffect wordt een \term{uitvoerkanaal} genoemd.
TESTed verzamelt volgende uitvoerkanalen:
\begin{itemize}
    \item De standaarduitvoerstroom.
    Dit wordt verzameld als tekstuele uitvoer.
    \item De standaardfoutstroom.
    Ook dit wordt als tekst verzameld.
    \item Fatale uitzonderingen.
    Hiermee bedoelen we uitzonderingen die tot aan de testcode geraken.
    Een uitzondering die afgehandeld wordt door de ingediende oplossing wordt niet verzameld.
    De uitzonderingen worden verzameld in een bestand.
    \item Returnwaarden.
    Deze waarden worden geëncodeerd en ook verzameld in een bestand.
    \item Exitcode.
    Het gaat om de exitcode van de testcode voor een context.
    Daar de code per context wordt uitgevoerd, wordt de exitcode ook verzameld per context (en niet per testcase, zoals de andere uitvoerkanalen).
    \item Bestanden.
    Tijdens het beoordelen van de verzamelde resultaten is het mogelijk de door de ingediende oplossing gemaakte bestanden te bekijken.
\end{itemize}

De standaarduitvoer- en standaardfoutstroom worden rechtstreeks opgevangen door TESTed.
De andere uitvoerkanalen (uitzonderingen en returnwaarden) worden naar een bestand geschreven.
De reden dat deze niet naar een andere \term{file descriptor} geschreven worden is eenvoudig: niet alle talen (zoals Java) ondersteunen het openen van bijkomende file descriptors.

Alle uitvoerkanalen (met uitzondering van de exitcode en de bestanden) worden per testcase verzameld.
Aangezien de uitvoerkanalen pas verzameld worden na het uitvoeren van de context, moet er een manier zijn om de uitvoer van de verschillende testgevallen te onderscheiden.
De testcode is hier verantwoordelijk voor, en schrijft een \english{separator} naar alle uitvoerkanalen tussen elk testgeval, zoals te zien is in \cref{lst:uitvoer}.

\begin{listing}
    \begin{minted}{text}
    {"data":"1 - 3 - 6 - 8 - 10 - 15","type":"text"}--gL9koJNv3-- SEP
    \end{minted}
    \caption{Voorbeeld van het uitvoerkanaal voor returnwaarden na het uitvoeren van de eerste context uit de voorbeeldoefening Lotto.}
    \label{lst:uitvoer}
\end{listing}

Tijdens het genereren van de code krijgen de sjablonen een reeks willekeurige tekens mee, de \english{secret}.
Deze secret wordt gebruikt voor verschillende dingen, zoals:
\begin{itemize}
    \item De separator.
    Door het gebruik van de willekeurige tekens is de kans dat de separator overeenkomt met een echte waarde praktisch onbestaand.
    \item Bestandsnamen.
    De testcode is verantwoordelijk voor het openen van de bestanden voor de uitvoerkanalen die naar een bestand geschreven worden.
    Bij het openen zal de testcode de secret in de bestandsnaam gebruiken.
    Dit is om het per abuis overschrijven van deze bestanden door de ingediende oplossing tegen te gaan.
\end{itemize}

\section{Oplossingen beoordelen}\label{sec:oplossingen-beoordelen}

Na het uitvoeren van de testcode voor elke context heeft TESTed alle relevante uitvoer gemeten en verzameld.
Deze uitvoer moet vervolgens beoordeeld worden om na te gaan in hoeverre deze uitvoer voldoet aan de verwachte uitvoer.
Dit kan op drie manieren:
\begin{enumerate}
    \item Generieke evaluatie: de uitvoer wordt beoordeeld door TESTed zelf.
    \item Geprogrammeerde evaluatie: de uitvoer wordt beoordeeld door programmacode geschreven door degene die de oefening opgesteld heeft, in een aparte omgeving (de evaluatieomgeving).
    \item Programmeertaalspecifieke evaluatie: de uitvoer wordt onmiddellijk na het uitvoeren van de testcode beoordeeld in het hetzelfde proces.
\end{enumerate}

\subsection{Generieke evaluatie}\label{subsec:ingebouwde-evaluator}

Voor eenvoudige beoordelingen (bijvoorbeeld tussen twee waarden) volstaat de generieke evaluatie binnen TESTed.
Het is mogelijk om de verwachte resultaten in het testplan op te nemen.
TESTed zal deze resultaten uit het testplan dan vergelijken met de resultaten geproduceerd door het uitvoeren van de testcode.
Als \english{proof of concept} zijn drie eenvoudige evaluatiemethoden ingebouwd in TESTed, die hieronder besproken worden.

\subsubsection{Tekstevaluatie}

Deze evaluator vergelijkt de verkregen uitvoer van een uitvoerkanaal (standaarduitvoer, standaardfout, \ldots) met de verwachte uitvoer uit het testplan.
Deze evaluator biedt enkele opties om het gedrag aan te passen:

\begin{description}
    \item[\texttt{ignoreWhitespace}]
    Witruimte voor en na het resultaat wordt genegeerd.
    Dit gebeurt op de volledige tekst, niet regel per regel.
    \item[\texttt{caseInsensitive}] Er wordt geen rekening gehouden met het verschil tussen hoofdletters en kleine letters.
    \item[\texttt{tryFloatingPoint}]
    De tekst zal geïnterpreteerd worden als een zwevendekommagetal (\english{floating point}).
    Bij het vergelijken met de verwachte waarde zal de functie \mintinline{python}{math.isclose()}\footnote{Documentatie is hier te vinden: \url{https://docs.python.org/3/library/math.html\#math.isclose}} uit de standaardbibliotheek van Python gebruikt worden.
    Deze functie controleert of twee zwevendekommagetallen "dicht bij elkaar" liggen.
    De standaardfoutmarges van Python worden gebruikt.
    Een punt voor de toekomst is het configureerbaar maken van deze foutmarges.
    \item[\texttt{applyRounding}] Of zwevendekommagetallen afgrond moeten worden tijdens het vergelijken.
    Indien wel wordt het aantal cijfers genomen van de optie \texttt{roundTo}.
    Na de afronding worden ze ook vergeleken met de functie \mintinline{python}{math.isclose()}.
    Deze afronding is enkel van toepassing op het vergelijken, niet op de uitvoer.
    \item[\texttt{roundTo}] Het aantal cijfers na de komma.
    Enkel nuttig als \texttt{applyRounding} waar is.
\end{description}

Deze configuratieopties worden op het niveau van de testen meegegeven.
Dit laat toe om voor elke test (zelfs binnen eenzelfde testgeval) andere opties mee te geven.
Een nadeel is wel dat dezelfde opties mogelijk veel herhaald moeten worden, bijvoorbeeld als een bepaalde oefening een optie voor elke test wil instellen.
Echter wordt er verwacht dat dit soort zaken opgelost kunnen worden door een \acronym{DSL} of door het testplan te genereren.

Dit is de standaardevaluatievorm in het testplan als niets anders gegeven wordt.
\Cref{lst:testplan-text} toont een fragment uit een testplan: de uitvoerspecificatie van een testgeval waarbij de tekstevaluatie gebruikt wordt.

\begin{listing}
    \inputminted{json}{code/testplan-text.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van de standaarduitvoerstroom voor een testgeval toont, waarbij de tekstevaluatie gebruikt wordt.}
    \label{lst:testplan-text}
\end{listing}

\subsubsection{Bestandsevaluatie}

In deze evaluatievorm worden twee bestanden vergeleken met elkaar.
Hiervoor bevat het testplan enerzijds een pad naar een bestand die met de oefening gegeven wordt met de verwachte inhoud en anderzijds de naam (of pad) van de locatie waar het verwachte bestand zich moet bevinden.
De bestandsevaluatie ondersteunt enkel tekstuele bestanden, geen binaire bestanden.
Het vergelijken van de bestanden gebeurt op één dezer manieren:

\begin{description}
    \item[\texttt{exact}] Beide bestanden moet exact hetzelfde zijn, inclusief regeleindes.
    \item[\texttt{lines}] Elke regel wordt vergeleken met overeenkomstige regel in het andere bestand.
    De evaluatie van de lijnen is exact, maar zonder de regeleindes.
    Dit betekent dat de witruimte bijvoorbeeld ook moet overeenkomen.
    \item[\texttt{values}] Elke regel in het bestand wordt afzonderlijk vergeleken met de tekstevaluatie.
    Indien deze modus gebruikt wordt, kunnen ook alle opties van de tekstevaluatie meegegeven worden.
\end{description}

Een voorbeeld van hoe dit eruitziet is \cref{lst:testplan-file}.
In dit fragment wordt de modus \texttt{values} gebruikt, en worden de opties van de tekstevaluatie ook meegegeven.
Het bestand met de verwachte inhoud heeft als naam \texttt{bestand-uit-de-oefening.txt} gekregen, terwijl de ingediende oplossing een bestand moet schrijven naar \texttt{waar-het-verwachte-bestand-komt.txt}.
Beide paden zijn relatief, maar ten opzichte van andere mappen: het bestand met verwachte inhoud is relatief tegenover de map van de oefening, terwijl het pad waar de ingediende oplossing naar moet schrijven relatief is ten opzichte van de werkmap van de context waarin de oplossing wordt uitgevoerd (zie \cref{lst:mapstructuur} voor een overzicht van de structuur).

\begin{listing}
    \inputminted{json}{code/testplan-file.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van een bestand voor een testgeval toont, waarbij de bestandsevaluatie gebruikt wordt.}
    \label{lst:testplan-file}
\end{listing}

\subsubsection{Waarde-evaluatie}

Voor uitvoerkanalen zoals de returnwaarden moet meer dan alleen tekst met elkaar vergeleken kunnen worden.
Staat er in het testplan welke waarde verwacht wordt (geëncodeerd in het serialisatieformaat), dan kan TESTed dit vergelijken met de eigenlijke waarde die geproduceerd werd door de ingediende oplossing.

Het vergelijken van een waarde bestaat uit twee stappen:
\begin{enumerate}
    \item Het gegevenstype wordt vergeleken, waarbij beide waarden (de verwachte waarde uit het testplan en de geproduceerde waarde uit de ingediende oplossing) hetzelfde type moeten hebben.
    Hierbij wordt rekening gehouden met de vertalingen tussen de verschillende programmeertalen, waarbij twee gevallen onderscheiden kunnen worden:
    \begin{enumerate}
        \item Specifieert het testplan een basistype, dan zullen alle types die tot dit basistype herleid kunnen worden als hetzelfde beschouwd worden.
        Is de verwachte waarde bijvoorbeeld \texttt{sequence}, zullen ook \texttt{array}s uit Java en \texttt{tuple}s uit Python goedgekeurd worden.
        \item Specifieert het testplan een uitgebreid type, dan zal het uitgebreid type gebruikt worden voor talen die dat type ondersteunen, terwijl voor andere talen het basistype gebruikt zal worden.
        Stel dat het testplan bijvoorbeeld een waarde met als gegevenstype \texttt{tuple} heeft.
        In Python en Haskell (twee talen die dat gegevenstype ondersteunen) zullen enkel \texttt{tuple}s goedgekeurd worden.
        Voor andere talen, zoals Java, worden alle gegevenstypes goedgekeurd die herleidbaar zijn tot het basistype.
        Concreet zullen dus \texttt{List}s en \texttt{array}s goedgekeurd worden.
        Merk op dat momenteel bij collecties (\texttt{sequence}s, \texttt{set}s en \texttt{map}s) enkel het type van de collectie gecontroleerd wordt.
    \end{enumerate}
    \item De twee waarden worden vergeleken op inhoud (indien de vergelijking van de gegevenstypes uit de vorige stap positief is).
    Hierbij maakt TESTed gebruik van de ingebouwde vergelijking van Python om twee waarden te evalueren.
    Dit betekent dat de regels voor \english{value comparisons} uit Python\footnote{Zie \url{https://docs.python.org/3/reference/expressions.html?highlight=comparison\#value-comparisons}} gevolgd worden.
    Eén uitzondering is zwevendekommagetallen, waarvoor opnieuw \mintinline{python}{math.isclose()} gebruikt wordt in plaats van \mintinline{python}{==}.
\end{enumerate}

Bij deze evaluatievorm zijn geen configuratieopties.
Een voorbeeld van het gebruik binnen een testplan is \cref{lst:testplan-value}.
Hier wordt als returnwaarde een verzameling met drie elementen (5, 10 en 15) verwacht.

\begin{listing}
    \inputminted{json}{code/testplan-value.json}
    \caption{Fragment uit een testplan dat de uitvoerspecificatie van de returnwaarde voor een testgeval toont, waarbij de waarde-evaluatie gebruikt wordt.}
    \label{lst:testplan-value}
\end{listing}

\subsection{Geprogrammeerde evaluatie}\label{subsec:geprogrammeerde-evaluatie}

Bij oefeningen met niet-deterministische resultaten, zoals de voorbeeldoefening Lotto, kunnen de verwachte waarden niet in het testplan komen.
Ook andere oefeningen waar geen directe vergelijking kan gemaakt worden, zoals het uitlijnen van sequenties (\english{sequence alignment}) uit de bio-informatica, volstaat een vergelijking met een verwachte waarde uit het testplan niet.

Toch is deze evaluatie niet programmeertaalafhankelijk: de logica om een sequentie uit te lijnen is dezelfde ongeacht de programmeertaal waarin dit gebeurt.
Voor dergelijke scenario's is geprogrammeerde evaluatie een oplossing: hierbij wordt code geschreven om de evaluatie te doen, maar deze evaluatiecode staat los van de ingediende oplossing en moet ook niet in dezelfde programmeertaal geschreven zijn.
Binnen TESTed wordt dit mogelijk gemaakt door geproduceerde waarden uit de ingediende oplossing te serialiseren bij het uitvoeren van de testcode, en terug te deserialiseren bij het uitvoeren van de evaluatiecode.

Deze evaluatiecode kan geschreven worden in een programmeertaal naar keuze, al moet de programmeertaal wel ondersteund worden door TESTed.
De implementatie volgt in alle programmeertalen hetzelfde stramien, maar de implementatiedetails kunnen verschillen.
In Python bestaat de evaluatiecode uit een module (een \texttt{.py}-bestand) met een functie die voldoet aan de definitie, zoals gegeven in \cref{lst:evaluation-python-custom}.
TESTed stelt ook een module \texttt{evaluation\_utils} ter beschikking.
De functie van hierboven moet dan één oproep doen naar de functie \texttt{evaluated()}.
Deze module is redelijk eenvoudig, zoals te zien in \cref{lst:evaluation-util-python}

\begin{listing}
    \inputminted{python}{code/custom_signature.py}
    \caption{De definitie van de functie die aanwezig moet zijn in de evaluatiecode voor een geprogrammeerde evaluatie geschreven in Python.}
    \label{lst:evaluation-python-custom}
\end{listing}

\begin{listing}
    \inputminted{python}{../../judge/src/tested/languages/python/templates/evaluation_utils.py}
    \caption{De implementatie van de module \texttt{evaluation\_utils}}
    \label{lst:evaluation-util-python}
\end{listing}

In de Java-implementatie is de situatie gelijkaardig: het gaat om het implementeren van een abstracte klasse.
Deze abstracte klasse biedt ook de functionaliteit aan van de module \texttt{evaluation\_utils} bij Python.
De te implementeren klasse en haar ouderklasse staan in \cref{lst:evaluation-util-java,lst:evaluation-java-custom}.

\begin{listing}
    \inputminted{java}{../../judge/src/tested/languages/java/templates/AbstractCustomEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractCustomEvaluator}.}
    \label{lst:evaluation-java-custom}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/src/tested/languages/java/templates/AbstractEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractEvaluator}.}
    \label{lst:evaluation-util-java}
\end{listing}

Een geprogrammeerde evaluatie wordt gebruikt in de voorbeeldoefening Lotto.
Het gebruik in het testplan wordt getoond in \cref{lst:testplan-custom}, waar de evaluatiecode voor de aangepaste evaluatie in Python geschreven is.
Er worden ook argumenten meegegeven aan deze code.
De evaluatiecode zelf is gegeven in \cref{lst:evaluation-lotto}.

\begin{listing}
    \inputminted{java}{code/testplan-custom.json}
    \caption{Fragment uit het testplan van de voorbeeldoefening Lotto, waar een geprogrammeerde evaluatie gebruikt wordt.}
    \label{lst:testplan-custom}
\end{listing}

\begin{listing}
    \inputminted{python}{../../exercise/lotto/evaluation/evaluator.py}
    \caption{De evaluatiecode voor de geprogrammeerde evaluatie van de voorbeeldoefening Lotto.}
    \label{lst:evaluation-lotto}
\end{listing}

\subsection{Programmeertaalspecifieke evaluatie}\label{subsec:programmeertaalspecifieke-evaluatie}

In sommige scenario's moeten programmeertaalspecifieke concepten beoordeeld worden.
Een mogelijkheid is deze oefeningen niet aanbieden in TESTed, maar in de programmeertaalspecifieke judges.
Toch zijn er nog voordelen om ook deze oefeningen in TESTed aan te bieden:
\begin{itemize}
    \item Het bijkomende werk om meer programmeertalen te ondersteunen beperkt zich tot een minimum.
    \item Het werk om een nieuwe programmeertaal toe te voegen aan TESTed is kleiner dan een volledig nieuwe judge te implementeren.
\end{itemize}
Het is desalniettemin het vermelden waard dat het niet zeker is of deze evaluatiemethode (en dit scenario meer algemeen) veel zal voorkomen.
Oefeningen die programmeertaalspecifieke aspecten moeten beoordelen zijn, net door hun programmeertaalspecifieke aard, moeilijker aan te bieden in meerdere programmeertalen.
Een oefening in de programmeertaal C die bijvoorbeeld beoordeelt op juist gebruik van pointers zal weinig nut hebben in Python.

\begin{listing}
    \inputminted{json}{code/testplan-specific.json}
    \caption{Fragment uit een testplan waar een programmeertaalspecifieke evaluatie gebruikt wordt.}
    \label{lst:testplan-specific}
\end{listing}

In gebruik lijkt de programmeertaalspecifieke evaluatie sterk op de geprogrammeerde evaluatie, met dat verschil dat het testplan niet evaluatiecode in één programmeertaal bevat, maar evaluatiecode in alle programmeertalen waarin de oefening aangeboden wordt, zoals geïllustreerd in \cref{lst:testplan-specific}.
Als de programmeertaalspecifieke evaluatie gebruikt wordt en er wordt geen evaluatiecode voor een bepaalde programmeertaal, zal de oefening niet opgelost kunnen worden in die programmeertaal.

Ook de implementatie lijkt op de geprogrammeerde evaluatie, zij het dat de te implementeren functie afwijkt.
In Python wordt dit \cref{lst:evaluation-python-specific}, in Java \cref{lst:evaluation-java-specific}.
Om het resultaat van de evaluatie aan de judge te geven, wordt dezelfde \texttt{evaluated}-functie als bij de aangepaste evaluator gebruikt (zie \cref{lst:evaluation-util-python,lst:evaluation-util-java}).
Het gebruik in het testplan is \cref{lst:testplan-specific}.

\begin{listing}
    \inputminted{python}{code/specific_signature.py}
    \caption{De definitie van de functie die aanwezig moet zijn in de evaluatiecode voor een programmeertaalspecifieke evaluatie geschreven in Python.}
    \label{lst:evaluation-python-specific}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/src/tested/languages/java/templates/AbstractSpecificEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractSpecificEvaluator}.}
    \label{lst:evaluation-java-specific}
\end{listing}

\section{Performantie}\label{sec:performantie}

Zoals eerder vermeld (\cref{subsec:testcode-uitvoeren}), wordt de testcode voor elke context afzonderlijk uitgevoerd.
Dat de contexten strikt onafhankelijk van elkaar uitgevoerd worden, werd reeds in het begin als een doel vooropgesteld.
Dit geeft wel enkele uitdagingen op het vlak van performantie.
Het belang van performante judges in Dodona is niet te verwaarlozen, in die zin dat Dodona een interactief platform is, waar studenten verwachten dat de feedback op hun ingediende oplossing onmiddellijk beschikbaar is.
Deze paragraaf beschrijft de evolutie van de implementatie van TESTed vanuit het perspectief van de performantie.

\subsection{Jupyter-kernels}\label{subsec:jupyter-kernels}

Het eerste prototype van TESTed gebruikte Jupyter-kernels voor het uitvoeren van de testcode.
Jupyter-kernels zijn de achterliggende technologie van Jupyter Notebooks \autocite{jupyter2016}.
De werking van een Jupyter-kernel kan als volgt samengevat worden: een Jupyter-kernel is een lokaal proces, dat code kan uitvoeren en de resultaten van die uitvoer teruggeeft.
Zo kan men naar de Python-kernel de expressie \mintinline{python}{5 + 9} sturen, waarop het antwoord \mintinline{python}{14} zal zijn.
Een andere manier om een Jupyter-kernel te bekijken is als een programmeertaalonafhankelijk protocol bovenop een \acronym{REPL} (een \english{read-eval-print loop}).
Deze keuze voor Jupyter-kernels als uitvoering was gebaseerd op volgende argumenten:
\begin{itemize}
    \item Hergebruik van bestaande kernels.
    Hierdoor is het niet nodig om voor elke programmeertaal veel tijd te besteden aan de implementatie of de configuratie: aangezien het protocol voor Jupyter-kernels programmeertaalonafhankelijk is, kunnen alle bestaande kernels gebruikt worden.
    \item De functionaliteit aangeboden door een Jupyter-kernel is de functionaliteit die nodig is voor TESTed: het uitvoeren van fragmenten code en het resultaat van die uitvoering verzamelen.
    \item Eerder werk \autocite{petegem2018}, dat gebruik maakt van Jupyter-kernels voor een gelijkaardig doel, rapporteert geen problemen met het gebruik van Jupyter-kernels.
\end{itemize}

Omdat contexten per definitie onafhankelijk van elkaar zijn, moet de gebruikte Jupyter-kernel gestopt en opnieuw gestart worden tussen elke context.
Dit brengt een onaanvaardbare performantiekost met zich mee, daar de meeste oefeningen niet computationeel intensief zijn.
Het beoordelen van eenvoudige oefeningen, zoals de Lotto-oefening, duurde als snel meerdere minuten.

Enkele ideeën om de performantiekost te verkleinen waren:

\begin{itemize}
    \item Het gebruiken van een \english{pool} kernels (een verzameling kernels die klaar staan voor gebruik).
    De werkwijze is als volgt:
    \begin{enumerate}
        \item Bij de start van het beoordelen van een ingediende oplossing worden meerdere kernels gestart.
        \item Bij elke context wordt een kernel uit de pool gehaald om de testcode uit te voeren.
        \item Na het uitvoeren wordt de kernel op een andere draad (\english{thread}) opnieuw opgestart en terug aan de pool toegevoegd.
    \end{enumerate}
    Het idee achter deze werkwijze is dat door op een andere draad de kernels te herstarten, er altijd een kernel klaarstaat om de testcode uit te voeren, en er dus niet gewacht moet worden op het opnieuw opstarten van die kernels.
    In de praktijk bleek echter dat zelfs met een twintigtal kernels in de pool, het uitvoeren van de testcode van eenvoudige oefeningen dermate snel gaat in vergelijking met het opnieuw opstarten van de kernels, de kernels nooit op tijd herstart zijn.
    \item De kernels niet opnieuw opstarten, maar de interne toestand opnieuw instellen.
    In bepaalde kernels, zoals de Python-kernel, is dit mogelijk.
    De Python-kernel (IPython) heeft bijvoorbeeld een magisch commando \texttt{\%reset}, dat de toestand van de kernel opnieuw instelt.
    Het probleem is er maar weinig kernels zijn die een gelijkaardig commando hebben.
\end{itemize}

Op dit punt is besloten dat de overhead van de Jupyter-kernels niet naar tevredenheid kon opgelost worden.
Bovendien is een bijkomend nadeel van het gebruik van Jupyter-kernels ondervonden: de kernels voor andere programmeertalen dan Python zijn van gevarieerde kwaliteit.
Zo schrijven de Java-kernel en de Julia-kernel bij het opstarten altijd een boodschap naar de standaarduitvoerstroom, wat in bepaalde gevallen problemen gaf bij het verzamelen van de resultaten van de uitvoer van de testcode.

\subsection{Sjablonen}\label{subsec:sjablonen}

We kozen ervoor om verder te gaan met een systeem van sjablonen (zie ook \cref{subsec:testcode-genereren}).
Hierbij wordt de code gegenereerd en vervolgens uitgevoerd via de commandoregel.

Voordelen ten opzichte van de Jupyter-kernels zijn:
\begin{itemize}
    \item Het is sneller om twee onafhankelijke programma's uit te voeren op de commandolijn dan een Jupyter-kernel tweemaal te starten.
    \item Het laat meer vrijheid toe in hoe de resultaten (gedrag en neveneffecten) van het uitvoeren van de testcode verzameld worden.
    \item Het implementeren en configureren van een programmeertaal in TESTed met de basisfunctionaliteit is minder werk dan dan het implementeren van een nieuwe Jupyter-kernel.
    Optionele functionaliteit, zoals linting, kan ervoor zorgen dat meer tijd en werk nodig is bij het toevoegen van een programmeertaal.
    Het systeem met de sjablonen hanteert echter het designprincipe dat optionele functionaliteit geen bijkomend werk mag vragen voor programmeertalen die er geen gebruik van maken.
    \item Er is geen verschil tussen hoe TESTed de ingediende oplossing uitvoert en hoe de student diezelfde oplossing uitvoert.
    Bij de Jupyter-kernels is dat soms wel het geval: bij de R-kernel zitten subtiele verschillen tussen het uitvoeren in de kernel en het uitvoeren op de commandoregel.
    \item Herbruikbaar in die zin dat het genereren van testcode in een programmeertaal op basis van de programmeertaalonafhankelijke specificatie in het testplan sowieso nodig is.
    \item De gegenereerde testcode bestaat uit normale codebestanden (bv.\ \texttt{.py}- of \texttt{.java}-bestanden), wat het toevoegen van een programmeertaal aan TESTed eenvoudiger te debuggen maakt: alle gegenereerde testcode is in een bestand beschikbaar voor inspectie.
\end{itemize}

Elke medaille heeft ook een keerzijde:
\begin{itemize}
    \item Het gebruik van sjablonen zorgt ervoor dat het uitvoeren van de testcode minder dynamisch is.
    Daar de testcode eerst gegenereerd moet worden, moet vóór het uitvoeren bepaald worden welke testcode zal uitgevoerd worden.
    \item Er moet meer zelf geïmplementeerd worden: het ecosysteem van Jupyter is groot.
    Er bestaan meer dan honderden kernels\footnote{Zie deze pagina voor een actuele lijst van kernels: \url{https://github.com/jupyter/jupyter/wiki/Jupyter-kernels}.}, goed voor ondersteuning voor meer dan tachtig programmeertalen.
    Niet elke kernel is onmiddellijk bruikbaar, maar er kan verder gewerkt worden op wat bestaat.
    \item Het uitvoeren op de commandoregel is bij veel programmeertalen trager indien er geen reset moet gebeuren tussen de verschillende uitvoeringen.
    Bij het uitvoeren op de commandoregel is er bij veel programmeertalen een performantiekost bij het opstarten van de interpreter (zoals Python) of virtuele machine (zoals Java).
    Bij een kernel is de opstartkost weliswaar groter, maar deze wordt maar één keer opgestart als er geen reset nodig is.
\end{itemize}

Bij een eerste iteratie van het systeem met sjablonen bestond enkel de \textbf{contextcompilatie} (\cref{subsec:testcode-genereren}).
Dit heeft een grote performantiekost bij alle programmeertalen, maar in het bijzonder bij talen zoals Java.
Daar moest voor elke context eerst een compilatie plaatsvinden, waarna de gecompileerde testcode werd uitgevoerd.
Bij een testplan met bijvoorbeeld 50 contexten vertaalt dit zich in 50 keer de \texttt{javac}-compiler uitvoeren en ook 50 keer het uitvoeren van de code zelf met \texttt{java}.

In een tweede iteratie werd een \textbf{partiële compilatie} geïmplementeerd.
Het idee hier is dat er veel testcode is die hetzelfde blijft voor elke context (denk aan de ingediende oplossing en hulpbestanden van TESTed).
In de partiële compilatie wordt de gemeenschappelijke testcode eerst gecompileerd (bij programmeertalen die dat ondersteunen).
Bij het beoordelen van een context wordt dan enkel de testcode specifiek voor de te beoordelen context gecompileerd.

In een derde iteratie werd de partiële compilatie uitgebreid naar \textbf{batchcompilatie}.
Hoewel dit een grote winst voor de uitvoeringstijd betekende, heeft deze modus wel een ander groot nadeel (zoals reeds vermeld in \cref{subsec:testcode-genereren}): bij statische talen zorgen compilatiefouten in één context ervoor dat geen enkele context beoordeeld kan worden, daar de compilatiefout ervoor zorgt dat er geen code gegenereerd wordt.
Dit wordt best geïllustreerd met het reeds vermelde scenario van een oefening waarbij de student twee functies dient te implementeren.
In batchcompilatie in bijvoorbeeld Java kan de student niet de oplossing voor de eerste functie indienen en laten beoordelen: omdat de tweede functie ontbreekt zullen compilatiefouten optreden.

De laatste en huidige iteratie bevat de mogelijkheid om contextcompilatie en batchcompilatie te combineren, waarbij TESTed terugvalt op de contextcompilatie indien er iets misgaat bij de batchcompilatie.
Dit terugvalmechanisme is geen geavanceerd systeem: faalt de compilatie, wordt onmiddellijk elke context afzonderlijk gecompileerd.
Een mogelijke optimalisatie bestaat er uit een tussenoplossing toe te passen, door bijvoorbeeld compilatie op het niveau van tabbladen proberen (dit wordt verder besproken in \cref{subsec:future-performance}).

\subsection{Parallelle uitvoering van contexten}\label{subsec:parallelle-uitvoering-van-contexten}

Een ander aspect is de ondersteuning voor parallelle uitvoering van de contexten.
Zoals al enkele malen aangehaald zijn de contexten van het testplan volledig onafhankelijk van elkaar, waardoor ze zich ertoe lenen om parallel uitgevoerd te worden.

Een eerste opmerking hierbij is dat de tijdswinst op Dodona kleiner waarschijnlijk zal zijn dan de tijdswinst geobserveerd in de tijdsmetingen (\cref{tab:meting}).
De reden hiervoor is dat Dodona een beperkt aantal \english{workers} heeft die beoordelingen uitvoeren.
Doordat er meerdere gebruikers tegelijk oplossingen indienen, zal er niet altijd ruimte zijn voor parallellisatie binnen de judge.

Een tweede opmerking is dat er verschillende beperkingen of opmerkingen zijn in de implementatie:

\begin{itemize}
    \item De parallelle uitvoering heeft enkel betrekking op het uitvoeren van de contexten, niet op de beoordeling ervan.
    De beoordeling gebeurt nog steeds sequentieel.
    \item De parallellisatie gebeurt enkel binnen tabbladen.
    De tabbladen zelf worden sequentieel uitgevoerd.
    Dit betekent dat de contexten van tabblad 1 parallel worden uitgevoerd, en pas aan tabblad 2 begonnen wordt als alle contexten van tabblad 1 klaar zijn.
    \item De volgorde waarin het resultaat van de beoordeling van een context naar Dodona gestuurd wordt blijft dezelfde volgorde als in het testplan.
    Dit impliceert dat het mogelijk is dat, bijvoorbeeld bij het overschrijden van de tijdslimiet, de uitvoer van uitgevoerde contexten niet getoond wordt.
    Stel dat contexten 1, 2, 4 en 5 klaar zijn wanneer de tijdslimiet overschreden wordt.
    Omdat TESTed aan het wachten was op context 3, zullen contexten 4 en 5 ook als niet uitgevoerd beschouwd worden.
    \item De tijdslimieten zullen minder nauwkeurig zijn.
    Door de parallelle uitvoering zal er meer variatie zitten op de totale tijd die nodig is voor het beoordelen van een oefening.
    Dit heeft geen effect op de tijdslimieten bij hun functie voor het detecteren van programmeerfouten, zoals oneindige lussen.
    Waar er wel moet opgelet worden is indien de tijdslimieten gebruikt worden om te controleren dat een oplossing efficiënt is.
    Door de parallelle uitvoering is het mogelijk dat een oplossing die normaal nooit onder de tijdslimiet zou zitten, nu wel aanvaard wordt.
\end{itemize}

\subsection{Snellere geprogrammeerde evaluatie}\label{subsec:snellere-geprogrammeerde-evaluatie}

Een laatste performantieverbetering werd behaald op het vlak van geprogrammeerde evaluatie (zie \cref{subsec:geprogrammeerde-evaluatie-is-traag} voor verdere ideeën voor verbeteringen).
Zoals beschreven in \cref{subsec:geprogrammeerde-evaluatie}, wordt normaliter voor elke geprogrammeerde evaluatie de evaluatiecode opnieuw gecompileerd en uitgevoerd in een nieuw subproces.
Voor evaluatiecode die geschreven is in Python worden deze stappen overgeslagen.
Er is speciale ondersteuning in TESTed ingebouwd om geprogrammeerde evaluaties waarvan de evaluatiecode in Python geschreven is, rechtstreeks in TESTed zelf uit te voeren.

\subsection{Tijdsmetingen}\label{subsec:tijdsmetingen}

\begin{table}
    \centering
    \begin{tabular}{ll|r|r|r|r|}
        \cline{3-6}
        & & \multicolumn{2}{c|}{Python (s)} & \multicolumn{2}{c|}{Java (s)}  \\
        \hline
        \multicolumn{1}{|l|}{Oefening}               & Compilatiemodus  & 1 thread & 4 threads & 1 thread & 4 threads \\
        \hline
        \multicolumn{1}{|l|}{\multirow{3}{*}{Lotto}} & Context          & 13       & 9         & 51       & 38        \\
        \multicolumn{1}{|l|}{}                       & Partieel         & 9        & 7         & 46       & 25        \\
        \multicolumn{1}{|l|}{}                       & Batch            & 9        & 6         & 12       & 10        \\
        \multicolumn{1}{|l|}{}                       & Batch+Eval       & 5        & 3         & 9        & 6         \\
        \hline
        \multicolumn{1}{|l|}{\multirow{3}{*}{Echo}}  & Context           & 6        & 3         & 48       & 28        \\
        \multicolumn{1}{|l|}{}                       & Partieel          & 5        & 3         & 46       & 26        \\
        \multicolumn{1}{|l|}{}                       & Batch             & 5        & 3         & 8        & 5         \\
        \hline
    \end{tabular}
    \caption{Tijdsmetingen voor de oefeningen Lotto en Echo, voor de programmeertalen Python en Java, in contextcompilatie, partiële compilatie en batchcompilatie.
        Bij Lotto is deze laatste uitgevoerd met en zonder optimalisatie voor geprogrammeerde evaluaties.}
    \label{tab:meting}
\end{table}

\Cref{tab:meting} toont enkele tijdsmetingen van de verschillende implementaties van het systeem met sjablonen, voor de programmeertalen Python en Java.
In deze tabel is de tijd gemeten in seconden om een (juiste) ingediende oplossing voor twee oefeningen te beoordelen:

\begin{itemize}
    \item De Lotto-oefening, zoals beschreven in \cref{sec:probleemstelling}.
    Deze oefening heeft een geprogrammeerde evaluatie en bestaat uit 50 contexten, die elks één testgeval bevatten.
    \item Een eenvoudigere oefening: Echo.
    Bij deze oefening bestaat de opgave er uit de invoer van de standaardinvoerstroom te lezen en te printen naar de standaarduitvoerstroom.
    Deze oefening bevat enkel een generieke evaluatie en bestaat ook uit 50 contexten, met telkens één testgeval per context.
\end{itemize}

Elke beoordeling is uitgevoerd met contextcompilatie, partiële compilatie en batchcompilatie.
De Lotto-oefening is ook gemeten met de optimalisatie voor geprogrammeerde evaluatie (daar de evaluatiecode voor deze oefening geschreven is in Python).
Dit is aangeduid met de naam "Batch+eval" in de kolom "Compilatiemodus".
Deze variant is niet uitgevoerd op de Echo-oefening, daar die oefening geen geprogrammeerde evaluatie heeft.
Elke tijdsmeting is ook uitgevoerd met en zonder parallelle uitvoering van de contexten.
Deze tijdsmetingen zijn uitgevoerd op een standaardcomputer (Windows 10, Intel \version{i7-8550U}, Python \version{3.8.1}, 64-bit), niet op de Dodona-server en niet in een Docker-container.
Die laatste twee factoren kunnen ervoor zorgen dat uitvoeringstijden op Dodona sterk verschillen van deze metingen.

Zoals verwacht levert batchcompilatie het meeste tijdswinst op bij Java: daar heeft de compilatiestap ook een veel groter aandeel in de uitvoeringstijd.
Bij Python is de compilatie veel sneller en ook veel minder belangrijk (en zelfs optioneel, de stap wordt enkel uitgevoerd om bepaalde syntaxisfouten vroeg op te vangen).

\section{Bijkomende taken}\label{sec:andere-taken}

Sommige judges doen meer dan enkel het oordelen over de juistheid van de ingediende oplossing.
Zo heeft onder andere de Python-judge ondersteuning voor \term{linting}.
Dit betekent dat de code van de ingediende oplossing geanalyseerd (maar niet uitgevoerd) wordt om zo allerlei mogelijke problemen op te sporen, zoals stijlfouten, mogelijke bugs, verdachte constructies, enz.

TESTed voorziet hier ondersteuning voor middels een optionele functie in de configuratie van een programmeertaal (zie \cref{ch:nieuwe-taal} voor de details over het configureren van een taal).
Deze functie krijgt als argumenten onder andere de configuratie, het testplan en het pad naar de code van de ingediende oplossing mee.
Als resultaat geeft de functie een lijst van berichten en annotaties die naar Dodona gestuurd worden.
Ter illustratie dat deze aanpak werkt, is linting geïmplementeerd voor Python.
Ook geeft dit veel vrijheid: de implementatie in Python gebruikt een bestand voor het configureren van de linter, en het is mogelijk om de naam van een eigen configuratiebestand (uit de oefeningenrepository) mee te geven via de programmeertaalspecifieke configuratie van TESTed.
Tot slot nog opmerken dat het formaat van de annotaties wordt voorgeschreven door Dodona, maar dit is voldoende generiek voor praktisch alle scenario's.
Het formaat ondersteunt onder andere:

\begin{itemize}
    \item Het aangeven van een gedetailleerde plaats in de ingediende oplossing, zoals de regel en kolom.
    Het aanduiden van meerdere opeenvolgende regels of kolommen is ook mogelijk.
    \item De boodschap is vrij tekstveld.
    \item De ernst van de boodschap, met ondersteuning voor een veelvoorkomende indeling: informatiebericht, waarschuwing en foutbericht.
\end{itemize}

Een mogelijke uitbreiding is bijvoorbeeld het toevoegen van identificatienummers aan de berichten.
Bij veel linters heeft elke boodschap een unieke code.
Aan de hand van die code zou Dodona de uitleg over de boodschap kunnen tonen (maar dit vereist wel dat Dodona de uitleg voor specifieke linters heeft).
Een alternatief zou zijn dat TESTed deze zelf toevoegt.
De gemakkelijkste oplossing is echter de annotatie uitbreiding met ondersteuning voor een \acronym{URL}: bij veel linters kan met de identificatiecode van het bericht een \acronym{URL} geconstrueerd worden waarop meer informatie staat.
Deze \acronym{URL} zou dan aan de studenten kunnen getoond worden.

\section{Robuustheid}\label{sec:robuustheid}

Een belangrijk aspect bij \term{educational software testing} is de feedback als het verkeerd loopt.
De feedback bij een verkeerde oplossing is in veel gevallen zelfs belangrijker dan de feedback bij een juiste oplossing: het is namelijk de bedoeling dat als studenten een verkeerde oplossing indienen, dat de feedback ze terug op weg kan helpen om hun oplossing te verbeteren.
De feedback die Dodona toont is afkomstig van de judges: het is de taak van TESTed om kwalitatief hoogstaande feedback te voorzien.
Met kwalitatief hoogstaand wordt bedoeld dat de feedback nuttige informatie bevat, maar ook geen verkeerde of misleidende informatie bevat.

\subsection{In de praktijk}\label{subsec:in-de-praktijk}

In het vak \emph{Scriptingtalen} van de bachelor informatica krijgen studenten elke week een reeks oefeningen om op te lossen (op Dodona).
Een van die oefeningen is de ISBN-oefening.
Om een idee te krijgen van hoe TESTed werkt in de praktijk is besloten om tijdens een van de weken de ISBN-oefening aan te vullen door een oefening met dezelfde opgave, maar die beoordeeld wordt door TESTed in plaats van de Python-judge.

De algemene bevindingen die uit de praktijktest naar boven kwamen waren:

\begin{itemize}
    \item TESTed heeft geen ondersteuning voor de Python Tutor.
    Dit is al vermeld, en wordt verder besproken in \cref{subsec:kleinere-functies}.
    \item De evaluatie met TESTed duurt ongeveer dubbel zo lang als met de Python-judge.
    Aan de ene kant zou de optimalisatie van de geprogrammeerde evaluatie (zie \cref{sec:performantie}) dit moeten verbeteren (maar wel in dit specifieke geval, moest de evaluatiecode in een andere programmeertaal dan Python geschreven zijn, zou het geen verschil maken).
    Aan de andere kant ligt een tragere uitvoer in vergelijking met de Python-judge binnen de verwachtingen: TESTed voert elke context uit in een afzonderlijk subproces.
    De Python-judge doet dit niet.
    Qua uitvoer worden wel contexten gebruikt, maar de interne werking van de Python-judge kan het best vergeleken worden met een oefeningen waar alle testen in dezelfde context opgenomen zijn.
    \item TESTed heeft geen verwerking van stacktraces.
    Deze functionaliteit is opgenomen als mogelijke uitbreiding als onderdeel van \cref{subsec:kleinere-functies}.
    \item TESTed heeft geen linting.
    Naar aanleiding van de praktijktest is hier ondersteuning voor toegevoegd, zie \cref{sec:andere-taken}.
    \item TESTed heeft geen ondersteuning voor vertalingen op het vlak van natuurlijke talen.
    Deze functie wordt besproken als uitbreiding in \cref{subsec:ondersteuning-voor-natuurlijke-talen}.
\end{itemize}

Verder heeft professor Dawyndt een heleboel fouten en scenario's uitgeprobeerd.
Deze worden hieronder besproken.

\subsection{Soorten fouten}\label{subsec:soorten-fouten}

TESTed moet robuust zijn tegen allerlei vormen van fouten in de ingediende oplossingen.
Hieronder volgt een lijst van (categorieën) van fouten waarvoor TESTed nuttige feedback geeft.
TESTed is zo opgebouwd dat er altijd iets van feedback komt, ook in onvoorziene omstandigheden, maar voor de soorten fouten op onderstaande lijst is expliciet gecontroleerd wat de kwaliteit van de feedback is.
Waar nuttig zijn deze gevallen omgezet naar een unit test voor TESTed.

\begin{description}
    \item[Compilatiefouten] De uitvoer van de compiler wordt altijd getoond aan de studenten, dus op dat vlak wordt juist afgehandeld.
    Er bestaat ook de mogelijkheid om de compilatie-uitvoer te verwerken en bijvoorbeeld om te zetten naar annotaties, die dan in de code worden getoond.
    Hierbij wordt vooral gedacht aan foutboodschappen als "syntaxisfout op regel 5, kolom 2".
    De stacktraces bij deze uitvoer bevatten wel referenties naar code die TESTed gegenereerd heeft.
    Deze verwijzingen naar interne code wegfilteren is opgenomen als uitbreiding als onderdeel van \cref{subsec:kleinere-functies}.
    Bij een batchcompilatie wordt de uitvoer in een nieuw tabblad getoond, terwijl bij contextcompilatie de uitvoer bij de relevant context staat.
    \item[Uitvoeringsfouten] Hier gaat het om crashes tijdens de uitvoering, zoals delen door nul.
    Ook hier wordt nog geen verdere verwerking van de foutboodschap gedaan (ook onderdeel van \cref{subsec:kleinere-functies}).
    \item[Tijdslimieten] TESTed heeft ondersteuning voor tijdslimieten in de judge.
    Dit laat toe om meer uitvoer te tonen dan als de tijdslimiet aan Dodona wordt overgelaten.
    Momenteel werkt de implementatie ervan als volgt: TESTed houdt bij hoe lang de beoordeling al duurt, en gebruikt de resterende tijd als een limiet op de uitvoering van een context.
    De eerste context heeft dan als limiet de volledige toegestane tijd (minus een percentage dat voorbehouden wordt voor TESTed zelf).
    De laatste context zal de kleinste limiet krijgen (de tijd die nog overschiet).
    Scenario's waar dit voorkomt zijn bijvoorbeeld oplossingen die te traag werken, maar ook oplossingen met bijvoorbeeld oneindige lussen.
    Als uitbreiding (opgenomen in \cref{subsec:kleinere-functies}) wordt beschreven dat er ook een tijdslimiet per context mogelijk zou kunnen zijn (zodat de eerste context niet de volledige beschikbare tijd krijgt, maar slechts een deel ervan).
    De hoofdreden om zelf de tijdslimiet te implementeren in TESTed is om zo de overige, niet-uitgevoerde testen ook te kunnen tonen.
    Momenteel is dat met een boodschap die uitlegt dat ze niet zijn uitgevoerd, maar op termijn is het de bedoeling in Dodona een bijkomende status toe te voegen.
    \item[Te grote uitvoer] Bij te grote uitvoer zal TESTed deze limiteren.
    Hierbij gaat het om scenario's zoals een oneindige lus die blijft schrijven naar stdout.
    TESTed beperkt tekstuele uitvoer tot tienduizend tekens.
    Dit is niet van toepassing op andere vormen van uitvoer: het inkorten van grote verzamelingen (bijvoorbeeld een lijst van duizend elementen) is opgenomen als uitbreiding in \cref{subsec:kleinere-functies}.
    \item[Te veel uitvoer] Hierbij wordt gedacht aan zaken zoals uitvoer op stdout of stderr terwijl die niet verwacht wordt.
    Standaard wordt een oefening als fout beschouwd indien er te veel uitvoer is, maar de auteur van de oefening kan kiezen om het teveel aan uitvoer te negeren (bijvoorbeeld als de studenten debugberichten schrijven naar stdout is dit niet in elke oefening een probleem).
    \item[Vroegtijdig stoppen van uitvoering] Hier gaat het om code van de studenten die bijvoorbeeld \mintinline{python}{exit(-5)} bevat.
    Dit zal standaard fout gerekend worden door TESTed.
    De exitcode is echter een uitvoerkanaal zoals elk ander, waar de verwachte waarde ingesteld kan worden.
    Is het dus de bedoeling om exitcode \texttt{-5} te krijgen, zal de oefening juist gerekend worden.

\end{description}
