\chapter{De universele judge}\label{ch:de-universele-judge}

\lettrine{H}{et antwoord} op de onderzoeksvraag uit het vorige hoofdstuk neemt de vorm aan van een nieuwe judge voor het Dodona-platform: de \term{universele judge}.
Deze kan oplossingen voor een opgave in meerdere programmeertalen evalueren.
Dit hoofdstuk licht de werking en implementatie van deze judge toe, beginnend met een algemeen overzicht, waarna elk onderdeel in meer detail besproken wordt.

\section{Overzicht}\label{sec:overzicht}

\begin{figure}
    \begin{adjustbox}{width=\textwidth}
        \input{figures/architecture.tikz}
    \end{adjustbox}
    \caption{Schematische voorstelling van de opbouw van de universele judge.}
    \label{fig:universal-judge}
\end{figure}

\Cref{fig:universal-judge} toont de opbouw van de judge op schematische wijze.
De twee dikkere stippellijnen geven een programmeertaalbarrière aan.
Dit betekent dat de programmeertaal kan verschillen in elk van de drie processen.
In meer detail is het stappenplan voor het evalueren van een oefening als volgt:

\begin{enumerate}
    \item De judge wordt opgestart en het testplan wordt geladen.
    Het gestarte proces noemen we het \term{kernproces}.
    \item Het testplan wordt gecontroleerd op vereiste functies, met andere woorden ondersteunt de oefening de gewenste taal?
    Als de oefening bijvoorbeeld programmeertaalspecifieke code bevat die enkel voor Java gegeven is, zal een oplossing in Python niet geëvalueerd kunnen worden.
    \item De code voor het evalueren van de oplossing wordt gegenereerd en eventueel gecompileerd, in wat we het \term{uitvoeringsproces} noemen.
    \item Nog steeds in het uitvoeringsproces, wordt elke context uit het testplan afzonderlijk uitgevoerd.
    Aangezien deze contexten onafhankelijk zijn van elkaar, kunnen ze in parallel worden uitgevoerd, indien de configuratie van de judge dit toelaat.
    \item De resultaten van de uitvoering van een context worden beoordeeld.
    Hiervoor zijn drie mogelijke manieren:
    \begin{enumerate}
        \item Programmeertaalspecifieke evaluatie.
        Hierbij wordt de evaluatie gedaan na de uitvoering gedaan in hetzelfde proces als de uitvoering.
        In het schema zitten we nog steeds in het uitvoeringsproces.
        \item Aangepaste evaluator.
        Hierbij is er evaluatiecode geschreven die los staat van de oplossing.
        De evaluatiecode kan in een andere programmeertaal geschreven zijn dan de oplossing.
        De aangepaste evaluator wordt gegenereerd, gecompileerd en uitgevoerd na het uitvoeren van de oplossing, in een nieuw proces: het \term{evaluatieproces}.
        \item Ingebouwde evaluatie.
        Hierbij is het de judge zelf die evalueert, waardoor dit vooral eenvoudige evaluaties betreft, zoals het vergelijken van geproduceerde uitvoer en verwachte uitvoer.
        Dit gebeurt in het kernproces.
    \end{enumerate}
    \item Tot slot verzamelt de judge alle evaluatieresultaten en stuurt ze door naar Dodona, waarna ze getoond worden aan de gebruiker.
\end{enumerate}


\section{Beschrijven van een opgave}\label{sec:testplan}

Elke evaluatie begint met het \term{testplan}, een document dat beschrijft hoe een oplossing voor een oefening geëvalueerd moet worden.
Het vervangt de taalspecifieke testen van de bestaande judges (ie.\ de jUnit-tests of de doctests in respectievelijk Java en Python).
Het bestaat uit verschillende onderdelen, die hierna besproken worden.

\subsection{Het testplan}\label{subsec:het-testplan}

Het eigenlijke testplan beschrijft de structuur van een evaluatie van een oplossing voor een oefening.
Qua structuur lijkt dit sterk op de structuur van de feedback zoals gebruikt door Dodona.
Dat de structuur van de oplossing in Dodona en van het testplan op elkaar lijken heeft als voordeel dat er moet geen mentale afbeelding tussen de structuur van het testplan en dat van Dodona bijgehouden worden.

Bij de keuze voor een formaat voor het testplan (json, xml, \ldots), hebben we vooraf enkele vereisten geformuleerd waaraan het gekozen formaat moet voldoen.
Het moet:

\begin{itemize}
    \item leesbaar zijn voor mensen,
    \item geschreven kunnen worden met minimale inspanning, met andere woorden de syntaxis dient eenvoudig te zijn, en
    \item programmeertaalonafhankelijk zijn.
\end{itemize}

Uiteindelijk is gekozen om het op te stellen in json.
Niet alleen voldoet json aan de vooropgestelde voorwaarden, het wordt ook door veel talen ondersteund.

Toch zijn er ook enkele nadelen aan het gebruik van json.
Zo is json geen beknopte of compacte taal om met de hand te schrijven.
Een oplossing hiervoor gebruikt de eigenschap dat veel talen json kunnen produceren: andere programma's kunnen desgewenst het testplan in het json-formaat genereren, waardoor het niet met de hand geschreven moet worden.
Hiervoor denken we aan een \termen{DSL} (\english{domain specific language}), maar dit valt buiten de thesis en wordt verder besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Een tweede nadeel is dat json geen programmeertaal is.
Terwijl dit de implementatie van de judge bij het interpreteren van het testplan weliswaar eenvoudiger maakt, is het tevens beperkend: beslissen of een testgeval moet uitgevoerd worden op basis van het resultaat van een vorig testgeval is bij wijze als voorbeeld niet mogelijk.
Ook deze beperking wordt uitgebreider besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

De structuur van het testplan vertaalt zich in json naar een reeks json-objecten, die hieronder beschreven worden.

\begin{description}
    \item[Tab] Een testplan bestaat uit verschillende \termen{tab}s of tabbladen.
    Deze komen overeen met de tabbladen in de gebruikersinterface van Dodona.
    Een tabblad kan een naam hebben, die zichtbaar is voor de gebruikers.
    \item[Context] Elk tabblad bestaat uit een of meerdere \termen{context}en.
    Een context is een onafhankelijke uitvoering van een evaluatie.
    De nadruk ligt op de "onafhankelijkheid".
    Elke context wordt in een nieuw proces en in een eigen map uitgevoerd, zodat de kans op het delen van informatie klein is.
    Hierbij willen we vooral onbedoeld delen van informatie (zoals statische variabelen of het overschrijven van bestanden) vermijden.
    De gemotiveerde student zal nog steeds informatie kunnen delen tussen de uitvoeringen, door bv.\ in een andere locatie een bestand aan te maken en later te lezen.
    \item[Testcase] Een context bestaat uit een of meerdere \termen{testcase}s of testgevallen.
    Een testgeval bestaat uit invoer en een aantal tests.
    De testgevallen kunnen onderverdeeld worden in twee soorten:
    \begin{description}
        \item[Main testcase] of hoofdtestgeval.
        Van deze soort is er maximaal een per context.
        Dit testgeval heeft als doel het uitvoeren van de main-functie (of de code zelf als het gaat om een scripttaal zoals Bash of Python).
        Als invoer voor dit testgeval kunnen enkel het standaardinvoerkanaal en de programma-argumenten meegegeven worden.
        \item[Normal testcase] of normaal testgeval.
        Hiervan kunnen er nul of meer zijn per context.
        Deze testgevallen zijn voor andere aspecten te testen, nadat de code van de gebruiker met success ingeladen is.
        De invoer is dan ook uitgebreider: het kan gaan om het standaardinvoerkanaal, functieoproepen en variabeletoekenningen.
        Een functieoproep of variabeletoekenning is verplicht (zonder functieoproep of toekenning aan een variabele is er geen code om te testen).
    \end{description}
    \item[Test] De uitvoer van een testgeval bestaat uit meerdere \term{test}s, die elk een aspect van een testcase controleren.
    Met aspect bedoelen we het standaarduitvoerkanaal, het standaardfoutkanaal, opgevangen uitzonderingen (\english{exceptions}), de teruggegeven waarden van een functieoproep (returnwaarde) en de inhoud van een bestand.
    Elke test bevat de verwachte uitvoer om mee te vergelijken of de code om het resultaat te evalueren.
\end{description}

\subsection{Dataserialisatie}\label{subsec:dataserialisatie}

In het testplan, zoals beschreven in de paragraaf hierboven, wordt gewag gemaakt van returnwaarden en variabeletoekenningen.
Aangezien het testplan programmeertaalonafhankelijk is, moet er dus een manier zijn om data uit de verschillende programmeertalen voor te stellen en te vertalen: het \term{serialisatieformaat}.

\subsubsection{Keuze van het formaat}

Zoals bij het testplan is een keuze voor een bepaald formaat gemaakt.
Daarvoor zijn er opnieuw enkele voorwaarden vooropgesteld, waaraan het serialisatieformaat moet voldoen.
Het formaat moet:

\begin{itemize}
    \item door mensen geschreven kunnen worden (\english{human writable}),
    \item onderdeel van het testplan kunnen zijn,
    \item in meerdere programmeertalen bruikbaar zijn, en
    \item de types ondersteunen die we willen aanbieden in het programmeertaalonafhankelijke deel van het testplan.
\end{itemize}

Een voor de hand liggende oplossing is ook hiervoor json gebruiken, en zelf in json een structuur op te stellen voor de waarden.
In tegenstelling tot het testplan bestaan er al een resem aan dataserialisatieformaten, waardoor het de moeite is om na te gaan of er geen bestaand formaat voldoet aan de vereisten.
Hiervoor is gestart van een overzicht op Wikipedia, \autocite{wiki2020}.
Uiteindelijk is niet gekozen voor een bestaand formaat, maar voor de json-oplossing.
De redenen hiervoor zijn samen te vatten als:

\begin{itemize}
    \item Het gaat om een binair formaat.
    Binaire formaten zijn uitgesloten op basis van de eerste twee voorwaarden die we opgesteld hebben: mensen kunnen het niet schrijven zonder hulp van bijkomende tools en het is moeilijk in te bedden in een json-bestand (zonder gebruik te maken van encoderingen zoals base64).
    Bovendien zijn binaire formaten moeilijker te implementeren in sommige talen.
    \item Het formaat ondersteunt niet alle gewenste types.
    Sommige formaten hebben ondersteuning voor complexere datatypes, maar niet voor alle complexere datatypes die wij nodig hebben.
    Uiteraard kunnen de eigen types samengesteld worden uit basistypes, maar dan biedt de ondersteuning voor de complexere types weinig voordeel, aangezien er toch een eigen dataschema voor die complexere types opgesteld zal moeten worden.
    \item Sommige formaten zijn omslachtig in gebruik.
    Vaak ondersteunen dit soort formaten meer dan wat wij nodig hebben.
    \item Het formaat is niet eenvoudig te implementeren in een programmeertaal waarvoor geen ondersteuning is.
    Sommige dezer formaten ondersteunen weliswaar veel talen, maar we willen niet dat het serialisatieformaat een beperkende factor wordt in welke talen door de judge ondersteund worden.
    Het mag niet de bedoeling zijn dat het implementeren van het serialisatieformaat het meeste tijd in beslag neemt.
\end{itemize}

Een lijst van de overwogen formaten met een korte beschrijving volgt:

\begin{description}
    \item[Apache Avro] Een volledig "systeem voor dataserialisatie".
    De specificatie van het formaat gebeurt in json, terwijl de eigenlijke data binair geëncodeerd wordt.
    Heeft uitbreidbare types, met veel ingebouwde types \autocite{avro}.
    \item[Apache Parquet] Minder relevant, dit is een bestandsformaat voor Hadoop \autocite{parquet}.
    \item[ASN.1] Staat voor \english{Abstract Syntax Notation One}, een ouder formaat uit de telecommunicatie.
    De hoofdstandaard beschrijft enkel de notatie voor een dataformaat.
    Andere standaarden beschrijven dan de serialisatie, in bv.\ binair formaat, json of xml.
    De meerdere serialisatievormen zijn in theorie aantrekkelijk: elke taal moet er slechts een ondersteunen, terwijl de judge ze allemaal kan ondersteunen.
    In de praktijk blijkt echter dat voor veel talen er slechts een serialisatieformaat is, en dat dit vaak het binaire formaat is \autocite{x680}.
    \item[Bencode] Schema gebruikt in BitTorrent.
    Het is gedeeltelijk binair, gedeeltelijk in text \autocite{cohen2017}.
    \item[Binn] Binair dataformaat \autocite{ramos2019}.
    \item[BSON] Een binaire variant op json, geschreven voor en door MongoDB \autocite{bson}.
    \item[CBOR] Een lichtjes op json gebaseerd formaat, ook binair.
    Heeft een goede standaard, ondersteunt redelijk wat talen \autocite{rfc7049}.
    \item[FlatBuffers] Lijkt op ProtocolBuffers, allebei geschreven door Google, maar verschilt wat in implementatie van ProtocolBuffers.
    De encodering is binair \autocite{flatbuffers}.
    \item[Fast Infoset] Is eigenlijk een manier om xml binair te encoderen (te beschouwen als een soort compressie voor xml), waardoor het minder geschikt voor ons gebruik wordt \autocite{x981}.
    \item[Ion] Een superset van json, ontwikkeld door Amazon.
    Het heeft zowel een tekstuele als binaire voorstelling.
    Naast de gebruikelijke json-types, bevat het enkele uitbreidingen. \autocite{ion}.
    \item[MessagePack] Nog een binair formaat dat lichtjes op json gebaseerd is.
    Lijkt qua types sterk op json.
    Heeft implementaties in veel talen \autocite{messagepack}.
    \item[OGDL] Afkorting voor \english{Ordered Graph Data Language}.
    Daar het om een serialisatieformaat voor grafen gaat, is het niet nuttig voor ons doel \autocite{ogdl}.
    \item[OPC Unified Architecture] Een protocol voor intermachinecommunicatie.
    Complex: de specificatie bevat 14 documenten, met ongeveer 1250 pagina's \autocite{tr62541}.
    \item[OpenDLL] Afkorting voor de \english{Open Data Description Language}.
    Een tekstueel formaat, bedoeld om arbitraire data voor te stellen.
    Wordt niet ondersteunt in veel programmeertalen, in vergelijking met bv.\ json \autocite{openddl}.
    \item[ProtocolBuffers] Lijkt zoals vermeld sterk op FlatBuffers, maar heeft nog extra stappen nodig bij het encoderen en decoderen, wat het minder geschikt maakt \autocite{protobuf}.
    \item[Smile] Nog een binaire variant van json \autocite{smile}.
    \item[SOAP] Afkorting voor \english{Simple Object Access Protocol}.
    Niet bedoeld als formaat voor dataserialisatie, maar voor communicatie tussen systemen over een netwerk \autocite{soap}.
    \item[SDXF] Binair formaat voor data-uitwisseling.
    Weinig talen ondersteunen dit formaat \autocite{rfc3072}.
    \item[Thrift] Lijkt sterk op ProtocolBuffers, maar geschreven door Facebook \autocite{slee2007}.
    \item[UBJSON] Nog een binaire variant van json \autocite{ubjson}.

\end{description}

Geen enkel overwogen formaat heeft grote voordelen tegenover een eigen structuur in json.
Daarenboven hebben vele talen het nadeel dat ze geen json zijn, waardoor we een nieuwe taal moeten inbedden in het bestaande json-testplan.
Dit nadeel, gekoppeld met het ontbreken van voordelen, heeft geleid tot de keuze voor json.

\subsubsection{Dataschema}

Het formaat is, zoals al vermeld, json.
Het bijhorende dataschema is met opzet eenvoudig gehouden, om implementaties makkelijker te maken.
Concreet wordt een waarde voorgesteld als een json-object dat bestaat uit de (geëncodeerde) waarde en het type van die waarde.
Een voorbeeld is \cref{lst:serialisation}.

\begin{listing}
    \inputminted{json}{code/format.json}
    \caption{Een lijst bestaande uit twee getallen, geëncodeerd in het serialisatieformaat.}
    \label{lst:serialisation}
\end{listing}

\subsubsection{Datatypes}

Naast de encodering van de data is er een tweede aspect van het serialisatieformaat: de datatypes.
Het formaat ondersteunt de meeste basistypes die in bijna elke programmeertaal beschikbaar zijn.
Hieronder volgt lijst met een korte omschrijving van de ondersteunde types.
Hierbij is er een speciaal type, aangeduid met een ster (*), dat niet gebruikt wordt bij het encoderen van data.
Het type \texttt{literal} bedoeld voor waarden die eigenlijk geen data zijn, maar verwijzingen naar bv.\ een variabele (een \english{identifier} in de programmeertaal).
Dit is nuttig bij functieargumenten (zo kunnen variabelen worden gebruikt bij een functieoproep).

\begin{description}
    \item[\texttt{integer}] Gehele getallen.
    \item[\texttt{rational}] Rationale getallen.
    \item[\texttt{text}] Een tekenreeks of string.
    \item[\texttt{literal*}] Een tekstuele waarde die rechtstreeks als \english{identifier} wordt gebruikt.
    Deze waarde wordt enkel gebruikt bij het aangeven van de types van functie-argumenten.
    \item[\texttt{unknown}] Dit type wordt gebruikt als er onbekende types zijn bij het encoderen van een waarde.
    Bij het omzetten van een waarde uit het serialisatieformaat naar een taal, worden waarden van dit type genegeerd.
    \item[\texttt{boolean}] Een Boolese waarde (of boolean).
    \item[\texttt{list}] Een wiskundige rij, wat wil zeggen dat de volgorde belangrijk is en dat dubbele elementen toegelaten zijn.
    Merk op dat sommige talen meerdere implementaties hebben voor het concept van lijst.
    Het is de implementatie vrij om te kiezen welk concept gebruikt wordt.
    Zo wordt bijvoorbeeld in de Java-implementatie \texttt{List} in plaats van \texttt{array} gebruikt, om consistent te zijn met de implementatie van \texttt{set} en \texttt{object}.
    \item[\texttt{set}] Een wiskundige verzameling, wat wil zeggen dat de volgorde niet belangrijk is en dat dubbele elementen niet toegelaten zijn.
    \item[\texttt{object}] Een wiskundige afbeelding: elk element wordt afgebeeld op een ander element.
    In Java is dit bijvoorbeeld een \texttt{Map}, in Python een \texttt{dict} en in Javascript een \texttt{object}.
    \item[\texttt{nothing}] Geeft aan dat er geen waarde is, ook wel \texttt{null}, \texttt{None} of \texttt{nil} genoemd.
\end{description}

\subsection{Functieoproepen en assignments}\label{subsec:functieoproepen}

Een ander onderdeel van het testplan verdient ook speciale aandacht: het toekennen van variabelen (\english{assignments}) en de functieoproepen.

In heel wat oefeningen, en zeker in objectgerichte programmeertalen, is het toekennen van een waarde aan een variabele om deze later te gebruiken onmisbaar.
Bijvoorbeeld zou een opgave kunnen bestaan uit het implementeren van een klasse.
Bij de evaluatie dient dan een instantie van die klasse aangemaakt te worden, waarna er methoden kunnen aangeroepen worden, zoals hieronder geïllustreerd in een fictief voorbeeld.

\inputminted{java}{code/assignment.jshell}

Concreet is ervoor gekozen om het testplan niet uit te breiden met generieke statements of expressions, maar de ondersteuning te beperken tot assignments en functieoproepen.
Dit om de implementatie van de vertaling van het testplan naar de ondersteunde programmeertalen niet nodeloos ingewikkeld te maken.
Een functieoproep ziet er als volgt uit:

\inputminted{json}{code/function.json}

Het type van de functie geeft aan welk soort functie het is.
Mogelijke waarden zijn momenteel \texttt{top}, \texttt{object}, \texttt{constructor} en \texttt{identity}.
De laatste soort is een speciaal geval, waarbij geen functienaam moet gegeven worden en exact één argument toegelaten is.
Die functie zal dan dat ene argument teruggeven.
De naam van de functie benoemt eenvoudig welke functie opgeroepen moet worden.
Het object van de functie laat toe om functies op objecten op te roepen.
De lijst van argumenten kan nul of meer waarden bevatten.
Deze waarden moeten in het formaat zijn zoals aangegeven in het vorige deel over de serialisatie van waarden.

Een beperking is dat het niet mogelijk is om rechtstreeks een functieoproep te doen als argument voor een andere functie,
of toch niet op een programmeertaalonafhankelijke manier.
Een oproep als \texttt{oproep(hallo(), 5)} is niet mogelijk.
Bij dergelijke dingen zal de functieoproep eerst aan variabele moeten toegekend worden, bv.\ \texttt{var param = hallo()}, waarna deze variabele als argument met type \texttt{literal} kan gegeven worden aan de oorspronkelijke functie: \texttt{oproep(param, 5)}.
De aandachtige lezer zal opmerken dat met die functieargumenten van het type \texttt{literal} rond deze beperking kan gewerkt worden, aangezien de tekstuele waarde van een dergelijk argument letterlijk in de taal komt.
We raden deze omweg echter ten sterkste af: dit maakt het testplan taalafhankelijk, want niet elke programmeertaal implementeert functieoproepen op eenzelfde wijze.

Dit brengt ons bij de variabeletoekenning of \english{assignment}.
In ons testplan beperkt dit zich tot het toekennen van een naam aan het resultaat van een functieoproep:

\inputminted{json}{code/assignment.json}

De \texttt{name} is de naam die aan de variabele gegeven zal worden.
Het veldje \texttt{expression} moet een object zijn dat een functieoproep voorstelt (zie hierna).
In een beperkt aantal gevallen kan de judge het type van de variabele afleiden uit de functieoproep, maar in veel gevallen is het nodig om zelf het type mee te geven.
Dit type moet een van de ondersteunde types zijn uit het serialisatieformaat, zij het dat er ondersteuning is voor eigen types (zoals een klasse die geïmplementeerd moest worden door de student).

Een gecombineerd voorbeeld staat hieronder.
Hier wordt de string \texttt{'Dodona'} toegekend aan een variabele met naam \texttt{name}.
De judge kan het type afleiden, dus we moeten niet opgeven dat \texttt{name} een \texttt{str} is.

\inputminted{json}{code/assign-variable.json}

\subsection{Vereiste functies}\label{subsec:vereiste-functies}

Voor elk onderdeel van een testplan wordt afgeleid welke functies een taal moet ondersteunen om van dat testplan gebruik te kunnen maken.
Bevat een testplan bijvoorbeeld waarden met als type \texttt{set}, dan kunnen enkel programmeertalen die een verzameling ondersteunen gebruikt worden.
Dat zijn bijvoorbeeld Python en Java, maar geen Bash.
Het testplan is zo opgebouwd dat het afleiden van de vereiste functies geen tussenkomst van de persoon die het testplan opstelt vereist.

\section{Uitvoeren van de oplossing}\label{sec:uitvoeren-van-de-oplossing}

Nadat de student een oplossing heeft ingediend en de judge is opgestart, begint de evaluatie van de oplossing.
Eerst wordt de uit te voeren code gegenereerd, waarna de uitvoering van die code volgt.

\subsection{Genereren van code}\label{subsec:genereren-van-code}

Het genereren van de code gebeurt met een sjabloonsysteem genaamd Mako \autocite{mako}.
Dit soort systemen wordt traditioneel gebruikt bij webapplicaties (zoals Ruby on Rails met \textsc{erb}, Phoenix met \textsc{eex}, Laravel met Blade, enz.) om een html-pagina te genereren.
In ons geval zijn de sjablonen verantwoordelijk voor de vertaling van programmeertaalonafhankelijke concepten naar implementaties in specifieke talen.
Voorbeelden hiervan zijn functieoproepen, assignments, enz.
Ook zijn de sjablonen verantwoordelijk voor het genereren van de code die de oplossing van de student zal oproepen en evalueren.

\subsubsection{Sjablonen}

Het aantal sjablonen en hoe ze geïmplementeerd worden is in principe vrij, zij het dat de judge wel enkele standaardsjablonen nodig heeft, waaraan vastgelegde parameters meegegeven worden.
Deze verplichte sjablonen zijn:
\begin{description}
    \item[\texttt{assignment}] Vertaalt een assignment uit het testplan naar code.
    \item[\texttt{context}] Het sjabloon dat de code genereert om een context te evalueren.
    Deze code moet uitvoerbaar zijn (d.w.z.\ een main-functie bevatten of een script zijn).
    \item[\texttt{selector}] Het sjabloon de code genereert om een bepaalde context uit te voeren.
    Om performantieredenen (later hierover meer) wordt de code van alle contexten soms uit een keer gegenereerd en gecompileerd.
    Aan de hand van een parameter (de naam van de context), wordt bij het uitvoeren de code voor de juiste context gekozen.
    \item[\texttt{evaluator\_executor}] Genereert code om een aangepaste evaluator te starten.
    \item[\texttt{function}] Vertaalt een functie-oproep naar code.
    \item[\texttt{value}] Vertaalt een waarde uit het serialisatieformaat naar code.
\end{description}

Daarnaast moet het encoderen naar serialisatieformaat ook geïmplementeerd worden in elke taal.
Veel talen hebben dus nog enkele bijkomende bestanden met code.
In alle bestaande implementaties is dit geïmplementeerd als een module of klasse met naam \texttt{Value}.

\subsubsection{Modus}\label{sss:modus}

De judge ondersteunt twee uitvoeringsmodi:
\begin{description}
    \item[Precompilatiemodus] In deze modus wordt de code voor alle contexten in een keer gecompileerd.
    Dit wordt gedaan om performantieredenen.
    In talen die resulteren in een uitvoerbaar bestand (zoals Haskell, C/C++), resulteert deze modus in één uitvoerbaar bestand voor alle contexten.
    Bij het uitvoeren wordt dan aan de hand van een parameter de juiste context uitgevoerd (met het \texttt{selector}-sjabloon van hierboven).
    \item[Individuele modus] Hierbij wordt elke context afzonderlijk gecompileerd.
\end{description}

Een flowchart van het generen van de code is \cref{fig:generation}, met een testplan met vier contexten.
In dit diagram zijn de stappen voor de individuele modus in het \textcolor{ugent-we}{aquablauw}.
De stappen van de precompilatiemodus zijn in het \textcolor{ugent-blue}{UGent-blauw}.
De gemeenschappelijke stappen zijn in het zwart.

Dit gedrag is configureerbaar in het testplan, maar standaard wordt de precompilatiemodus gebruikt, zij het met terugval op de individuele modus.
Deze terugval is handig voor talen met sterke compilatie.
Een voorbeeldscenario is als volgt: stel een oefening waarbij de student twee functies moet implementeren.
De student implementeert de eerste functie en dient in om al feedback te krijgen.
Bij talen als Java of Haskell zal dit niet lukken: daar alle contexten in een keer gecompileerd worden, zal de ontbrekende tweede functie ervoor zorgen dat de volledige compilatie faalt.
In individuele modus is dit geen probleem: de contexten die de eerste functie testen zullen compileren en kunnen uitgevoerd worden.
De individuele modus brengt wel een niet te onderschatten kost qua uitvoeringstijd met zich mee (zie ook \cref{ch:beperkingen-en-toekomstig-werk}).

\begin{figure}
    \begin{adjustbox}{width=\textwidth}
        \input{generated/generation.tikz}
    \end{adjustbox}
    \caption{
        Schematische voorstelling van het genereren van de code.
        Gemeenschappelijke stappen zijn zwart, stappen voor de individuele modus \textcolor{ugent-we}{aquablauw} en stappen voor de precompilatiemodus \textcolor{ugent-blue}{UGent-blauw}.
    }
    \label{fig:generation}
\end{figure}

\subsection{Uitvoeren van de code}\label{subsec:uitvoeren-van-de-code}

Na het genereren wordt alle code gecompileerd (bij de talen waar dit mogelijk is).
Dit gebeurt ofwel eenmaal voor alle contexten afzonderlijk, ofwel eenmaal voor alle contexten samen, afhankelijk van de modus.
De werking hiervan wordt behandeld in \cref{sss:modus} en \cref{fig:generation}.

Vervolgens wordt elke context uit het testplan uitgevoerd en wordt de uitvoer verzameld.
Het uitvoeren zelf gebeurt op de normale manier dat een programmeertaal uitgevoerd wordt: via de commandoregel.
Deze aanpak heeft een voordeel: er is geen verschil tussen hoe de judge de code van de student uitvoert en hoe de student zijn code zelf uitvoert op zijn eigen computer.
Dit voorkomt dat er subtiele verschillen in de resultaten sluipen.

Indien de configuratie het toelaat, worden de contexten in parallel uitgevoerd.
Om te vermijden dat bestanden of uitvoer overschreven wordt, wordt de gecompileerde code gekopieerd naar een aparte map, waar de uitvoer gebeurt.
\Cref{lst:mapstructuur} illustreert dit met een voorbeeld voor een oplossing in Java.
Deze mapstructuur stelt de toestand van de werkmap voor na het uitvoeren van de code.
In de map \texttt{common} zit alle code en de gecompileerde bestanden.
Voor elke context worden de gecompileerde bestanden gekopieerd naar een andere map, bv.\ \texttt{context-1}, wat de map is voor context \texttt{1} van het testplan.

\begin{listing}
    \inputminted{text}{code/dir-listing.txt}
    \caption{Mapstructuur na het uitvoeren van de evaluatie van een oplossing in Python.
    Context 0-0 staat voor de eerste context van het eerste tabblad.}
    \label{lst:mapstructuur}
\end{listing}

\subsection{Verzamelen van resultaten}\label{subsec:verzamelen-van-resultaten}

De uitvoering van een oplossing genereert resultaten die door de judge geïnterpreteerd moeten worden.
Er zijn verschillende soorten uitvoerresultaten (zoals vermeld heeft elke soort uitvoer een aparte test in het testplan).
We noemen de verschillende soorten uitvoer de \term{uitvoerkanalen}.
Twee ervan, het standaarduitvoer- en standaardfoutkanaal komen overeen met de standaarduitvoer- en standaardfoutstroom van het proces dat de code uitvoert.
Uitvoer naar een bestand (het bestandskanaal) resulteert in een bestand en vormt ook geen probleem.
De overige uitvoerkanalen, het kanaal voor exceptions (uitzonderingenkanaal) en het returnkanaal (voor returnwaarden) worden geschreven naar een bestand.
Het is namelijk niet in elke taal mogelijk om nieuwe kanalen te openen.
De sjablonen krijgen de verwachte namen van die bestanden mee van de judge, maar zijn wel verantwoordelijk voor het openen, schrijven en sluiten van deze bestanden.
Deze naam bevat willekeurige tekens, zodat de kans dat deze bestanden overschreven worden door de oplossing minimaal is.
De bestanden waar de exceptions and returnwaarden naartoe geschreven worden, worden na de uitvoering gelezen door de judge.
Hierna worden alle kanalen op dezelfde manier behandeld door de judge.

In de bestaande implementaties ligt de verantwoordelijkheid om naar deze bestanden te schrijven bij de \texttt{Value}-module van hierboven.

\section{Evalueren van een oplossing}\label{sec:evalueren-van-een-oplossing2}

Na de uitvoering van elke context heeft de judge alle relevant uitvoer verzamelt, zoals de standaardkanalen.
Deze uitvoer moet vervolgens beoordeeld worden om na te gaan in hoeverre deze uitvoer voldoet aan de verwachte uitvoer.
Dit kan op drie manieren:
\begin{enumerate}
    \item Ingebouwde evaluator: de oplossing wordt geëvalueerd in de judge zelf.
    \item Aangepaste evaluator: de oplossing wordt geëvalueerd door eigen code, maar dezelfde wordt gebruikt voor alle programmeertalen, in het evaluatieproces.
    \item Taalspecifieke evaluator: de oplossing wordt onmiddellijk na de uitvoering geëvalueerd in het uitvoeringsproces.
\end{enumerate}

\subsection{Ingebouwde evaluator}\label{subsec:ingebouwde-evaluator}

Voor eenvoudige evaluaties volstaat de ingebouwde evaluator van de judge.
Momenteel zijn er drie soorten ingebouwde evaluatoren, die hieronder besproken worden.

\subsubsection{Tekstevaluator}

Deze evaluator vergelijkt de verkregen uitvoer van een uitvoerkanaal (standaarduitvoer, returnwaarde, \ldots) met de verwachte uitvoer uit het testplan.
Alle data worden als string behandeld.
Deze evaluator biedt enkele opties om het gedrag aan te passen:

\begin{description}
    \item[\texttt{ignoreWhitespace}] Witruimte voor en na het resultaat wordt genegeerd.
    \item[\texttt{caseInsensitive}] Er wordt geen rekening gehouden met het verschil tussen hoofdletters en kleine letters.
    \item[\texttt{tryFloatingPoint}] De waarde moet geïnterpreteerd worden als een zwevendekommagetal (\english{floating point}), waarbij rekening gehouden wordt met de foutmarge.
    \item[\texttt{applyRounding}] Of zwevendekommagetallen afgrond moeten worden.
    Indien wel wordt het aantal cijfers genomen van de optie \texttt{roundTo}.
    \item[\texttt{roundTo}] Het aantal cijfers na de komma.
    Enkel nuttig als \texttt{applyRounding} waar is.
\end{description}

\subsubsection{Bestandsevaluator}

Hiermee kan een geproduceerd bestand vergeleken worden met een gegeven bestand uit het testplan.
Het gaat om tekstuele bestanden.
Deze evaluator kan werken in drie modi:

\begin{description}
    \item[\texttt{exact}] Beide bestanden moet exact hetzelfde zijn, inclusief regeleindes.
    \item[\texttt{lines}] Elke regel wordt vergeleken met overeenkomstige regel in het andere bestand.
    De evaluatie van de lijnen is exact, maar zonder de regeleindes.
    \item[\texttt{values}] Elke regel wordt geïnterpreteerd als een tekstuele waarde en vergeleken met de tekstevaluator.
    In deze modus worden kunnen ook alle opties van de tekstevaluator gebruikt worden.
\end{description}

\subsubsection{Waarde-evaluator}

Deze evaluator vergelijkt twee waarden, zoals gedefinieerd door het serialisatieformaat.
De twee waarden moeten exact overeenkomen, met uitzondering van zwevendekommagetallen.


\subsection{Aangepaste evaluator}\label{subsec:aangepaste-evaluator}

Voor de aangepaste evaluator moet een bestand geschreven worden in een programmeertaal naar keuze.
Het resultaat van de uitvoering wordt vervolgens geserialiseerd en gedeserialiseerd naar het evaluatieproces.
Hoe een evaluator moet geïmplementeerd worden, hangt af van de programmeertaal.

In Python bestaat de aangepaste evaluator uit een module met een functie die voldoet aan de definitie, zoals gegeven in \cref{lst:evaluation-python-custom}.
De judge stelt ook een module \texttt{evaluation\_utils} ter beschikking.
De functie van hierboven moet dan één oproep doen naar de functie \texttt{evaluated()}.
Deze module is redelijk eenvoudig, zoals te zien in \cref{lst:evaluation-util-python}.

\begin{listing}
    \inputminted{python}{code/custom_signature.py}
    \caption{De definitie van de aangepaste evaluator.}
    \label{lst:evaluation-python-custom}
\end{listing}

\begin{listing}
    \inputminted{python}{../../judge/runners/templates/python/evaluation_utils.py}
    \caption{De implementatie van de module \texttt{evaluation\_utils}}
    \label{lst:evaluation-util-python}
\end{listing}

In de Java-implementatie is de situatie gelijkaardig: het gaat om het implementeren van een abstracte klasse, die ook dienst doet als de module van Python.
Deze klassen en haar ouder staan in \cref{lst:evaluation-util-java,lst:evaluation-java-custom}.

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractEvaluator}.}
    \label{lst:evaluation-util-java}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractCustomEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractCustomEvaluator}.}
    \label{lst:evaluation-java-custom}
\end{listing}

\subsubsection{Taalspecifieke evaluator}

De taalspecifieke evaluator lijkt sterk op de aangepaste evaluator.
is de eenvoudigste: deze neemt een codefragment met daarin één functie \texttt{evaluate}, die één argument aanvaardt, de geproduceerde waarde.
Waar de geproduceerde waarde bij de aangepaste evaluator in het serialisatieformaat moet kunnen, is dit hier niet het geval: de functie wordt rechtstreeks opgeroepen tijdens de uitvoering.
In Python wordt dit \cref{lst:evaluation-python-specific}, in Java \cref{lst:evaluation-java-specific}.
Om het resultaat van de evaluatie aan de judge te geven, wordt dezelfde \texttt{evaluated}-functie als bij de aangepaste evaluator gebruikt (respectievelijk \cref{lst:evaluation-util-python,lst:evaluation-util-java})

\begin{listing}
    \inputminted{python}{code/specific_signature.py}
    \caption{De definitie van de taalspecifieke evaluator.}
    \label{lst:evaluation-python-specific}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractSpecificEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractSpecificEvaluator}.}
    \label{lst:evaluation-java-specific}
\end{listing}
