\chapter{TESTed}\label{ch:de-universele-judge}

\lettrine{I}{n het kader} van deze masterproef werd een prototype geïmplementeerd van een judge voor Dodona.
Het doel hiervan is een antwoord te bieden aan de onderzoeksvraag uit het vorige hoofdstuk en de beperkingen van deze aanpak in kaart te brengen.
Deze judge heeft de naam \term{TESTed} gekregen.
Bij TESTed is een oefening programmeertaalonafhankelijk en kunnen oplossingen in verschillende programmeertalen beoordeeld worden aan de hand van een en dezelfde specificatie.
Dit hoofdstuk begint met het ontwerp van de judge toe te lichten, waarna elk onderdeel in meer detail besproken wordt.
Tot slot wordt afgesloten met een samenvattend overzicht.

\section{Overzicht}\label{sec:ontwerp}

\subsection{Architecturaal ontwerp}\label{subsec:architecturaal-overzicht}

\Cref{fig:universal-judge} toont het architecturaal ontwerp van TESTed.
De twee stippellijnen geven programmeertaalbarrières aan, en verdelen TESTed in drie logisch omgevingen:

\begin{enumerate}
    \item TESTed zelf is geschreven in Python: in het middelste deel staat de programmeertaal dus vast.
    Dit onderdeel is verantwoordelijk voor de regie van de beoordeling op basis van het testplan.
    \item De ingediende oplossing wordt uitgevoerd in de \term{uitvoeringsomgeving}, waar de programmeertaal overeenkomt met de programmeertaal van de oplossing.
    \item Tot slot is er nog de \term{evaluatieomgeving}, waar door de lesgever geschreven beoordelingscode wordt uitgevoerd.
    Deze moet niet in dezelfde programmeertaal als de oplossing of TESTed geschreven zijn.
\end{enumerate}

\subsection{Stappenplan bij een beoordeling}\label{subsec:stappenplan-van-een-beoordeling}

De rest van het hoofdstuk bespreekt alle onderdelen van en stappen die gebeuren bij een beoordeling van een ingediende oplossing in detail.
In \cref{fig:tested-flow} zijn deze stappen gegeven als een flowchart, en een uitgeschreven versie volgt:

\begin{enumerate}
    \item De Docker-container voor TESTed wordt gestart.
    Dodona stelt de invoer ter beschikking aan de container: het testplan komt uit de oefening, terwijl de ingediende oplossing en de configuratie uit Dodona komen.
    \item Als eerste stap wordt gecontroleerd dat het testplan de programmeertaal van de ingediende oplossing ondersteunt.
    De programmeertaal van de oplossing wordt gegeven via de configuratie uit Dodona.
    Merk op dat de ingediende oplossing zelf hierbij niet nodig is: deze controle zou idealiter gebeuren bij het importeren van de oefening in Dodona, zodat Dodona weet in welke programmeertalen een bepaalde oefening aangeboden kan worden (zie \cref{ch:beperkingen-en-toekomstig-werk}).
    Als het testplan bijvoorbeeld programmeertaalspecifieke code bevat die enkel in Java geschreven is, zal een oplossing in Python niet beoordeeld kunnen worden.
    Bevat het testplan bijvoorbeeld een functie die een verzameling moet teruggeven, dan zullen talen als Bash niet in aanmerking komen.
    \item De testcode wordt gegenereerd voor elke context uit het testplan, wat resulteert in de testcode voor elke context.
    Deze stap is de overgang naar de \term{uitvoeringsomgeving}.
    \item De code wordt optioneel gecompileerd.
    Dit kan op twee manieren gebeuren (meer details in \cref{subsec:uitvoeren-van-de-code}):
    \begin{enumerate}
        \item Precompilatiemodus: hierbij wordt de testcode van alle contexten verzameld en gecompileerd tot één uitvoerbaar bestand (executable).
        Dit heeft als voordeel dat er slechts een keer een compilatie nodig is, wat goed is voor de performantie.
        \item Individuele modus: hierbij wordt de testcode voor elke context afzonderlijk gecompileerd tot een uitvoerbaar bestand.
    \end{enumerate}
    In talen die geen compilatie nodig hebben of ondersteunen, wordt deze stap overgeslagen.
    \item Nu kan het uitvoeren van de beoordeling zelf beginnen: de gegenereerde code wordt uitgevoerd (nog steeds in de uitvoeringsomgeving).
    Elke context uit he testplan wordt in een afzonderlijk subproces uitgevoerd, teneinde het delen van informatie tegen te gaan.
    De onafhankelijkheid van de contexten laat ons ook toe om contexten in parallel uit te voeren.
    Toch moeten we opmerken dat het parallel uitvoeren vooral een theoretische mogelijkheid is: in werkelijkheid gebruiken er meerdere gebruikers tegelijk Dodona, waardoor er ook meerdere oefeningen tegelijk beoordeeld worden, wat op zijn beurt ervoor zorgt dat er geen grote ruimte voor snelheidswinst door parallellisatie binnen een judge is.
    \item De uitvoering van de executable in de vorige stap resulteert in resultaten (voor elke context), zoals de standaarduitvoerstroom, de standaardfoutstroom, returnwaardes, exceptions of exitcodes.
    Deze bundel resultaten wordt nu geëvalueerd op juistheid.
    Hiervoor zijn drie mogelijke manieren:
    \begin{enumerate}
        \item Programmeertaalspecifieke evaluatie (afgekort tot SE in de flowchart).
        De code voor de evaluatie is opgenomen in de executable en wordt onmiddellijk uitgevoerd in hetzelfde proces.
        Via deze mogelijkheid kunnen taalspecifieke aspecten gecontroleerd worden.
        Daar de evaluatie in hetzelfde proces gebeurt, blijft dit in de uitvoeringsomgeving.
        \item Geprogrammeerde evaluatie (afgekort tot PE in de flowchart).
        Hierbij is er evaluatiecode geschreven die los staat van de oplossing, waardoor deze evaluatiecode ook in een andere programmeertaal geschreven kan zijn.
        De code ter uitvoering van de geprogrammeerde evaluatiecode wordt gegenereerd en dan uitgevoerd.
        Het doel van deze modus is om complexe evaluaties toe te laten op een programmeertaalonafhankelijke manier.
        Deze stap vindt plaats in de evaluatieomgeving.
        \item Generieke evaluatie.
        Hierbij evalueert TESTed zelf het resultaat.
        Deze modus is bedoeld voor gestandaardiseerde evaluaties, zoals het vergelijken van geproduceerde uitvoer en verwachte uitvoer.
        Hier gebeurt de evaluatie binnen TESTed zelf.
    \end{enumerate}
    \item Tot slot verzamelt TESTed alle evaluatieresultaten en stuurt ze gebundeld door naar Dodona, waarna ze getoond worden aan de gebruiker.
\end{enumerate}

\begin{figure}
    \centering
    \input{figures/architecture.tex}
    \caption{Schematische voorstelling van het architecturale ontwerp van de TESTed.}
    \label{fig:universal-judge}
\end{figure}

\begin{figure}
    \centering
    \input{figures/flow.tex}
    \caption{
        Flowchart van een beoordeling door TESTed.
        In het schema worden kleuren gebruikt als er een keuze gemaakt moet worden, en slechts een van de mogelijkheden gebruikt wordt.
        De afkortingen PE, GE en SE staan respectievelijk voor geprogrammeerde evaluatie, generieke evaluatie en (programmeertaal)specifieke evaluatie.
    }
    \label{fig:tested-flow}
\end{figure}


\section{Beschrijven van een oefening}\label{sec:testplan}

De beoordeling van een ingediende oplossing van een oefening begint bij de invoer die TESTed krijgt.
Centraal in deze invoer is een \term{testplan}, een specificatie die op een programmeertaalonafhankelijke manier beschrijft hoe een oplossing voor een oefening beoordeeld moet worden.
Het vervangt de taalspecifieke testen van de bestaande judges (ie.\ de jUnit-tests of de doctests in respectievelijk Java en Python).
Het testplan \latin{sensu lato} wordt opgedeeld in verschillende onderdelen, die hierna besproken worden.

\subsection{Het testplan}\label{subsec:het-testplan}

Het testplan \latin{sensu stricto} beschrijft de structuur van de beoordeling van een ingediende oplossing voor een oefening.
Deze structuur lijkt qua opbouw sterk op de structuur van de feedback zoals gebruikt door Dodona.
Dat de structuur van de oplossing in Dodona en van het testplan op elkaar lijken, heeft als voordeel dat er geen mentale afbeelding moet gemaakt worden tussen de structuur van het testplan en dat van Dodona.
Concreet is de structuur een hiërarchie met volgende elementen:

\begin{description}
    \item[Plan] Het top-level object van het testplan.
    Dit object bevat twee belangrijke objecten: de tabbladen en de configuratie.
    Deze configuratie is de plaats om opties aan TESTed mee te geven, zowel voor de TESTed zelf als voor programmeertaalspecifieke dingen.
    \item[Tab] Een testplan bestaat uit verschillende \termen{tab}s of tabbladen.
    Deze komen overeen met de tabbladen in de gebruikersinterface van Dodona.
    Een tabblad kan een naam hebben, die zichtbaar is voor de gebruikers.
    \item[Context] Elk tabblad bestaat uit een of meerdere \termen{context}en.
    Een context is een onafhankelijke uitvoering van een evaluatie.
    De nadruk ligt op de "onafhankelijkheid", zoals al vermeld.
    Elke context wordt in een nieuw proces en in een eigen map (directory) uitgevoerd, zodat de kans op het delen van informatie klein is.
    Hierbij willen we vooral onbedoeld delen van informatie (zoals statische variabelen of het overschrijven van bestanden) vermijden.
    De gemotiveerde student zal nog steeds informatie kunnen delen tussen de uitvoeringen, door bv.\ in een andere locatie een bestand aan te maken en later te lezen.
    \item[Testcase] Een context bestaat uit een of meerdere \termen{testcase}s of testgevallen.
    Een testgeval bestaat uit invoer en een aantal tests.
    De testgevallen kunnen onderverdeeld worden in twee soorten:
    \begin{description}
        \item[Main testcase] of hoofdtestgeval.
        Van deze soort is er maximaal één per context (dus geen hoofdtestgeval is ook mogelijk).
        Dit testgeval heeft als doel het uitvoeren van de main-functie (of de code zelf als het gaat om een scripttaal zoals Bash of Python).
        Als invoer voor dit testgeval kunnen enkel het standaardinvoerkanaal en de programma-argumenten meegegeven worden.
        De exitcode van een uitvoering kan ook enkel in het hoofdtestgeval gecontroleerd worden.
        \item[Normal testcase] of normaal testgeval.
        Hiervan kunnen er nul of meer zijn per context.
        Deze testgevallen dienen om andere aspecten van de ingediende oplossing te testen, nadat de code van de gebruiker met success ingeladen is.
        De invoer is dan ook uitgebreider: het kan gaan om het standaardinvoerkanaal, functieoproepen en variabeletoekenningen.
        Een functieoproep of variabeletoekenning is verplicht (zonder functieoproep of toekenning aan een variabele is er geen code om te testen).
    \end{description}
    Het hoofdtestgeval wordt altijd als eerste uitgevoerd.
    Dit is verplicht omdat bepaalde programmeertalen (zoals Python en andere scripttalen) de code onmiddellijk uitvoeren bij het inladen.
    Om te vermijden dat de volgorde van de testgevallen zou verschillen tussen de programmeertalen, wordt het hoofdtestgeval altijd eerst uitgevoerd.
    \item[Test] De beoordeling van een testgeval bestaat uit meerdere \term{test}s, die elk een aspect van een dat testgeval controleren.
    Met aspect bedoelen we het standaarduitvoerkanaal, het standaardfoutkanaal, opgevangen uitzonderingen (\english{exceptions}), de teruggegeven waarden van een functieoproep (returnwaarde) of de inhoud van een bestand.
    De exitcode is ook mogelijk, maar enkel in het hoofdtestgeval.
\end{description}

Bij de keuze voor een formaat voor het testplan (json, xml, \ldots), hebben we vooraf enkele vereisten geformuleerd waaraan het gekozen formaat moet voldoen.
Het moet:

\begin{itemize}
    \item leesbaar zijn voor mensen,
    \item geschreven kunnen worden met minimale inspanning, met andere woorden de syntaxis dient eenvoudig te zijn, en
    \item programmeertaalonafhankelijk zijn.
\end{itemize}

Uiteindelijk is gekozen om het op te stellen in json.
Niet alleen voldoet json aan de vooropgestelde voorwaarden, het wordt ook door veel talen ondersteund.

Toch zijn er ook enkele nadelen aan het gebruik van json.
Zo is json geen beknopte of compacte taal om met de hand te schrijven.
Een oplossing hiervoor gebruikt de eigenschap dat veel talen json kunnen produceren: andere programma's kunnen desgewenst het testplan in het json-formaat genereren, waardoor het niet met de hand geschreven moet worden.
Hiervoor denken we aan een \termen{DSL} (\english{domain specific language}), maar dit valt buiten de thesis en wordt verder besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Een tweede nadeel is dat json geen programmeertaal is.
Terwijl dit de implementatie van de judge bij het interpreteren van het testplan weliswaar eenvoudiger maakt, is het tevens beperkend: beslissen of een testgeval moet uitgevoerd worden op basis van het resultaat van een vorig testgeval is bijvoorbeeld niet mogelijk.
Ook deze beperking wordt uitgebreider besproken in \cref{ch:beperkingen-en-toekomstig-werk}.

Tot slot bevat \cref{lst:testplan} een testplan met één context voor de voorbeeldoefening Lotto uit \cref{ch:dodona}.

\begin{listing}
    \inputminted{python}{code/testplan.json}
    \caption{
        Een ingekorte versie van het testplan voor de voorbeeldoefening Lotto.
        Het testplan bevat maar één context.
    }
    \label{lst:testplan}
\end{listing}

\subsection{Dataserialisatie}\label{subsec:dataserialisatie}

Bij de beschrijving van het testplan wordt gewag gemaakt van returnwaarden en variabeletoekenningen.
Aangezien het testplan programmeertaalonafhankelijk is, moet er dus een manier zijn om data uit de verschillende programmeertalen voor te stellen en te vertalen: het \term{serialisatieformaat}.

\subsubsection{Keuze van het formaat}

Zoals bij het testplan, werd voor de voorstelling van waarden ook een keuze voor een bepaald formaat gemaakt.
Daarvoor werden opnieuw enkele voorwaarden vooropgesteld, waaraan het serialisatieformaat moet voldoen.
Het formaat moet:

\begin{itemize}
    \item door mensen geschreven kunnen worden (\english{human writable}),
    \item onderdeel van het testplan kunnen zijn,
    \item in meerdere programmeertalen bruikbaar zijn, en
    \item de basisgegevenstypes ondersteunen die we willen aanbieden in het programmeertaalonafhankelijke deel van het testplan.
    Deze gegevenstypes zijn:
    \begin{itemize}
        \item Primitieven: gehele getallen, reële getallen, Boolese waarden en tekenreeksen.
        \item Collecties: rijen (eindige, geordende reeks; \texttt{list} of \texttt{array}), verzamelingen (eindige, ongeordende reeks zonder herhaling; \texttt{set}) en afbeeldingen (elk element wordt afgebeeld op een ander element; \texttt{map}, \texttt{dict} of \texttt{object}).
    \end{itemize}
\end{itemize}

Een voor de hand liggende oplossing is om ook hiervoor json te gebruiken, en zelf in json een structuur op te stellen voor de waarden.
In tegenstelling tot het testplan bestaan er al een resem aan dataserialisatieformaten, waardoor het de moeite loont om na te gaan of er geen bestaand formaat voldoet aan de vereisten.
Hiervoor is gestart van een overzicht op Wikipedia, \autocite{wiki2020}.
Uiteindelijk is niet gekozen voor een bestaand formaat, maar voor de json-oplossing.
De redenen hiervoor zijn samen te vatten als:

\begin{itemize}
    \item Het gaat om een binair formaat.
    Binaire formaten zijn uitgesloten op basis van de eerste twee voorwaarden die we opgesteld hebben: mensen kunnen het niet schrijven zonder hulp van bijkomende tools en het is moeilijk in te bedden in een json-bestand (zonder gebruik te maken van encoderingen zoals base64).
    Bovendien zijn binaire formaten moeilijker te implementeren in sommige talen.
    \item Het formaat ondersteunt niet alle gewenste types.
    Sommige formaten hebben ondersteuning voor complexere datatypes, maar niet voor alle complexere datatypes die wij nodig hebben.
    Uiteraard kunnen de eigen types samengesteld worden uit basistypes, maar dan biedt de ondersteuning voor de complexere types weinig voordeel, aangezien er toch een eigen dataschema voor die complexere types opgesteld zal moeten worden.
    \item Sommige formaten zijn omslachtig in gebruik.
    Vaak ondersteunen dit soort formaten meer dan wat wij nodig hebben.
    \item Het formaat is niet eenvoudig te implementeren in een programmeertaal waarvoor geen ondersteuning is.
    Sommige dezer formaten ondersteunen weliswaar veel talen, maar we willen niet dat het serialisatieformaat een beperkende factor wordt in welke talen door de judge ondersteund worden.
    Het mag niet de bedoeling zijn dat het implementeren van het serialisatieformaat het meeste tijd in beslag neemt.
\end{itemize}

Een lijst van de overwogen formaten met een korte beschrijving:

\begin{description}
    \item[Apache Avro] Een volledig "systeem voor dataserialisatie".
    De specificatie van het formaat gebeurt in json (vergelijkbaar met JSON Schema), terwijl de eigenlijke data binair geëncodeerd wordt.
    Heeft uitbreidbare types, met veel ingebouwde types \autocite{avro}.
    \item[Apache Parquet] Minder relevant, dit is een bestandsformaat voor Hadoop \autocite{parquet}.
    \item[ASN.1] Staat voor \english{Abstract Syntax Notation One}, een formaat uit de telecommunicatie.
    De hoofdstandaard beschrijft enkel de notatie voor een dataformaat.
    Andere standaarden beschrijven dan de serialisatie, bv.\ een binair formaat, json of xml.
    De meerdere serialisatievormen zijn in theorie aantrekkelijk: elke taal moet er slechts een ondersteunen, terwijl de judge ze allemaal kan ondersteunen.
    In de praktijk blijkt echter dat voor veel talen er slechts één serialisatieformaat is, en dat dit vaak het binaire formaat is \autocite{x680}.
    \item[Bencode] Schema gebruikt in BitTorrent.
    Het is gedeeltelijk binair, gedeeltelijk in text \autocite{cohen2017}.
    \item[Binn] Binair dataformaat \autocite{ramos2019}.
    \item[BSON] Een binaire variant op json, geschreven voor en door MongoDB \autocite{bson}.
    \item[CBOR] Een lichtjes op json gebaseerd formaat, ook binair.
    Heeft een goede standaard, ondersteunt redelijk wat talen \autocite{rfc7049}.
    \item[FlatBuffers] Lijkt op ProtocolBuffers, allebei geschreven door Google, maar verschilt wat in implementatie van ProtocolBuffers.
    De encodering is binair \autocite{flatbuffers}.
    \item[Fast Infoset] Is eigenlijk een manier om xml binair te encoderen (te beschouwen als een soort compressie voor xml), waardoor het minder geschikt voor ons gebruik wordt \autocite{x981}.
    \item[Ion] Een superset van json, ontwikkeld door Amazon.
    Het heeft zowel een tekstuele als binaire voorstelling.
    Naast de gebruikelijke json-types, bevat het enkele uitbreidingen. \autocite{ion}.
    \item[MessagePack] Nog een binair formaat dat lichtjes op json gebaseerd is.
    Lijkt qua types sterk op json.
    Heeft implementaties in veel talen \autocite{messagepack}.
    \item[OGDL] Afkorting voor \english{Ordered Graph Data Language}.
    Daar het om een serialisatieformaat voor grafen gaat, is het niet nuttig voor ons doel \autocite{ogdl}.
    \item[OPC Unified Architecture] Een protocol voor intermachinecommunicatie.
    Complex: de specificatie bevat 14 documenten, met ongeveer 1250 pagina's \autocite{tr62541}.
    \item[OpenDLL] Afkorting voor de \english{Open Data Description Language}.
    Een tekstueel formaat, bedoeld om arbitraire data voor te stellen.
    Wordt niet ondersteund in veel programmeertalen, in vergelijking met bv.\ json \autocite{openddl}.
    \item[ProtocolBuffers] Lijkt zoals vermeld sterk op FlatBuffers, maar heeft nog extra stappen nodig bij het encoderen en decoderen, wat het minder geschikt maakt \autocite{protobuf}.
    \item[Smile] Nog een binaire variant van json \autocite{smile}.
    \item[SOAP] Afkorting voor \english{Simple Object Access Protocol}.
    Niet bedoeld als formaat voor dataserialisatie, maar voor communicatie tussen systemen over een netwerk \autocite{soap}.
    \item[SDXF] Binair formaat voor data-uitwisseling.
    Weinig talen ondersteunen dit formaat \autocite{rfc3072}.
    \item[Thrift] Lijkt sterk op ProtocolBuffers, maar geschreven door Facebook \autocite{slee2007}.
    \item[UBJSON] Nog een binaire variant van json \autocite{ubjson}.

\end{description}

Geen enkel overwogen formaat heeft grote voordelen tegenover een eigen structuur in json.
Daarenboven hebben veel talen het nadeel dat ze geen json zijn, waardoor we een nieuwe taal moeten inbedden in het bestaande json-testplan.
Dit nadeel, gekoppeld met het ontbreken van voordelen, heeft geleid tot de keuze voor json.

\subsubsection{Dataschema}

Het formaat is, zoals al vermeld, json.
Het bijhorende dataschema is met opzet eenvoudig gehouden, om implementaties makkelijker te maken.
Concreet wordt een waarde voorgesteld als een json-object dat bestaat uit de (geëncodeerde) waarde en het type van die waarde.
Een voorbeeld is \cref{lst:serialisation}.

\begin{listing}
    \inputminted{json}{code/format.json}
    \caption{Een lijst bestaande uit twee getallen, geëncodeerd in het serialisatieformaat.}
    \label{lst:serialisation}
\end{listing}

\subsubsection{Datatypes}

Naast de encodering van de data is er een tweede aspect van het serialisatieformaat: de datatypes.
Het formaat ondersteunt de meeste basistypes die in bijna elke programmeertaal beschikbaar zijn.
Hieronder volgt lijst met een korte omschrijving van de ondersteunde types.
Hierbij is er een speciaal type, aangeduid met een ster (*), dat niet gebruikt wordt bij het encoderen van data.
Het type \texttt{literal} is bedoeld voor waarden die eigenlijk geen data zijn, maar verwijzingen naar bv.\ een variabele (een \english{identifier} in de programmeertaal).
Dit is nuttig bij functieargumenten (zo kunnen variabelen worden gebruikt bij een functieoproep).

\begin{description}
    \item[\texttt{integer}] Gehele getallen.
    \item[\texttt{rational}] Rationale getallen.
    \item[\texttt{text}] Een tekenreeks of string.
    \item[\texttt{literal*}] Een tekstuele waarde die rechtstreeks als \english{identifier} wordt gebruikt.
    Deze waarde wordt enkel gebruikt bij het aangeven van de types van functie-argumenten.
    \item[\texttt{unknown}] Dit type wordt gebruikt als er onbekende types zijn bij het encoderen van een waarde.
    Bij het omzetten van een waarde uit het serialisatieformaat naar een programmeertaal (deserialisatie) worden waarden van dit type genegeerd.
    \item[\texttt{boolean}] Een Boolese waarde (of boolean).
    \item[\texttt{list}] Een wiskundige rij, wat wil zeggen dat de volgorde belangrijk is en dat dubbele elementen toegelaten zijn.
    Merk op dat sommige talen meerdere implementaties hebben voor het concept van lijst.
    Het is de implementatie vrij om te kiezen welk concept gebruikt wordt.
    Zo wordt bijvoorbeeld in de Java-implementatie \texttt{List} in plaats van \texttt{array} gebruikt, om consistent te zijn met de implementatie van \texttt{set} en \texttt{object}.
    \item[\texttt{set}] Een wiskundige verzameling, wat wil zeggen dat de volgorde niet belangrijk is en dat dubbele elementen niet toegelaten zijn.
    \item[\texttt{object}] Een wiskundige afbeelding: elk element wordt afgebeeld op een ander element.
    In Java is dit bijvoorbeeld een \texttt{Map}, in Python een \texttt{dict} en in Javascript een \texttt{object}.
    \item[\texttt{nothing}] Geeft aan dat er geen waarde is, ook wel \texttt{null}, \texttt{None} of \texttt{nil} genoemd.
\end{description}

\subsection{Functieoproepen en assignments}\label{subsec:functieoproepen}

Een ander onderdeel van het testplan verdient ook speciale aandacht: het toekennen van variabelen (\english{assignments}) en de functieoproepen.

In heel wat oefeningen, en zeker in objectgerichte programmeertalen, is het toekennen van een waarde aan een variabele, om deze later te gebruiken, onmisbaar.
Bijvoorbeeld zou een opgave kunnen bestaan uit het implementeren van een klasse.
Bij de evaluatie dient dan een instantie van die klasse aangemaakt te worden, waarna er methoden kunnen aangeroepen worden, zoals hieronder geïllustreerd in een fictief voorbeeld.

\inputminted{java}{code/assignment.jshell}

Concreet is ervoor gekozen om het testplan niet uit te breiden met generieke statements of expressions, maar de ondersteuning te beperken tot assignments en functieoproepen.
Dit om de implementatie van de vertaling van het testplan naar de ondersteunde programmeertalen niet nodeloos ingewikkeld te maken.
Een functieoproep ziet er als volgt uit:

\inputminted{json}{code/function.json}

De lijst van argumenten is een lijst waarden, voorgesteld in het serialisatieformaat.
Het type van de functie geeft aan welke soort functie het is.
Mogelijke waarden zijn momenteel:
\begin{description}
    \item[\texttt{top}] Een \english{top-level} functie.
    Afhankelijk van de programmeertaal zal deze functie toch omgezet worden naar een functie op een object (bv.\ naar een statische functie in Java).
    \item[\texttt{object}] Een functie van een object.
    De invulling hiervan is gedeeltelijk programmeertaalafhankelijk, en meer specifiek van welke invulling gegeven wordt aan een object.
    Bij dit soort functies moet het \texttt{object} gegeven worden, wat het object is waaraan de functie vasthangt.
    In de praktijk kan dit mechanisme ook gebruikt worden voor functies in bijvoorbeeld een namespace of een statische functie in Java.
    \item[\texttt{constructor}] Deze soort heeft dezelfde semantiek als een top-level functie, met dien verstande dat het om een constructor gaat.
    In Java zal bijvoorbeeld het keyword \texttt{new} vanzelf toegevoegd worden.
    De functienaam doet dienst als naam van de klasse.
    \item[\texttt{identity}] Dit is een speciaal geval: er mag geen functienaam gegeven worden en er moet exact één argument gegeven worden.
    Dat ene argument zal de returnwaarde van de functie zijn.
    In de implementaties wordt dit vaak ook niet vertaalt als een functie, maar wordt de waarde rechtstreeks gebruikt.
    De bestaansreden van dit soort functies is het toekennen van waarden aan variabelen, om redenen die we later zullen bespreken.
\end{description}

Een beperking is dat het niet mogelijk is om rechtstreeks een functieoproep te doen als argument voor een andere functie,
of toch niet op een programmeertaalonafhankelijke manier.
Een oproep als \texttt{oproep(hallo(), 5)} is niet mogelijk.
Bij dergelijke dingen zal de functieoproep eerst aan variabele moeten toegekend worden, bv.\ \texttt{var param = hallo()}, waarna deze variabele als argument met type \texttt{literal} kan gegeven worden aan de oorspronkelijke functie: \texttt{oproep(param, 5)}.
De aandachtige lezer zal opmerken dat met die functieargumenten van het type \texttt{literal} rond deze beperking kan gewerkt worden, aangezien de tekstuele waarde van een dergelijk argument letterlijk in de taal komt.
We raden deze omweg echter ten sterkste af: dit maakt het testplan taalafhankelijk, want niet elke programmeertaal implementeert functieoproepen op eenzelfde wijze.

Dit brengt ons bij de variabeletoekenning of \english{assignment}.
In ons testplan beperkt dit zich tot het toekennen van een naam aan het resultaat van een functieoproep.
Dit is ook meteen de reden voor het bestaan van de functiesoort \texttt{identity}: via deze weg blijft het testplan eenvoudiger (de waarde van een assignment is altijd een functieoproep), maar toch kunnen gewoon waarden toegekend worden aan een variabelen.
Concreet ziet een variabeletoekenning er als volgt uit:

\inputminted{json}{code/assignment.json}

De \texttt{name} is de naam die aan de variabele gegeven zal worden.
Het veldje \texttt{expression} moet een object zijn dat een functieoproep voorstelt, in het formaat zoals hiervoor besproken.
In een beperkt aantal gevallen kan de judge het type van de variabele afleiden uit de functieoproep, maar in veel gevallen is het nodig om zelf het type mee te geven.
Dit type moet een van de ondersteunde types zijn uit het serialisatieformaat, zij het dat er ondersteuning is voor eigen types (zoals een klasse die geïmplementeerd moest worden door de student).

Een gecombineerd voorbeeld staat hieronder.
Hier wordt de string \texttt{'Dodona'} toegekend aan een variabele met naam \texttt{name}.
De judge kan het type afleiden, dus we moeten niet opgeven dat \texttt{name} een \texttt{str} is.

\inputminted{json}{code/assign-variable.json}

\subsection{Vereiste functies}\label{subsec:vereiste-functies}

We hebben in het overzicht al vermeld dat voor de evaluatie gecontroleerd wordt of een testplan de programmeertaal van gegeven oplossing ondersteunt.
Concreet gebeurt dit door voor elk item in het testplan af te leiden welke functies nodig zijn in een programmeertaal om dat item te ondersteunen.
Bevat een testplan bijvoorbeeld waarden met als type \texttt{set} (de verzameling), dan kunnen enkel programmeertalen die een verzameling ondersteunen gebruikt worden.
Dat zijn bijvoorbeeld Python en Java, maar geen Bash.
Het afleiden van de vereiste functies gebeurt volledig automatisch aan de hand van het testplan.

\section{Uitvoeren van de oplossing}\label{sec:uitvoeren-van-de-oplossing}

De eerste stap die wordt uitgevoerd bij de evaluatie van de oplossing is het genereren van de code, die de code van de student zal uitvoeren.

\subsection{Genereren van code}\label{subsec:genereren-van-code}

Het genereren van de code gebeurt met een sjabloonsysteem genaamd Mako \autocite{mako}.
Dit soort systemen wordt traditioneel gebruikt bij webapplicaties (zoals Ruby on Rails met \textsc{erb}, Phoenix met \textsc{eex}, Laravel met Blade, enz.) om html-pagina's te genereren.
In ons geval zijn de sjablonen verantwoordelijk voor de vertaling van de programmeertaalonafhankelijke concepten in het testplan naar eigenlijke code in de programmeertaal van de oplossing.
Hierbij denken we aan de functieoproepen, assignments, enz.
Ook zijn de sjablonen verantwoordelijk voor het genereren van de code die de oplossing van de student zal oproepen en evalueren.

\subsubsection{Sjablonen}

Het aantal sjablonen en hoe ze geïmplementeerd worden is in principe vrij, zij het dat de judge wel enkele standaardsjablonen nodig heeft, waaraan vastgelegde parameters meegegeven worden.
Deze verplichte sjablonen zijn:
\begin{description}
    \item[\texttt{assignment}] Vertaalt een assignment uit het testplan naar code.
    \item[\texttt{context}] Het sjabloon dat de code genereert om een context te evalueren.
    Deze code moet uitvoerbaar zijn (d.w.z.\ een main-functie bevatten of een script zijn).
    \item[\texttt{selector}] Het sjabloon de code genereert om een bepaalde context uit te voeren.
    Om performantieredenen (later hierover meer) wordt de code van alle contexten soms uit een keer gegenereerd en gecompileerd.
    Aan de hand van een parameter (de naam van de context), wordt bij het uitvoeren de code voor de juiste context gekozen.
    \item[\texttt{evaluator\_executor}] Genereert code om een aangepaste evaluator te starten.
    \item[\texttt{function}] Vertaalt een functie-oproep naar code.
    \item[\texttt{value}] Vertaalt een waarde uit het serialisatieformaat naar code.
\end{description}

Daarnaast moet het encoderen naar het serialisatieformaat ook geïmplementeerd worden in elke taal.
Veel talen hebben dus nog enkele bijkomende bestanden met code.
In alle bestaande implementaties is dit geïmplementeerd als een module of klasse met naam \texttt{Value}.

\subsubsection{Modus}\label{sss:modus}

De judge ondersteunt twee uitvoeringsmodi:
\begin{description}
    \item[Precompilatiemodus] In deze modus wordt de code voor alle contexten in een keer gecompileerd.
    Dit wordt gedaan om performantieredenen.
    In talen die resulteren in een uitvoerbaar bestand (zoals Haskell, C/C++), resulteert deze modus in één uitvoerbaar bestand voor alle contexten.
    Bij het uitvoeren wordt dan aan de hand van een parameter de juiste context uitgevoerd (met het \texttt{selector}-sjabloon van hierboven).
    \item[Individuele modus] Hierbij wordt elke context afzonderlijk gecompileerd.
\end{description}

Een flowchart van het generen van de code is \cref{fig:generation}, met een testplan met vier contexten.
In dit diagram zijn de stappen voor de individuele modus in het \textcolor{ugent-we}{aquablauw}.
De stappen van de precompilatiemodus zijn in het \textcolor{ugent-blue}{UGent-blauw}.
De gemeenschappelijke stappen zijn in het zwart.

Dit gedrag is configureerbaar in het testplan, maar standaard wordt de precompilatiemodus gebruikt, zij het met terugval op de individuele modus.
Deze terugval is handig voor talen met sterke compilatie.
Een voorbeeldscenario is als volgt: stel een oefening waarbij de student twee functies moet implementeren.
De student implementeert de eerste functie en dient in om al feedback te krijgen.
Bij talen als Java of Haskell zal dit niet lukken: daar alle contexten in een keer gecompileerd worden, zal de ontbrekende tweede functie ervoor zorgen dat de volledige compilatie faalt.
In individuele modus is dit geen probleem: de contexten die de eerste functie testen zullen compileren en kunnen uitgevoerd worden.
De individuele modus brengt wel een niet te onderschatten kost qua uitvoeringstijd met zich mee (zie ook \cref{ch:beperkingen-en-toekomstig-werk}).

\begin{figure}
    \begin{adjustbox}{width=\textwidth}
        \input{generated/generation.tikz}
    \end{adjustbox}
    \caption{
        Schematische voorstelling van het genereren van de code.
        Gemeenschappelijke stappen zijn zwart, stappen voor de individuele modus \textcolor{ugent-we}{aquablauw} en stappen voor de precompilatiemodus \textcolor{ugent-blue}{UGent-blauw}.
    }
    \label{fig:generation}
\end{figure}

\subsection{Uitvoeren van de code}\label{subsec:uitvoeren-van-de-code}

Na het genereren wordt alle code gecompileerd (bij de talen waar dit mogelijk is).
Dit gebeurt ofwel eenmaal voor alle contexten afzonderlijk, ofwel eenmaal voor alle contexten samen, afhankelijk van de modus.
De werking hiervan wordt behandeld in \cref{sss:modus} en \cref{fig:generation}.

Vervolgens wordt elke context uit het testplan uitgevoerd en wordt de uitvoer verzameld.
Het uitvoeren zelf gebeurt op de normale manier dat een programmeertaal uitgevoerd wordt: via de commandoregel.
Deze aanpak heeft een voordeel: er is geen verschil tussen hoe de judge de code van de student uitvoert en hoe de student zijn code zelf uitvoert op zijn eigen computer.
Dit voorkomt dat er subtiele verschillen in de resultaten sluipen.

Indien de configuratie het toelaat, worden de contexten in parallel uitgevoerd.
Om te vermijden dat bestanden of uitvoer overschreven wordt, wordt de gecompileerde code gekopieerd naar een aparte map, waar de uitvoer gebeurt.
\Cref{lst:mapstructuur} illustreert dit met een voorbeeld voor een oplossing in Java.
Deze mapstructuur stelt de toestand van de werkmap voor na het uitvoeren van de code.
In de map \texttt{common} zit alle code en de gecompileerde bestanden.
Voor elke context worden de gecompileerde bestanden gekopieerd naar een andere map, bv.\ \texttt{context-1}, wat de map is voor context \texttt{1} van het testplan.

\begin{listing}
    \inputminted{text}{code/dir-listing.txt}
    \caption{Mapstructuur na het uitvoeren van de evaluatie van een oplossing in Python.
    Context 0-0 staat voor de eerste context van het eerste tabblad.}
    \label{lst:mapstructuur}
\end{listing}

\subsection{Verzamelen van resultaten}\label{subsec:verzamelen-van-resultaten}

De uitvoering van een oplossing genereert resultaten die door de judge geïnterpreteerd moeten worden.
Er zijn verschillende soorten uitvoerresultaten (zoals vermeld heeft elke soort uitvoer een aparte test in het testplan).
We noemen de verschillende soorten uitvoer de \term{uitvoerkanalen}.
Twee ervan, het standaarduitvoer- en standaardfoutkanaal komen overeen met de standaarduitvoer- en standaardfoutstroom van het proces dat de code uitvoert.
Uitvoer naar een bestand (het bestandskanaal) resulteert in een bestand en vormt ook geen probleem.
De overige uitvoerkanalen, het kanaal voor exceptions (uitzonderingenkanaal) en het returnkanaal (voor returnwaarden) worden geschreven naar een bestand.
Het is namelijk niet in elke taal mogelijk om nieuwe kanalen te openen.
De sjablonen krijgen de verwachte namen van die bestanden mee van de judge, maar zijn wel verantwoordelijk voor het openen, schrijven en sluiten van deze bestanden.
Deze naam bevat willekeurige tekens, zodat de kans dat deze bestanden overschreven worden door de oplossing minimaal is.
De bestanden waar de exceptions and returnwaarden naartoe geschreven worden, worden na de uitvoering gelezen door de judge.
Hierna worden alle kanalen op dezelfde manier behandeld door de judge.

In de bestaande implementaties ligt de verantwoordelijkheid om naar deze bestanden te schrijven bij de \texttt{Value}-module van hierboven.

\section{Evalueren van een oplossing}\label{sec:evalueren-van-een-oplossing2}

Na de uitvoering van elke context heeft de judge alle relevant uitvoer verzamelt, zoals de standaardkanalen.
Deze uitvoer moet vervolgens beoordeeld worden om na te gaan in hoeverre deze uitvoer voldoet aan de verwachte uitvoer.
Dit kan op drie manieren:
\begin{enumerate}
    \item Ingebouwde evaluator: de oplossing wordt geëvalueerd in de judge zelf.
    \item Aangepaste evaluator: de oplossing wordt geëvalueerd door eigen code, maar dezelfde wordt gebruikt voor alle programmeertalen, in het evaluatieproces.
    \item Taalspecifieke evaluator: de oplossing wordt onmiddellijk na de uitvoering geëvalueerd in het uitvoeringsproces.
\end{enumerate}

\subsection{Ingebouwde evaluator}\label{subsec:ingebouwde-evaluator}

Voor eenvoudige evaluaties volstaat de ingebouwde evaluator van de judge.
Momenteel zijn er drie soorten ingebouwde evaluatoren, die hieronder besproken worden.

\subsubsection{Tekstevaluator}

Deze evaluator vergelijkt de verkregen uitvoer van een uitvoerkanaal (standaarduitvoer, returnwaarde, \ldots) met de verwachte uitvoer uit het testplan.
Alle data worden als string behandeld.
Deze evaluator biedt enkele opties om het gedrag aan te passen:

\begin{description}
    \item[\texttt{ignoreWhitespace}] Witruimte voor en na het resultaat wordt genegeerd.
    \item[\texttt{caseInsensitive}] Er wordt geen rekening gehouden met het verschil tussen hoofdletters en kleine letters.
    \item[\texttt{tryFloatingPoint}] De waarde moet geïnterpreteerd worden als een zwevendekommagetal (\english{floating point}), waarbij rekening gehouden wordt met de foutmarge.
    \item[\texttt{applyRounding}] Of zwevendekommagetallen afgrond moeten worden.
    Indien wel wordt het aantal cijfers genomen van de optie \texttt{roundTo}.
    \item[\texttt{roundTo}] Het aantal cijfers na de komma.
    Enkel nuttig als \texttt{applyRounding} waar is.
\end{description}

\subsubsection{Bestandsevaluator}

Hiermee kan een geproduceerd bestand vergeleken worden met een gegeven bestand uit het testplan.
Het gaat om tekstuele bestanden.
Deze evaluator kan werken in drie modi:

\begin{description}
    \item[\texttt{exact}] Beide bestanden moet exact hetzelfde zijn, inclusief regeleindes.
    \item[\texttt{lines}] Elke regel wordt vergeleken met overeenkomstige regel in het andere bestand.
    De evaluatie van de lijnen is exact, maar zonder de regeleindes.
    \item[\texttt{values}] Elke regel wordt geïnterpreteerd als een tekstuele waarde en vergeleken met de tekstevaluator.
    In deze modus worden kunnen ook alle opties van de tekstevaluator gebruikt worden.
\end{description}

\subsubsection{Waarde-evaluator}

Deze evaluator vergelijkt twee waarden, zoals gedefinieerd door het serialisatieformaat.
De twee waarden moeten exact overeenkomen, met uitzondering van zwevendekommagetallen.


\subsection{Aangepaste evaluator}\label{subsec:aangepaste-evaluator}

Voor de aangepaste evaluator moet een bestand geschreven worden in een programmeertaal naar keuze.
Het resultaat van de uitvoering wordt vervolgens geserialiseerd en gedeserialiseerd naar het evaluatieproces.
Hoe een evaluator moet geïmplementeerd worden, hangt af van de programmeertaal.

In Python bestaat de aangepaste evaluator uit een module met een functie die voldoet aan de definitie, zoals gegeven in \cref{lst:evaluation-python-custom}.
De judge stelt ook een module \texttt{evaluation\_utils} ter beschikking.
De functie van hierboven moet dan één oproep doen naar de functie \texttt{evaluated()}.
Deze module is redelijk eenvoudig, zoals te zien in \cref{lst:evaluation-util-python}.

\begin{listing}
    \inputminted{python}{code/custom_signature.py}
    \caption{De definitie van de aangepaste evaluator.}
    \label{lst:evaluation-python-custom}
\end{listing}

\begin{listing}
    \inputminted{python}{../../judge/runners/templates/python/evaluation_utils.py}
    \caption{De implementatie van de module \texttt{evaluation\_utils}}
    \label{lst:evaluation-util-python}
\end{listing}

In de Java-implementatie is de situatie gelijkaardig: het gaat om het implementeren van een abstracte klasse, die ook dienst doet als de module van Python.
Deze klassen en haar ouder staan in \cref{lst:evaluation-util-java,lst:evaluation-java-custom}.

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractEvaluator}.}
    \label{lst:evaluation-util-java}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractCustomEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractCustomEvaluator}.}
    \label{lst:evaluation-java-custom}
\end{listing}

\subsubsection{Taalspecifieke evaluator}

De taalspecifieke evaluator lijkt sterk op de aangepaste evaluator.
is de eenvoudigste: deze neemt een codefragment met daarin één functie \texttt{evaluate}, die één argument aanvaardt, de geproduceerde waarde.
Waar de geproduceerde waarde bij de aangepaste evaluator in het serialisatieformaat moet kunnen, is dit hier niet het geval: de functie wordt rechtstreeks opgeroepen tijdens de uitvoering.
In Python wordt dit \cref{lst:evaluation-python-specific}, in Java \cref{lst:evaluation-java-specific}.
Om het resultaat van de evaluatie aan de judge te geven, wordt dezelfde \texttt{evaluated}-functie als bij de aangepaste evaluator gebruikt (respectievelijk \cref{lst:evaluation-util-python,lst:evaluation-util-java})

\begin{listing}
    \inputminted{python}{code/specific_signature.py}
    \caption{De definitie van de taalspecifieke evaluator.}
    \label{lst:evaluation-python-specific}
\end{listing}

\begin{listing}
    \inputminted{java}{../../judge/runners/templates/java/AbstractSpecificEvaluator.java}
    \caption{De implementatie van de klasse \texttt{AbstractSpecificEvaluator}.}
    \label{lst:evaluation-java-specific}
\end{listing}
