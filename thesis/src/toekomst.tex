\chapter{Beperkingen en toekomstig werk}\label{ch:beperkingen-en-toekomstig-werk}

\lettrine{D}{e doelstelling} van deze thesis is het formuleren van een antwoord op de vraag of programmeertaalonafhankelijke oefeningen mogelijk zijn (zie \cref{sec:probleemstelling}).
Als antwoord hierop is een prototype, TESTed, ontwikkelt.
Op basis van het prototype blijkt dat het antwoord op de onderzoeksvraag positief is.
Eigen aan een prototype is dat het nog niet \english{production ready} is.
Dit hoofdstuk handelt over ontbrekende functionaliteit of beperkingen van TESTed, en reikt mogelijke ideeën voor een oplossing aan.

\section{Dodona-platform}\label{sec:dodona-platform}

Geen enkele bestaande judge in het Dodona-platform ondersteunt meerdere programmeertalen.
Dit heeft gevolgen voor hoe Dodona omgaat met programmeertalen, zoals hieronder duidelijk wordt.

\subsection{Programmeertaalkeuze in het Dodona-platform}\label{subsec:programmeertaalkeuze-in-het-dodona-platform}

Een eerste probleem is dat binnen Dodona de programmeertaal gekoppeld is aan een oefening, niet aan een oplossing.
Dit impliceert dat elke oplossing van een oefening in dezelfde programmeertaal geschreven is.
Een aanname die niet meer geldt bij TESTed.
Een uitbreiding van Dodona bestaat er dus in om ondersteuning toe te voegen voor het kiezen van een programmeertaal per oplossing.
Dit moet ook in de gebruikersinterface mogelijk zijn, zodat studenten kunnen kiezen in welke programmeertaal ze een oefening oplossen.

Als \english{workaround} zijn er twee suboptimale opties:
\begin{itemize}
    \item Dezelfde oefening in meerdere programmeertalen aanbieden, door bijvoorbeeld met symlinks dezelfde opgave, testplan en andere bestanden te gebruiken, maar met een ander config-bestand.
    Op deze manier zullen er in Dodona meerdere oefeningen zijn.
    Deze aanpak heeft nadelen.
    Zo zijn de oefeningen effectief andere oefeningen vanuit het perspectief van Dodona.
    Dient een student bijvoorbeeld een oplossing in voor één van de oefeningen, worden de overige oefeningen niet automatisch als ingediend gemarkeerd.
    Ook is de manier waarop de opgave, het testplan, enzovoort gedeeld worden (de symlinks) niet echt handig.
    \item TESTed zelf de programmeertaal laten afleiden op basis van de ingediende oplossing.
    Hiervoor zijn verschillende implementaties mogelijk, zoals een heuristiek die op basis van de ingediende broncode de programmeertaal voorspelt of de student die programmeertaal expliciet laten aanduiden.
    Er is voorlopig gekozen om de tweede manier te implementeren: TESTed ondersteunt een speciale \term{shebang}\footnote{Zie \url{https://en.wikipedia.org/wiki/Shebang_(Unix)}.} die aangeeft in welke programmeertaal de ingediende oplossing beoordeeld moet worden.
    De eerste regel van de ingediende oplossing kan er als volgt uitzien:
    \begin{minted}{bash}
#!tested [programmeertaal]
    \end{minted}
    Hierbij wordt \texttt{[programmeertaal]} vervangen door de naam van de programmeertaal, zoals \texttt{java} of \texttt{python}.
    Dit is een voorlopige oplossing die toelaat om oefeningen in meerdere programmeertalen aan te bieden zonder dat Dodona moet aangepast worden.
    Een nadeel aan deze oplossing is dat de student niet kan zien in welke programmeertalen ingediend kan worden;
    dit moet door de lesgever aangeduid worden.
\end{itemize}

\subsection{Detectie ondersteunde programmeertalen}\label{subsec:detectie-beschikbare-programmeertalen}

Een ander gevolg van de koppeling van de programmeertaal aan een oefening binnen Dodona is dat Dodona momenteel altijd weet voor welke programmeertaal een oplossing werd ingediend: de programmeertaal van de oefening.
Het is echter niet voldoende dat de programmeertaal van de oplossing gekend is bij het indienen,
het is ook nuttig te weten in welke programmeertalen een oefening gemaakt kan worden.
Dit om enerzijds dingen als syntaxiskleuring te kunnen toepassen, maar ook om ervoor te zorgen dat een student geen nutteloos werk verricht en een oplossing maakt in een niet-ondersteunde programmeertaal.

Zoals besproken in \cref{subsec:vereiste-functies}, controleert TESTed of de programmeertaal van de ingediende oplossing ondersteund wordt door het testplan van de oefening.
Die controle komt echter te laat, namelijk bij het beoordelen van een oplossing.
Aan de andere kant is de controle onafhankelijk van de oplossing: enkel het testplan is nodig.
Een beter ogenblik om de ondersteunde programmeertalen van een oefening te bepalen is bij het importeren van de oefening in het Dodona-platform.
Bij het importeren gebeurt al een verwerking van de oefening door Dodona, om bijvoorbeeld de configuratiebestanden te lezen en de oefening in de databank op te slaan.
Deze verwerking zou kunnen uitgebreid worden om ook de ondersteunde programmeertalen te bepalen.
Merk op dat deze controle opnieuw moet uitgevoerd worden elke keer dat een nieuwe programmeertaal aan TESTed toegevoegd wordt.

De volgende vraag die zich stelt is hoe dit praktisch moet aangepakt worden.
Het bepalen van de ondersteunde programmeertalen is iets specifieks voor TESTed, terwijl judge-specifieke code zoveel mogelijk buiten Dodona wordt gehouden.
Een mogelijke oplossing is de verwerking van de oefeningen uit te breiden, zodat de judge ook een verwerking kan doen van de oefening.
Deze aanpak heeft als voordeel dat het een generieke oplossing is: alle judges krijgen de mogelijkheid tot het verwerken van de oefening.

Een andere variant van deze oplossing wordt overwogen door het Dodona-team\footnote{Zie \url{https://github.com/dodona-edu/dodona/issues/1754}}.
Het voorstel is om oefeningen en judges te combineren in Dodona tot één Docker-container.
Interessant binnen TESTed is het toevoegen van een \texttt{prepare}-stap aan de judge (momenteel heeft een judge slechts één stap, namelijk \texttt{run}, het beoordelen).
Deze voorbereidende stap resulteert dan in een Docker-container op maat van de oefening.
Het is in deze voorbereidende stap dat het detecteren van de ondersteunde programmeertalen zou plaatsvinden.

Een andere oplossing voor dit probleem is de ondersteunde programmeertalen manueel opgeven in de configuratiebestanden van de oefening.
Hier is dan weer het voordeel dat de verwerkingsstap van Dodona niet uitgebreider en trager moet worden.
Aan de andere kant mag de auteur van de oefening dan niet vergeten om de programmeertalen in de configuratie op te nemen.
Eventueel kan dit gemakkelijker gemaakt worden door een script te voorzien dat de ondersteunde programmeertalen detecteert en het resultaat in de configuratiebestanden schrijft.
In de twee eerdere voorstellen wordt telkens gesproken over een verwerkingsstap, waarin het detecteren plaatsvindt.
Bij deze laatste oplossing verschuift de verantwoordelijkheid voor het uitvoeren van deze stap van Dodona naar de auteur van de oefening.

\subsection{Beperken ondersteunde programmeertalen}\label{subsec:beperken-beschikbare-programmeertalen}

De vorige paragraaf heeft het steeds over het automatisch detecteren van de ondersteunde programmeertalen.
Om twee redenen kan het echter nuttig zijn om deze automatisch detectie te overschrijven.

Ten eerste is de automatische detectie niet altijd voldoende streng, wat ertoe leidt dat TESTed een programmeertaal als ondersteund beschouwd, terwijl de oefening niet oplosbaar is in die programmeertaal.
Een concreet voorbeeld hiervan is de detectie van functieargumenten met heterogene gegevenstypes (zie \cref{subsec:vereiste-functies} voor een voorbeeld).
De automatische detectie houdt enkel rekening met \english{literal} functieargumenten.
De gegevenstypes van functieoproepen en identifiers worden niet gecontroleerd, aangezien deze informatie niet eenvoudig af te leiden is uit het testplan.
In dit eerste geval is het dus nuttig dat de auteur van de oefening op het niveau van een oefening een \term{blacklist} of zwarte lijst van programmeertalen kan meegeven: programmeertalen waarin de oefening niet gemaakt kan worden.
Een belangrijk detail is dat als er nieuwe programmeertalen bijkomen in TESTed, de oefening standaard wel ondersteund zou worden in die nieuwe programmeertaal.

De andere grote reden is dat lesgevers de programmeertalen waarin een oefening opgelost kan worden willen beperken.
Dit kan bijvoorbeeld zijn omdat de lesgever slechts programmeertalen wil toelaten die hij machtig is, of dat de oefening gebruikt wordt in een vak waar één bepaalde programmeertaal onderwezen wordt.
Binnen Dodona stelt een lesgever binnen een vak reeksen oefeningen op.
Die reeksen kunnen bestaan uit eigen, zelfgeschreven oefeningen, maar kunnen ook bestaan uit oefeningen die beschikbaar zijn in het Dodona-platform.
De auteur van de oefening en lesgever zijn dus niet altijd dezelfde persoon.
Om die reden is het nuttig als de lesgevers op het niveau van het vak, de reeks of de oefening een \term{whitelist} of witte lijst van programmeertalen kunnen opgeven: oefeningen in dat vak of die reeksen zullen enkel in de toegelaten programmeertalen kunnen opgelost worden.
Bij deze beperking worden nieuwe programmeertalen niet automatisch ondersteund.
Het zou ook nuttig zijn ook hier met een zwarte lijst te kunnen werken, waarbij nieuwe talen wel automatisch ondersteund worden.

\section{Testplan}\label{sec:beperkingen-testplan}

De gekozen aanpak voor het testplan heeft bepaalde beperkingen (zie \cref{subsec:het-testplan} voor de gemaakte keuze).
Deze paragraaf bespreekt drie grote beperkingen en mogelijke oplossingen.

\subsection{Handgeschreven testplannen zijn omslachtig}\label{subsec:handgeschreven-testplannen-zijn-omslachtig}

In TESTed is gekozen om het testplan in \acronym{JSON} op te stellen (zie \cref{subsec:het-testplan}).
Bovendien is een designkeuze geweest om zoveel mogelijk informatie expliciet te maken.
Dit heeft als voornaamste voordeel dat de implementatie in TESTed eenvoudiger blijft.
Anderzijds zorgt dit wel voor redelijk wat herhaalde informatie in het testplan.

De combinatie van \acronym{JSON} met de herhaalde informatie zorgt ervoor dat een testplan vaak lang is.
Te lang om nog met de hand te schrijven.
Een oplossing hiervoor, die al gebruikt wordt, is om het testplan niet met de hand te schrijven, maar te laten generen door bijvoorbeeld een Python-script.

Een andere oplossing is het toevoegen van een bijkomende stap voor het testplan: het testplan wordt dan in een ander formaat geschreven, en bijvoorbeeld tijdens het importeren in Dodona omgezet naar het \acronym{JSON}-bestand.
Voor dat andere formaat kan een \acronym{DSL} (\english{domain-specific language}) nuttig zijn.
Dit is een programmeertaal die specifiek ontworpen is voor een bepaald onderwerp of doel.
Hier zou ze specifiek ontworpen zijn om een testplan te schrijven.

Een bijkomend voordeel van \acronym{JSON} als formaat is dat het een neutraal formaat is.
Verschillende soorten oefeningen hebben andere noden op het vlak van schrijven van het testplan.
Het testplan voor een oefening die enkel stdin en stdout gebruikt zal er helemaal anders uitzien dan een testplan voor een oefening waar functieoproepen gebruikt worden.
Het zou bijvoorbeeld kunnen dat voor verschillende soorten oefeningen een andere \acronym{DSL} ontworpen wordt.

Zo zijn er bijvoorbeeld meerdere standaarden voor het schrijven van programmeeroefeningen.
Een programma dat oefening die in deze formaten beschreven zijn omzet naar het eigen formaat van TESTed kan een nuttige uitbreiding zijn.
Op het moment van schrijven lijken er echter nog geen wijdverspreide standaarden te zijn.
Bij wijze van voorbeeld worden twee standaarden besproken, telkens met een beschrijving van dezelfde oefening, de \emph{Palindroom}-oefening.
Bij deze voorbeeldoefening moet invoer gelezen worden en dan als uitvoer moet aangegeven worden of de invoer al dan niet een palindroom is.

Een eerste standaard is \acronym{PExIL} (\term{Programming Exercises Interoperability Language}) \autocite{queiros2011pexil}.
Dit is een op \acronym{XML}-gebaseerd formaat, gepubliceerd in 2011.
Ontwikkeling van deze standaard lijkt gestopt te zijn.
We hebben geen online oefeningen gevonden die in dit formaat beschreven zijn en de de nodige resources voor het zelf schrijven van een oefeningen (zoals het \acronym{XML} Schema) zijn niet meer beschikbaar online.
De beschrijving van de \emph{Palindroom}-oefening is \cref{lst:pexil}.

\begin{listing}
    \inputminted{xml}{code/pexil-in.xml}
    \inputminted{xml}{code/pexil-out.xml}
    \caption{Beschrijving van de oefening "palindroom" (gesplitst in invoer en uitvoer) in \acronym{PExIL}.}
    \label{lst:pexil}
\end{listing}

Een tweede standaard is \acronym{PEML} (\english{Programming Exercise Markup Language}) \autocite{peml}.
P\acronym{EML} is een tekstueel formaat (een soort \acronym{DSL}) dat er veelbelovend uitziet om met de hand te schrijven.
Op het moment van schrijven van deze thesis is \acronym{PEML} nog in volle ontwikkeling.
Aangezien een van de \english{design goals} van \acronym{PEML} is dat de omzetting van \acronym{PEML} naar \acronym{JSON} eenvoudig zal zijn, lijkt dit een interessant formaat.
De beschrijving van de \emph{Palindroom}-oefening staat in \cref{lst:peml}.
Dit voorbeeld is overgenomen uit de documentatie van \acronym{PEML}.

\begin{listing}
    \inputminted{yaml}{code/peml.peml}
    \caption{
        Beschrijving van de oefening "palindroom" in \acronym{PEML}.
        Overgenomen uit \autocite{peml}.
    }
    \label{lst:peml}
\end{listing}


\subsection{Dynamische scheduling van testen}\label{subsec:dynamische-scheduling-van-testen}

In TESTed is gekozen om het testplan in \acronym{JSON} op te stellen (zie \cref{subsec:het-testplan}).
Een nadeel van deze aanpak is dat de \term{scheduling} (het plannen van welke testcode wanneer uitgevoerd worden) statisch gebeurt.
Een analogie is bijvoorbeeld een gecompileerde taal zoals C/C++ en een dynamische taal zoals Python: TESTed vervult in deze analogie de rol van compiler.
Door dit statische testplan is het niet mogelijk om in het algemeen op basis van het resultaat van een vorig testgeval het verloop van de volgende testgevallen te bepalen.

Het statisch testplan \emph{an sich} is echter voor specifieke gevallen geen probleem.
Het testplan kan uitgebreid kunnen worden met verschillende mogelijkheden.
Zo is reeds voorzien in het testplan om een testgeval als essentieel aan te duiden: faalt het testgeval, dan zullen volgende testgevallen niet meer uitgevoerd worden.
Dit zou uitgebreid kunnen worden zodat het ook op het niveau van contexten werkt.

Dit systeem kan verder uitgebreid worden door aan elke context een unieke identificatiecode toe te kennen.
Daarna kan voor elke context een preconditie gegeven worden: welke contexten moet geslaagd zijn om de huidige context uit te voeren.
Zijn niet alle contexten uit de preconditie geslaagd, dan wordt de context niet uitgevoerd.
Men kan bijvoorbeeld aangeven dat context 15 enkel uitgevoerd kan worden indien contexten 1, 7 en 14 ook succesvol waren.

\subsection{Herhaalde uitvoeringen van testgevallen}\label{subsec:herhaalde-uitvoeringen-van-testgevallen}

Een andere, maar aan het dynamisch uitvoeren verwante, beperking is het ontbreken van de mogelijkheid om contexten of testgevallen meerdere keren uit te voeren.
Dit herhaald uitvoeren is nuttig bij niet-deterministische oefeningen.
Een oefening waarbij waarden bijvoorbeeld uit een normale verdeling getrokken moeten worden, zal een duizendtal keren uitgevoerd moeten worden vooraleer met voldoende zekerheid kan geconstateerd worden of er effectief een normale verdeling gebruikt wordt of niet.

Dit is ook op te lossen met een uitbreiding van het testplan: per testgeval of per context zou kunnen aangeduid worden hoeveel keer deze moet uitgevoerd worden.
Om de performantie te verbeteren, zouden ook stopvoorwaarden kunnen meegegeven worden.
Een aspect dat dit complexer maakt is dat de evaluatoren (zowel de ingebouwde evaluatie binnen TESTed als de geprogrammeerde en programmeertaalspecifieke evaluatie) aangepast zullen moeten worden om niet enkel met één resultaat, maar met een reeks resultaten te werken.

\section{TESTed}\label{sec:beperkingen-tested}

In deze paragraaf komen de beperkingen aan bod die ofwel enkel betrekking hebben op TESTed zelf, ofwel betrekking hebben op meerdere onderdelen, bijvoorbeeld zowel TESTed als het testplan.

\subsection{Geprogrammeerde evaluatie is traag}\label{subsec:geprogrammeerde-evaluatie-is-traag}

Het uitvoeren van een geprogrammeerde evaluatie (zie \cref{subsec:geprogrammeerde-evaluatie}) zorgt voor een niet te verwaarlozen kost op het vlak van performantie (zie \cref{tab:meting} voor enkele tijdsmetingen).
De reden hiervoor is eenvoudig te verklaren: zoals de contexten wordt elke geprogrammeerde evaluatie in een afzonderlijk subproces uitgevoerd (afhankelijk van de programmeertaal ook met eigen compilatiestap).
Zoals vermeld in \cref{sec:performantie} is TESTed oorspronkelijk gestart met het uitvoeren van code in Jupyter-kernels.
De grootste reden dat we daar vanaf gestapt zijn, is dat de kost voor het opnieuw opstarten van een kernel te groot is, en het heropstarten noodzakelijk is om de onafhankelijkheid van de contexten te garanderen.
Bij $n$ contexten moet de kernel dus $n$ keer opnieuw gestart worden.

Bij een geprogrammeerde evaluatie wordt geen code van de student uitgevoerd: het opnieuw starten van de kernel is dus niet nodig.
Hierdoor wordt de opstartkost van de kernel verspreid over alle contexten die van die programmeerde evaluatie gebruikmaken.
Wordt er $n$ keer andere evaluatiecode in een andere programmeertaal gebruikt, zal dit uiteraard geen voordeel opleveren, omdat er dan nog steeds $n$ keer een kernel gestart wordt.
In de meeste gevallen wordt echter dezelfde evaluatiecode gebruikt voor alle testgevallen.
Het \english{worst case scenario} is dan weliswaar trager, maar in de meeste gevallen zal de geprogrammeerde evaluatie sneller zijn.

Hierop is één uitzondering: een geprogrammeerde evaluatie waarbij de programmeertaal van de evaluatiecode in Python geschreven is.
Voor Python is er speciale ondersteuning (vermits TESTed zelf ook in Python geschreven is, wordt deze evaluatie ook rechtsreeks in TESTed gedaan zonder subproces): hiervoor is het gebruik van Jupyter-kernels niet nuttig.

\subsection{Ondersteuning voor natuurlijke talen}\label{subsec:ondersteuning-voor-natuurlijke-talen}

Sommige judges voor specifieke programmeertalen in Dodona hebben ondersteuning voor het vertalen op het gebied van natuurlijke talen, zoals de vertaling van bijvoorbeeld namen van functies of variabelen.
De vertaling van de beschrijving van de opgave is dan weer geïmplementeerd in Dodona zelf door een tweede bestandsextensie.
Zo zullen \texttt{description.nl.md} en \texttt{description.en.md} gebruikt worden voor respectievelijk Nederlands en Engels.

Een eenvoudige oplossing voor het vertalen in de code is iets gelijkaardigs doen met het testplan: \texttt{testplan.nl.json} en \texttt{testplan.en.json} voor respectievelijk Nederlands en Engels.
Deze aanpak heeft wel als nadeel dat veel dingen in het testplan onnodig zullen gedupliceerd moeten worden;
een waarde \texttt{5} is niet taalafhankelijk.

Een idee dat dit zou voorkomen is om binnen eenzelfde testplan ondersteuning te bieden voor vertalingen, bijvoorbeeld door telkens meerdere talen te aanvaarden.
Momenteel ziet een functienaam er bijvoorbeeld als volgt uit:

\inputminted{json}{code/example-name.json}

Ondersteuning voor vertaling kan dan deze vorm aannemen:

\inputminted{json}{code/example-name-trans.json}

Als een functie binnen een testplan veel wordt opgeroepen, zorgt dit nog steeds voor veel herhaling.
Dit kan opgelost worden door een \acronym{DSL}, maar dat neemt niet weg dat het testplan onnodig lang zal worden.
Een mogelijke oplossing hiervoor is om een soort vertaalwoordenboek mee te geven in het testplan, waar met sleutelwoorden gewerkt wordt.
Een voorbeeld illustreert dit:

\inputminted{json}{code/example-name-dict.json}

De functieoproep gebruikt dan een sleutelwoord uit het vertaalwoordenboek, waarna TESTed bij het uitvoeren de juiste naam gebruikt:

\inputminted{json}{code/example-name-key.json}

Ook de foutboodschappen die TESTed zelf genereert zijn bijvoorbeeld nog niet vertaald: de boodschappen met studenten als doelpubliek zijn in het Nederlands, terwijl de boodschappen voor cursusbeheerders in het Engels zijn.
Dit is echter eenvoudig op te lossen: TESTed krijgt de natuurlijke taal mee van Dodona in de configuratie, en Python heeft meerdere internationalisatie-API's\footnote{Zie \url{https://docs.python.org/3/library/gettext.html}}.

\subsection{Programmeertaalonafhankelijke opgaven}\label{subsec:programmeertaalonafhankelijke-opgaven}

In veel oefeningen bevat de opgave een stuk code ter illustratie van de opgave.
Dit is expliciet buiten het bestek van deze thesis gehouden, maar deze voorbeelden zijn idealiter in de programmeertaal waarin de student wenst in te dienen.

Een idee is hier de codevoorbeelden uit de opgave ook in het formaat van het testplan op te stellen.
TESTed bevat namelijk alles om het testplan om te zetten naar concrete code.

Concreet gaat het om volgende zaken:
\begin{enumerate}
    \item De programmeertaalkeuze in het Dodona-platform moet uitgebreid worden zodat de opgave ook programmeertaalafhankelijk is en dus kan gewijzigd worden als de student een andere programmeertaal kiest.
    \item De uitleg bij een opgave (bijvoorbeeld omkadering of bijkomende informatie) is vaak programmeertaalonafhankelijk, terwijl de meer technische onderdelen programmeertaalafhankelijk zijn.
    Voorbeelden van dat laatste zijn datastructuren (\texttt{list}/\texttt{array}) of codevoorbeelden.
    Hiervoor zijn ook meerdere mogelijkheden: zo zou alles binnen eenzelfde bestand kunnen, of kan er één bestand per programmeertaal zijn, waarbij de gemeenschappelijke delen in een apart bestand komen.
\end{enumerate}

\subsection{Beperkte expressies in het testplan}\label{subsec:beperkte-expressies-in-het-testplan}

De expressies in het testplan zijn opzettelijk eenvoudig gehouden (zie \cref{subsec:expressions-and-statements}).
Dit levert wel beperkingen op: zo is het niet mogelijk om bijvoorbeeld een functieoproep als \mintinline{python}{test(5 + 5)} in het testplan te schrijven.
Een idee zou dus kunnen zijn om het testplan uit te breiden met meer taalconstructies, waarbij het testplan dan dienst doet als een soort \acronym{AST} (abstract syntax tree).
Afhankelijk van hoe ver men hierin gaat, begint dit wel een universele programmeertaal te worden (wat ook expliciet buiten het bestek van deze thesis valt).
In dat geval loont het de moeite om te onderzoeken of geen eenvoudige, bestaande programmeertaal \english{transpiled} zou kunnen worden naar het testplan, in plaats van zelf een nieuwe taal op te stellen.
Hiervoor moet het testplan wel uitgebreid worden met functies van een \acronym{AST} of \acronym{IR} (intermediary language).
Ook TESTed zelf zal uitgebreid moeten worden, aangezien er meer zal moeten omgezet kunnen worden naar de programmeertalen (de sjablonen zullen dus uitgebreid moeten worden).

\subsection{Programmeertaalonafhankelijke programmeerparadigma}\label{subsec:programmeertaalonafhankelijke-programmeerparadigma}

TESTed vertaalt geen programmeerparadigma tussen verschillende programmeertalen.
Dit kan ervoor zorgen dat bepaalde oefeningen in sommige programmeertalen op onnatuurlijke wijze opgelost moeten worden.
Stel als voorbeeld de \acronym{ISBN}-oefeningen.
In Python is het \english{pythonic} om hiervoor twee top-level functies te schrijven (\texttt{is\_isbn} en \texttt{are\_isbn})
In Java zal TESTed dan twee statische functies verwachten, terwijl deze opgave in de Java-wereld ook vaak opgelost zal worden met bijvoorbeeld een klasse \texttt{IsbnValidator}, met twee methoden \texttt{check} en \texttt{checkAll}.

Er zijn meerdere denkpistes om hier een oplossing voor te bieden:

\begin{itemize}
    \item Voorzie binnen TESTed ook vertalingen van programmeerparadigma.
    Zo zou voor een oefening opgegeven kunnen worden of de Java-oplossing een klasse of statische methodes moet gebruiken.
    \item Maak een systeem met hybride oefeningen, waarbij de invoer programmeertaalafhankelijk is, terwijl de evaluatie van resultaten programmeertaalonafhankelijk blijft.
    In het voorbeeld hierboven zou een lesgever dan voor elke programmeertaal opgeven hoe een resultaat bekomen moet worden (in Python twee functieoproepen, in Java een instantie van een klasse maken en twee methoden oproepen), waarna TESTed overneemt om een programmeertaalonafhankelijke evaluatie te doen van de resultaten.
\end{itemize}

Het omgekeerde, programmeertaalonafhankelijk invoer en programmeertaalspecifieke beoordeling, bestaat al binnen TESTed (zie \cref{subsec:programmeertaalspecifieke-evaluatie}).
Als tot slot zowel de invoer als de uitvoer verschilt van programmeertaal tot programmeertaal, kan er niet meer gesproken worden van een programmeertaalonafhankelijke oefeningen.
In dat geval is het beter een bestaande programmeertaalspecifieke judge van Dodona te gebruiken.

\subsection{Kleinere functionaliteit}\label{subsec:kleinere-functies}

Er zijn in de loop van de thesis verschillende kleinere features naar boven gekomen, waarvoor ook ondersteuning zou toegevoegd kunnen worden:

\begin{itemize}
    \item Ondersteuning voor Python Tutor.
    Dit is een visuele debugger, origineel geschreven voor Python \autocite{10.1145/2445196.2445368}.
    Ondertussen worden ook andere talen ondersteund, zoals Java, C++, JavaScript en Ruby.
    In Dodona ondersteunt momenteel enkel de Python-judge de Python Tutor.
    Het zou een mooie toevoeging zijn indien dit bij TESTed voor meerdere programmeertalen kan toegevoegd worden.
    \item Interpreteren van output van de compiler.
    Momenteel ondersteunt TESTed linting (zie \cref{sec:andere-taken}), maar bij sommige talen worden ook veel nuttige waarschuwingen getoond bij het compileren van de code.
    Deze uitvoeren wordt momenteel getoond in TESTed, en er is ondersteuning om deze uitvoer om te zetten in annotaties op de code van de oplossing.
    Dit laatste is momenteel geïmplementeerd voor de compilatiestap in Python.
    De ondersteuning hiervoor zou uitgebreid kunnen worden naar andere talen en flexibeler gemaakt worden.
    Zo wordt de uitvoer van de compiler altijd getoond, ook al is die uitvoer omgezet naar annotaties (dezelfde informatie wordt dus dubbel getoond).
    \item Foutboodschappen beter afhandelen.
    Als momenteel een fout optreed wordt de stacktrace zonder verdere verwerking getoond aan de gebruiker.
    Het is echter nuttig hier een verwerkingsstap toe te voegen, zodat:
    \begin{itemize}
        \item De code gegenereerd door TESTed weggefilterd kan worden.
        Het kan verwarrend zijn voor studenten als er in de stacktrace code staat zij niet ingediend hebben.
        \item Het aanbrengen van links naar specifieke plaatsen in de ingediende code.
        Als er bijvoorbeeld in de foutboodschap staat "fout op regel 5, kolom 10", dan zou een snelkoppeling naar die plaats in de code nuttig zijn.
        Dit is ook iets onder andere door de Python-judge geïmplementeerd is.
    \end{itemize}
    \item Lange uitvoer beperkt tonen.
    TESTed bevat momenteel een zeer eenvoudige limiet: de uitvoer wordt beperkt tot 20 regels en de regellengte is beperkt tot 150 tekens.
    Deze limiet houdt echter geen rekening met de aard van de uitvoer (bijvoorbeeld een boodschap van TESTed of de uitvoer van de compiler).
    Zo zou een stacktrace anders ingekort kunnen worden.
    \item Waarden mooier weergeven (\english{pretty printing}).
    Enerzijds gaat het om grote verzamelingen verkort weer te geven.
    Is de uitvoer bijvoorbeeld een lijst van duizend elementen, zouden enkel de eerste tien getoond kunnen worden.
    Momenteel worden alle waarden verbatim aan de studenten getoond.
    Anderzijds gaat het ook om het beter weergeven van waarden om het vergelijken met de verwachte uitvoer eenvoudiger te maken.
    Een voorbeeld hier zijn verzamelingen: door de verzameling gesorteerd te tonen wordt het zoeken naar bijvoorbeeld ontbrekende waarden een stuk eenvoudiger.
    \item Tijdslimieten op het niveau van de contexten.
    Momenteel is het enkel mogelijk om op het niveau van een oefening een tijdslimiet in te stellen.
    De implementatie hiervan zou eenvoudig moeten zijn: intern werkt TESTed al met een tijdslimiet per context, namelijk de totale tijdslimiet van de oefening minus de verstreken tijd.
    \item Gerelateerd aan de tijdslimieten is binnen Dodona een status voor een test die niet uitgevoerd is\footnote{Zie de issue: \url{https://github.com/dodona-edu/dodona/issues/1785}}.
    Momenteel toont TESTed de niet-uitgevoerde testen met de status "fout".
    Het is echter wenselijk deze oefening als "niet-uitgevoerd" aan te duiden.
    Ook visueel zou er een verschil kunnen zijn, door bijvoorbeeld een grijze achtergrond te gebruiken in plaats van een rode achtergrond.
\end{itemize}