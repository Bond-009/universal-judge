\chapter{Conclusie, beperkingen en toekomstig werk}\label{ch:beperkingen-en-toekomstig-werk}

\lettrine{D}{e doelstelling} van deze thesis was het formuleren van een antwoord op de vraag of programmeertaalonafhankelijke oefeningen mogelijk zijn (zie \cref{sec:probleemstelling}).
Als antwoord hierop is een prototype, \tested{}, ontwikkeld.
Op basis van het prototype kunnen we concluderen dat het antwoord op de onderzoeksvraag positief is.

Eigen aan een prototype is dat het nog niet \english{production ready} is.
Dit rest van dit hoofdstuk handelt over ontbrekende functionaliteit of beperkingen van \tested{}, en reikt mogelijke ideeën voor oplossingen aan.

\section{Dodona-platform}\label{sec:dodona-platform}

Geen enkele bestaande judge in het Dodona-platform ondersteunt meerdere programmeertalen.
Dit heeft gevolgen voor hoe Dodona omgaat met programmeertalen, zoals hieronder duidelijk wordt.

\subsection{Programmeertaalkeuze in het Dodona-platform}\label{subsec:programmeertaalkeuze-in-het-dodona-platform}

Een eerste probleem is dat binnen Dodona de programmeertaal gekoppeld is aan een oefening en aan een judge, niet aan een oplossing.
Dit impliceert dat elke oplossing van een oefening in dezelfde programmeertaal geschreven is.
Een aanname die niet meer geldt bij \tested{}.
Een uitbreiding van Dodona bestaat er dus in om ondersteuning toe te voegen voor het kiezen van een programmeertaal per oplossing.
Dit moet ook in de gebruikersinterface mogelijk zijn, zodat studenten kunnen kiezen in welke programmeertaal ze een oefening oplossen.

Als \english{workaround} zijn er twee suboptimale opties:
\begin{itemize}
    \item Dezelfde oefening in meerdere programmeertalen aanbieden, door bijvoorbeeld met symlinks dezelfde opgave, testplan en andere bestanden te gebruiken, maar met een ander config-bestand.
    Op deze manier zullen er in Dodona meerdere oefeningen zijn.
    Deze aanpak heeft nadelen.
    Zo zijn de oefeningen effectief andere oefeningen vanuit het perspectief van Dodona.
    Dient een student bijvoorbeeld een oplossing in voor één van de oefeningen, worden de overige oefeningen niet automatisch als ingediend gemarkeerd.
    Ook is de manier waarop de opgave, het testplan, enzovoort gedeeld worden (de symlinks) niet echt handig.
    \item \tested{} zelf de programmeertaal laten afleiden op basis van de ingediende oplossing.
    Hiervoor zijn verschillende implementaties mogelijk, zoals een heuristiek die op basis van de ingediende broncode de programmeertaal voorspelt of de student die programmeertaal expliciet laten aanduiden.
\end{itemize}

Er is voorlopig gekozen om de tweede manier te implementeren: \tested{} ondersteunt een speciale \term{shebang}\footnote{Zie \url{https://en.wikipedia.org/wiki/Shebang_(Unix)}.} die aangeeft in welke programmeertaal de ingediende oplossing beoordeeld moet worden.
De eerste regel van de ingediende oplossing kan er als volgt uitzien:

%! suppress = EscapeHashOutsideCommand
\begin{minted}{bash}
    #!tested [programmeertaal]
\end{minted}

Hierbij wordt \texttt{[programmeertaal]} vervangen door de naam van de programmeertaal, zoals \texttt{java} of \texttt{python}.
Dit is een voorlopige oplossing die toelaat om oefeningen in meerdere programmeertalen aan te bieden zonder dat Dodona moet aangepast worden.
Een nadeel aan deze oplossing is dat de student niet kan zien in welke programmeertalen ingediend kan worden;
dit moet door de lesgever aangeduid worden.

\subsection{Detectie ondersteunde programmeertalen}\label{subsec:detectie-beschikbare-programmeertalen}

Een ander gevolg van de koppeling van de programmeertaal aan een oefening binnen Dodona is dat Dodona momenteel altijd weet voor welke programmeertaal een oplossing wordt ingediend: de programmeertaal van de oefening.
Het is echter niet voldoende dat de programmeertaal van de oplossing gekend is bij het indienen,
het is ook nuttig te weten in welke programmeertalen een oefening gemaakt kan worden.
Dit om enerzijds dingen als syntaxiskleuring te kunnen toepassen, maar ook om ervoor te zorgen dat een student geen nutteloos werk verricht en een oplossing maakt in een niet-ondersteunde programmeertaal.

Zoals besproken in \cref{subsec:vereiste-functies}, controleert \tested{} of de programmeertaal van de ingediende oplossing ondersteund wordt door het testplan van de oefening.
Die controle komt echter te laat, namelijk bij het beoordelen van een oplossing.
Aan de andere kant is de controle onafhankelijk van de oplossing: enkel het testplan is nodig.
Een beter ogenblik om de ondersteunde programmeertalen van een oefening te bepalen, is bij het importeren van de oefening in het Dodona-platform.
Bij het importeren gebeurt al een verwerking van de oefening door Dodona, om bijvoorbeeld de configuratiebestanden te lezen en de oefening in de databank op te slaan.
Deze verwerking zou kunnen uitgebreid worden om ook de ondersteunde programmeertalen te bepalen.
Merk op dat deze controle opnieuw moet uitgevoerd worden elke keer dat een nieuwe programmeertaal aan \tested{} toegevoegd wordt.
Dat deze controle steeds opnieuw moet gebeuren is misschien gewenst: zo is meteen duidelijk welke oefeningen in de nieuwe programmeertaal opgelost kunnen worden.

De volgende vraag die zich stelt is hoe dit praktisch moet aangepakt worden.
Het bepalen van de ondersteunde programmeertalen is iets specifieks voor \tested{}, terwijl judge-specifieke code zoveel mogelijk buiten Dodona wordt gehouden.
Een mogelijke oplossing is de verwerking van de oefeningen uit te breiden, zodat de judge ook een verwerking kan doen van de oefening.
Deze aanpak heeft als voordeel dat het een generieke oplossing is: alle judges krijgen de mogelijkheid tot het verwerken van de oefening.

Een andere variant van deze oplossing wordt overwogen door het Dodona-team\footnote{Zie \url{https://github.com/dodona-edu/dodona/issues/1754}}.
Het voorstel is om oefeningen en judges te combineren in Dodona tot één Docker-container.
Interessant voor \tested{} is het toevoegen van een \texttt{prepare}-stap aan de judge (momenteel heeft een judge slechts één stap, namelijk \texttt{run}, het beoordelen).
Deze voorbereidende stap resulteert dan in een Docker-container op maat van de oefening.
Het is in deze voorbereidende stap dat het detecteren van de ondersteunde programmeertalen zou kunnen plaatsvinden.

Een andere oplossing voor dit probleem is de ondersteunde programmeertalen manueel opgeven in de configuratiebestanden van de oefening.
Hier is dan weer het voordeel dat de verwerkingsstap van Dodona niet uitgebreider en trager moet worden.
Aan de andere kant mag de auteur van de oefening dan niet vergeten om de programmeertalen in de configuratie op te nemen.
Eventueel kan dit gemakkelijker gemaakt worden door een script te voorzien dat de ondersteunde programmeertalen detecteert en het resultaat in de configuratiebestanden schrijft.
In de twee eerdere voorstellen wordt telkens gesproken over een verwerkingsstap, waarin het detecteren plaatsvindt.
Bij deze laatste oplossing verschuift de verantwoordelijkheid voor het uitvoeren van deze stap van Dodona naar de auteur van de oefening.
Als er een nieuwe programmeertaal aan \tested{} wordt toegevoegd, dan verwacht deze oplossing ook dat de auteur voor elke oefening het configuratiebestand aanpast (indien de nieuwe oefening beschikbaar moet zijn in de nieuwe programmeertaal).

\subsection{Beperken van ondersteunde programmeertalen}\label{subsec:beperken-beschikbare-programmeertalen}

De vorige paragraaf heeft het steeds over het automatisch detecteren van de ondersteunde programmeertalen.
Om twee redenen kan het echter nuttig zijn om deze automatisch detectie te overschrijven.

Ten eerste is de automatische detectie niet altijd voldoende streng, wat ertoe leidt dat \tested{} een programmeertaal als ondersteund beschouwd, terwijl de oefening niet oplosbaar is in die programmeertaal.
Een concreet voorbeeld hiervan is de detectie van functieargumenten met heterogene gegevenstypes (zie \cref{subsec:vereiste-functies} voor een voorbeeld).
De automatische detectie houdt enkel rekening met \english{literal} functieargumenten.
De gegevenstypes van functieoproepen en identifiers worden niet gecontroleerd, aangezien deze informatie niet eenvoudig af te leiden is uit het testplan.
In dit eerste geval is het dus nuttig dat de auteur van de oefening op het niveau van een oefening een \term{blacklist} of zwarte lijst van programmeertalen kan meegeven: programmeertalen waarin de oefening niet kan gemaakt worden.
Een belangrijk detail is dat als er nieuwe programmeertalen bijkomen in \tested{}, de oefening standaard wel ondersteund zou worden in die nieuwe programmeertaal.

De andere grote reden is dat lesgevers de programmeertalen waarin een oefening opgelost kan worden willen beperken.
Dit kan bijvoorbeeld zijn omdat de lesgever slechts programmeertalen wil toelaten die hij machtig is, of dat de oefening gebruikt wordt in een vak waar één bepaalde programmeertaal onderwezen wordt.
Binnen Dodona stelt een lesgever binnen een vak reeksen oefeningen op.
Die reeksen kunnen bestaan uit eigen, zelfgeschreven oefeningen, maar kunnen ook bestaan uit oefeningen die beschikbaar zijn in het Dodona-platform.
De auteur van de oefening en lesgever zijn dus niet altijd dezelfde persoon.
Om die reden is het nuttig als lesgevers op het niveau van het vak, de reeks of de oefening een \term{whitelist} of witte lijst van programmeertalen zouden kunnen opgeven: oefeningen in dat vak of die reeksen zullen enkel in de toegelaten programmeertalen kunnen opgelost worden.
Bij deze beperking worden nieuwe programmeertalen niet automatisch ondersteund.
Het zou ook nuttig zijn om ook hier met een zwarte lijst te kunnen werken, waarbij nieuwe talen wel automatisch ondersteund worden.

\section{Testplan}\label{sec:beperkingen-testplan}

De gekozen aanpak voor het testplan heeft bepaalde beperkingen (zie \cref{subsec:het-testplan} voor de gemaakte keuze).
Deze paragraaf bespreekt drie grote beperkingen en mogelijke oplossingen.

\subsection{Handgeschreven testplannen zijn omslachtig}\label{subsec:handgeschreven-testplannen-zijn-omslachtig}

In \tested{} is gekozen om het testplan in \acronym{JSON} op te stellen (zie \cref{subsec:het-testplan}).
Bovendien is het een ontwerpkeuze geweest om zoveel mogelijk informatie expliciet te maken.
Dit heeft als voornaamste voordeel dat de implementatie in \tested{} eenvoudiger blijft.
Anderzijds zorgt dit wel voor redelijk wat herhaalde informatie in het testplan.

De combinatie van \acronym{JSON} met de herhaalde informatie zorgt ervoor dat een testplan vaak lang is.
Te lang om nog met de hand te schrijven.
Een oplossing hiervoor, die al gebruikt wordt, is om het testplan niet met de hand te schrijven, maar te laten generen door bijvoorbeeld een Python-script.

Een andere oplossing is het toevoegen van een bijkomende stap voor het testplan: het testplan wordt dan in een ander formaat geschreven, en bijvoorbeeld tijdens het importeren in Dodona omgezet naar het \acronym{JSON}-bestand.
Voor dat andere formaat kan een \acronym{DSL} (\english{domain-specific language}) nuttig zijn.
Dit is een programmeertaal die specifiek ontworpen is voor een bepaald onderwerp of doel.
Hier zou ze specifiek ontworpen zijn om een testplan te schrijven.

Een bijkomend voordeel van \acronym{JSON} als formaat is dat het een neutraal formaat is.
Verschillende soorten oefeningen hebben andere noden op het vlak van schrijven van het testplan.
Het testplan voor een oefening die enkel \texttt{stdin} en \texttt{stdout} gebruikt zal er helemaal anders uitzien dan een testplan voor een oefening waar functieoproepen gebruikt worden.
Het zou bijvoorbeeld kunnen dat voor verschillende soorten oefeningen een andere \acronym{DSL} ontworpen wordt.

Zo zijn er bijvoorbeeld meerdere standaarden voor het schrijven van programmeeroefeningen.
Een programma dat oefeningen, die in deze formaten beschreven zijn, omzet naar het eigen formaat van \tested{} kan een nuttige uitbreiding zijn.
Op het moment van schrijven lijken er echter nog geen wijdverspreide standaarden te zijn.
Bij wijze van voorbeeld worden twee standaarden besproken, telkens met een beschrijving van dezelfde oefening: de \emph{Palindroom}-oefening.
Bij deze voorbeeldoefening moet invoer gelezen worden en dan als uitvoer moet aangegeven worden of de invoer al dan niet een palindroom is.

Een eerste standaard is \acronym{PExIL} (\term{Programming Exercises Interoperability Language}) \autocite{queiros2011pexil}.
Dit is een op \acronym{XML}-gebaseerd formaat, gepubliceerd in 2011.
Ontwikkeling van deze standaard lijkt gestopt te zijn.
We hebben geen online oefeningen gevonden die in dit formaat beschreven zijn en de nodige resources voor het zelf schrijven van een oefeningen (zoals het \acronym{XML} Schema) zijn niet meer online beschikbaar.
De beschrijving van de \emph{Palindroom}-oefening staat in \cref{lst:pexil}.

\begin{listing}
    \inputminted{xml}{code/pexil-in.xml}
    \inputminted{xml}{code/pexil-out.xml}
    \caption{Beschrijving van de oefening "palindroom" (gesplitst in invoer en uitvoer) in \acronym{PExIL}.}
    \label{lst:pexil}
\end{listing}

Een tweede standaard is \acronym{PEML} (\english{Programming Exercise Markup Language}) \autocite{peml}.
P\acronym{EML} is een tekstueel formaat (een soort \acronym{DSL}) dat er veelbelovend uitziet om met de hand te schrijven.
Op het moment van schrijven van deze thesis is \acronym{PEML} nog in volle ontwikkeling.
Aangezien een van de \english{design goals} van \acronym{PEML} is dat de omzetting van \acronym{PEML} naar \acronym{JSON} eenvoudig zal zijn, lijkt dit een interessant formaat.
De beschrijving van de \emph{Palindroom}-oefening staat in \cref{lst:peml}.
Dit voorbeeld is overgenomen uit de documentatie van \acronym{PEML}.

\begin{listing}
    \inputminted{yaml}{code/peml.peml}
    \caption{
        Beschrijving van de oefening "palindroom" in \acronym{PEML}.
        Overgenomen uit \autocite{peml}.
    }
    \label{lst:peml}
\end{listing}


\subsection{Dynamische scheduling van testen}\label{subsec:dynamische-scheduling-van-testen}

Een ander nadeel van de keuze voor \acronym{JSON} als formaat voor het testplan, is dat de \term{scheduling} (het plannen van welke testcode wanneer uitgevoerd worden) statisch gebeurt.
Een analogie is bijvoorbeeld een gecompileerde taal zoals C/C++ en een dynamische taal zoals Python: \tested{} vervult in deze analogie de rol van compiler.
Door dit statische testplan is het niet mogelijk om in het algemeen op basis van het resultaat van een vorig testgeval het verloop van de volgende testgevallen te bepalen.

Het statisch testplan \emph{an sich} is echter voor specifieke gevallen geen probleem.
Het testplan kan uitgebreid worden met verschillende mogelijkheden.
Zo is reeds voorzien in het testplan om een testgeval als essentieel aan te duiden: faalt het testgeval, dan zullen volgende testgevallen niet meer uitgevoerd worden.
Dit zou uitgebreid kunnen worden zodat het ook op het niveau van contexten werkt.

Dit systeem kan verder uitgebreid worden door aan elke context een unieke identificatiecode toe te kennen.
Daarna kan voor elke context een preconditie gegeven worden: welke contexten moet geslaagd zijn om de huidige context uit te voeren.
Zijn niet alle contexten uit de preconditie geslaagd, dan wordt de context niet uitgevoerd.
Het testplan kan bevatten dat context 15 enkel uitgevoerd kan worden indien contexten 1, 7 en 14 ook succesvol waren.

\subsection{Herhaalde uitvoeringen van testgevallen}\label{subsec:herhaalde-uitvoeringen-van-testgevallen}

Een andere, maar aan het dynamisch uitvoeren verwante, beperking is het ontbreken van de mogelijkheid om contexten of testgevallen meerdere keren uit te voeren.
Dit herhaald uitvoeren is nuttig bij niet-deterministische oefeningen.
Een oefening waarbij waarden bijvoorbeeld uit een normale verdeling getrokken moeten worden, zal een duizendtal keren uitgevoerd moeten worden vooraleer met voldoende zekerheid kan geconstateerd worden of er effectief een normale verdeling gebruikt wordt of niet.

Dit is ook op te lossen met een uitbreiding van het testplan: per testgeval of per context zou kunnen aangeduid worden hoeveel keer deze moet uitgevoerd worden.
Om de performantie te verbeteren, zouden ook stopvoorwaarden kunnen meegegeven worden.
Een aspect dat dit complexer maakt, is dat de evaluatoren (zowel de ingebouwde evaluatie binnen \tested{} als de geprogrammeerde en programmeertaalspecifieke evaluatie) aangepast zullen moeten worden om niet enkel met één resultaat, maar met een reeks resultaten te werken.
Dit vermijden is mogelijk door de bestaande evaluatoren niet te wijzigen en ze per uitvoering op te roepen, zoals nu gebeurt.
We kunnen dan een finale, globale evaluator toevoegen die de resultaten in hun geheel beoordeelt.
Dit geeft ook een mogelijkheid voor optimalisatie: als een evaluatie van een individuele uitvoering faalt, kan het uitvoeren al gestop worden.
Stel bijvoorbeeld de \emph{Lotto}-oefening.
Als bij de eerste uitvoering al blijkt dat de returnwaarde (een lijst) niet voldoende getallen bevat, kan de evaluatie al stoppen.

\subsection{Beperkingen van de statements en expressies in het testplan}\label{subsec:beperkte-expressies-in-het-testplan}

De statements en expressies in het testplan zijn opzettelijk eenvoudig gehouden (zie \cref{subsec:expressions-and-statements}).
Dit levert wel beperkingen op: zo is het niet mogelijk om bijvoorbeeld een functieoproep als \mintinline{python}{test(5 + 5)} in het testplan te schrijven.
Een idee zou dus kunnen zijn om het testplan uit te breiden met meer taalconstructies, waarbij het testplan dan dienst doet als een soort \acronym{AST} (abstract syntax tree).
Afhankelijk van hoe ver men we hierin willen gaan, begint de vorm van een universele programmeertaal aan te nemen (wat ook expliciet buiten het bestek van deze thesis valt).
In dat geval loont het de moeite om te onderzoeken of geen eenvoudige, bestaande programmeertaal \english{transpiled} zou kunnen worden naar het testplan, in plaats van zelf een nieuwe taal op te stellen.
Hiervoor moet het testplan wel uitgebreid worden met functies van een \acronym{AST} of \acronym{IR} (intermediary language).
Ook \tested{} zelf zal uitgebreid moeten worden, aangezien er meer zal moeten omgezet kunnen worden naar de programmeertalen (de sjablonen zullen dus uitgebreid moeten worden).
We kunnen nog verder gaan door een transpiler en/of compiler in \tested{} zelf te integreren.
In dat geval bevat het testplan geen \acronym{AST} of \acronym{IR}, maar de programmeertaal (in tekstuele vorm).

\subsection{Ondersteuning voor meerdere natuurlijke talen}\label{subsec:ondersteuning-voor-natuurlijke-talen}

Sommige judges voor specifieke programmeertalen in Dodona hebben ondersteuning voor het vertalen op het gebied van natuurlijke talen, zoals de vertaling van bijvoorbeeld namen van functies of variabelen.
De vertaling van de beschrijving van de opgave is dan weer geïmplementeerd in Dodona zelf door een tweede bestandsextensie.
Zo zullen \texttt{description.nl.md} en \texttt{description.en.md} gebruikt worden voor respectievelijk Nederlands en Engels.

Een eenvoudige oplossing voor het vertalen in de code is iets gelijkaardigs doen met het testplan: \texttt{testplan.nl.json} en \texttt{testplan.en.json} voor respectievelijk Nederlands en Engels.
Deze aanpak heeft wel als nadeel dat veel zaken in het testplan onnodig zullen gedupliceerd moeten worden;
een waarde \texttt{5} is niet taalafhankelijk.

Een idee dat dit zou kunnen voorkomen is om binnen eenzelfde testplan ondersteuning te bieden voor vertalingen, bijvoorbeeld door telkens meerdere talen te aanvaarden.
Momenteel ziet een functienaam er bijvoorbeeld als volgt uit:

\inputminted{json}{code/example-name.json}

Ondersteuning voor vertaling kan dan deze vorm aannemen:

\inputminted{json}{code/example-name-trans.json}

Als een functie binnen een testplan veel wordt opgeroepen, dan zorgt dit nog steeds voor veel herhaling.
Dit kan opgelost worden door een \acronym{DSL}, maar dat neemt niet weg dat het testplan onnodig lang zal worden.
Een mogelijke oplossing hiervoor is om een soort vertaalwoordenboek mee te geven in het testplan, waar met sleutelwoorden gewerkt wordt.
Een voorbeeld illustreert dit:

\inputminted{json}{code/example-name-dict.json}

De functieoproep gebruikt dan een sleutelwoord uit het vertaalwoordenboek, waarna \tested{} bij het uitvoeren de juiste naam gebruikt:

\inputminted{json}{code/example-name-key.json}

Ook de foutboodschappen die \tested{} zelf genereert zijn bijvoorbeeld nog niet vertaald: de boodschappen met studenten als doelpubliek zijn in het Nederlands, terwijl de boodschappen voor cursusbeheerders in het Engels zijn.
Dit is echter eenvoudig op te lossen: \tested{} krijgt de natuurlijke taal mee van Dodona in de configuratie, en Python heeft meerdere internationalisatie-API's\footnote{Zie \url{https://docs.python.org/3/library/gettext.html}}.
Het is waarschijnlijk ook nodig om de evaluatoren uit te breiden zodat zij ook de taal als een argument meekrijgen, zodat zij hun boodschappen ook kunnen vertalen.

\section{Performantie}\label{sec:future-performantie}

Ook op het gebied van performantie zijn er nog mogelijkheden tot verbetering.

\subsection{Uitvoeren van contexten}\label{subsec:future-performance}

Het uitvoeren van elke context in een afzonderlijk subprocess brengt een inherente performantiekost met zich mee.
Een mogelijke uitbreiding is het toelaten om meerdere contexten in hetzelfde proces en na elkaar uit te voeren, waardoor de totale tijd die nodig is om een oplossing te beoordelen kan verminderd worden, of minstens onderzoeken of wat het effect hiervan is.
De meeste infrastructuur om dit te doen bestaat al: de selector voert bijvoorbeeld een context uit op basis van een argument.
Er zou een speciaal argument \texttt{all} kunnen voorzien worden, waarmee alle contexten zullen uitgevoerd worden.
Mogelijke problemen of zaken die nog moeten opgelost worden:

\begin{itemize}
    \item Programmeertalen waar geen selector nodig is, zoals Python, zullen dan toch een selector nodig hebben.
    \item Bij oefeningen waarbij bestanden aangemaakt moeten worden, mogen die bestanden elkaar niet overschrijven.
    \item Geprogrammeerde evaluaties zullen pas na het uitvoeren van alle contexten kunnen gebeuren.
    Ook de resultaten van contexten zullen pas erna verwerkt kunnen worden.
    Dit omdat de geprogrammeerde evaluatie niet noodzakelijk in dezelfde programmeertaal als de oefening gebeurt en dus via \tested{} moet gaan.
    Momenteel voert \tested{} de testcode uit, wacht op het resultaat en voert vervolgens de evaluatiecode uit (per context).
    Het uitvoeren van alle contexten in een keer verandert niets aan de volgorde waarin deze stappen gebeuren.
    Dit wijzigen zal een ingrijpende verandering aan \tested{} zijn.
\end{itemize}

Ook blijven alle redenen waarom contexten afzonderlijk uitgevoerd worden gelden, zoals het feit dat studenten dan bijvoorbeeld met statische variabelen gegevens kunnen delen tussen de contexten.
We zouden dan ook enkel aanraden om deze modus te gebruiken bij eenvoudige oefeningen.

Hetzelfde effect kan nu al bereikt worden door alle testgevallen in het testplan in dezelfde context onder te brengen.
Het "probleem" hierbij is dat dat dan ook zo aan de studenten getoond wordt in Dodona.
Een alternatieve oplossing bestaat er dan uit om toe te laten dat \tested{} aan Dodona doorgeeft dat elk testgeval in een eigen context uitgevoerd werd, terwijl dat niet zo is.
Een bedenking hier is dat dit een verschil introduceert tussen de structuur van het testplan en de structuur van de uitvoer die Dodona toont, net iets dat we proberen te vermijden.

\subsection{Geprogrammeerde evaluatie}\label{subsec:geprogrammeerde-evaluatie-is-traag}

Het uitvoeren van een geprogrammeerde evaluatie (zie \cref{subsec:geprogrammeerde-evaluatie}) zorgt voor een niet te verwaarlozen kost op het vlak van performantie (zie \cref{tab:meting} voor enkele tijdsmetingen).
De reden hiervoor is eenvoudig te verklaren: zoals de contexten wordt elke geprogrammeerde evaluatie in een afzonderlijk subproces uitgevoerd (afhankelijk van de programmeertaal ook met eigen compilatiestap).
Zoals vermeld in \cref{sec:performantie} is \tested{} oorspronkelijk gestart met het uitvoeren van code in Jupyter-kernels.
De grootste reden dat we daar vanaf gestapt zijn, is dat de kost voor het opnieuw opstarten van een kernel te groot is, en het heropstarten noodzakelijk is om de onafhankelijkheid van de contexten te garanderen.
Bij $n$ contexten moet de kernel dus $n$ keer opnieuw gestart worden.

Bij een geprogrammeerde evaluatie wordt geen code van de student uitgevoerd: het opnieuw starten van de kernel is dus niet nodig.
Hierdoor wordt de opstartkost van de kernel verspreid over alle contexten die van een programmeerde evaluatie in dezelfde programmeertaal gebruikmaken.
Wordt er $n$ keer andere evaluatiecode in een andere programmeertaal gebruikt, dan zal dit uiteraard geen voordeel opleveren, omdat er dan nog steeds $n$ keer een kernel gestart wordt.
In de meeste gevallen wordt echter dezelfde evaluatiecode gebruikt voor alle testgevallen.
Het \english{worst case scenario} is dan weliswaar trager, maar in de meeste gevallen zal de geprogrammeerde evaluatie sneller zijn.

Hierop is één uitzondering: een geprogrammeerde evaluatie waarbij de programmeertaal van de evaluatiecode in Python geschreven is.
Voor Python is er speciale ondersteuning (vermits \tested{} zelf ook in Python geschreven is, wordt deze evaluatie ook rechtsreeks in \tested{} gedaan zonder subproces): hiervoor is het gebruik van Jupyter-kernels niet nuttig.

\subsection{Optimalisaties bestaande parallellisatie}\label{subsec:optimalisaties-bestaande-parallellisatie}

Zoals we vermeld hebben in \cref{subsec:parallelle-uitvoering-van-contexten}, zijn er nog een aantal beperkingen en opmerkingen bij de implementatie van het parallel uitvoeren van de contexten, waarvan de belangrijkste zijn:

\begin{itemize}
    \item De contexten per tabblad geparallelliseerd worden, terwijl tabbladen nog steeds sequentieel uitgevoerd worden.
    In het ergste geval, waarbij elk tabblad één context heeft, gebeurt niets in parallel.
    \item Enkel het uitvoeren gebeurt in parallel, niet de beoordeling van het resultaat van het uitvoeren.
    Een gevolg hiervan is dat geprogrammeerde evaluaties ook sequentieel uitgevoerd worden, terwijl dit ook in parallel zou kunnen gebeuren.
    De reden hiervoor is technisch van aard: de uitvoer van de beoordeling moet in volgorde naar Dodona gestuurd worden, maar intern stuurt \tested{} een resultaat meestal direct door nadat het beschikbaar is.
    De beoordelingen in parallel uitvoeren zou er dus voor zorgen dat de uitvoer door elkaar staat.
\end{itemize}

\section{TESTed}\label{sec:beperkingen-tested}

In deze paragraaf komen de beperkingen aan bod die ofwel enkel betrekking hebben op \tested{} zelf, ofwel betrekking hebben op meerdere onderdelen, bijvoorbeeld zowel \tested{} als het testplan.

\subsection{Programmeertaalonafhankelijke opgaven}\label{subsec:programmeertaalonafhankelijke-opgaven}

Bij veel oefeningen bevat de opgave een stuk code ter illustratie van de opgave.
Dit is expliciet buiten het bestek van deze thesis gehouden, maar deze voorbeelden zijn idealiter in de programmeertaal waarin de student wenst in te dienen.

Een idee is hier om de codevoorbeelden uit de opgave ook in het formaat van het testplan op te stellen.
\tested{} bevat namelijk alles om het testplan om te zetten naar concrete code.

Concreet gaat het om volgende zaken:
\begin{enumerate}
    \item De programmeertaalkeuze in het Dodona-platform moet uitgebreid worden zodat de opgave ook programmeertaalafhankelijk is en dus kan gewijzigd worden als de student een andere programmeertaal kiest.
    \item De uitleg bij een opgave (bijvoorbeeld omkadering of bijkomende informatie) is vaak programmeertaalonafhankelijk, terwijl de meer technische onderdelen programmeertaalafhankelijk zijn.
    Voorbeelden van dat laatste zijn datastructuren (\texttt{list}/\texttt{array}) of codevoorbeelden.
    Hiervoor zijn ook meerdere mogelijkheden: zo zou alles binnen eenzelfde bestand kunnen, of kan er één bestand per programmeertaal zijn, waarbij de gemeenschappelijke delen in een apart bestand komen.
\end{enumerate}

\subsection{Programmeertaalonafhankelijke programmeerparadigmata}\label{subsec:programmeertaalonafhankelijke-programmeerparadigma}

\tested{} vertaalt geen programmeerparadigmata tussen verschillende programmeertalen.
Dit kan ervoor zorgen dat bepaalde oefeningen in sommige programmeertalen op onnatuurlijke wijze opgelost moeten worden.
Stel als voorbeeld de \acronym{ISBN}-oefeningen.
In Python is het \english{pythonic} om hiervoor twee top-level functies te schrijven (\texttt{is\_isbn} en \texttt{are\_isbn}).
In Java zal \tested{} dan twee statische functies verwachten, terwijl deze opgave in de Java-wereld ook vaak opgelost zal worden met bijvoorbeeld een klasse \texttt{IsbnValidator}, met twee methoden \texttt{check} en \texttt{checkAll}.

Er zijn meerdere denkpistes om hier een oplossing voor te bieden:

\begin{itemize}
    \item Voorzie binnen \tested{} ook vertalingen van programmeerparadigmata.
    Zo zou voor een oefening opgegeven kunnen worden of de Java-oplossing een klasse of statische methodes moet gebruiken.
    \item Maak een systeem met hybride oefeningen, waarbij de invoer programmeertaalafhankelijk is, terwijl de evaluatie van resultaten programmeertaalonafhankelijk blijft.
    In het voorbeeld hierboven zou een lesgever dan voor elke programmeertaal opgeven hoe een resultaat bekomen moet worden (in Python twee functieoproepen, in Java een instantie van een klasse maken en twee methoden oproepen), waarna \tested{} overneemt om een programmeertaalonafhankelijke evaluatie te doen van de resultaten.
\end{itemize}

Het omgekeerde, programmeertaalonafhankelijk invoer en programmeertaalspecifieke beoordeling, bestaat al binnen \tested{} (zie \cref{subsec:programmeertaalspecifieke-evaluatie}).
Als tot slot zowel de invoer als de uitvoer verschilt van programmeertaal tot programmeertaal, kan er niet meer gesproken worden van een programmeertaalonafhankelijke oefeningen.
In dat geval is het beter een bestaande programmeertaalspecifieke judge van Dodona te gebruiken of voor elke programmeertaal een eigen testplan op te stellen.

\subsection{Flexibelere evaluatievormen}\label{subsec:combinaties-van-evaluatiemanieren}

Momenteel moet bij de evaluatie een keuze gemaakt worden: het is ofwel een ingebouwde evaluatie, ofwel een geprogrammeerde evaluatie, ofwel een programmeertaalspecifieke evaluatie.
In sommige scenario's is het echter wenselijk om een combinatie te kunnen gebruiken, vooral bij de geprogrammeerde evaluatie.
Een voorbeeld is het volgende: stel dat de oefening vereist dat een functie geïmplementeerd moet worden die een getal moet teruggeven.
Dit getal is het resultaat van een niet-deterministische berekening, dus is een geprogrammeerde evaluatie aangewezen.
Het zou dan nuttig zijn dat \tested{} al controleert of de door de oplossing geproduceerde waarde het juiste gegevenstype heeft.
In talen zoals Haskell of C is het niet eenvoudig om een evaluatiefunctie te schrijven waaraan arbitraire types kunnen meegegeven worden.

Een ander scenario waar dit nuttig kan zijn, is als er voorbereidend werk nodig is voor de evaluatie.
Stel dat een functie geïmplementeerd moet worden die een datum teruggeeft.
In veel programmeertalen is dit een object (denk aan \texttt{DateTime} in Java of \texttt{datetime.Date} in Python).
Het zou nuttig kunnen zijn als in de programmeertaalspecifieke evaluatie deze datum bijvoorbeeld omgezet wordt naar een string, waarna \tested{} de evaluatie kan overnemen.
Op deze manier moet er in elke programmeertaal minder evaluatiecode geschreven worden.

Hieraan gerelateerd is een nieuwe evaluatiemodus, die we een \term{orakelevaluatie} kunnen noemen.
In deze evaluatiemodus staat de waarde waarmee vergeleken moet worden niet in het testplan, maar wordt deze waarde berekend, door bijvoorbeeld de voorbeeldoplossing uit te voeren met dezelfde invoer als de ingediende oplossing.
Het voordeel van deze evaluatiemodus is dat de vergelijking nog steeds kan gebeuren door \tested{} en niet moet gebeuren in een geprogrammeerde evaluatie.
Vanuit dat oogpunt is de orakelevaluatie op te vatten als een combinatie van een geprogrammeerde evaluatie en een ingebouwde evaluatie, wat ons weer bij de vorige alinea's brengt.
De geprogrammeerde evaluatie zou dan de waarde berekenen, waarna \tested{} de evaluatie overneemt en de ingebouwde evaluatie uitvoert.
Dit kan nuttig zijn bij oefeningen die bijvoorbeeld iets berekenen op basis van de huidige datum.

Het samenvoegen van beide concepten leidt tot een evaluatiepijplijn, waar de evaluatie doorheen verschillende stadia gaat:

\begin{enumerate}
    \item Het verwachte resultaat kan geleverd worden door de orakelevaluatie.
    \item Een programmeertaalspecifieke evaluatie wordt uitgevoerd.
    \item Een geprogrammeerde evaluatie wordt uitgevoerd.
    \item De ingebouwde evaluatie van \tested{} wordt uitgevoerd.
\end{enumerate}

Elk van de stappen is optioneel, al moet minstens één stap uitgevoerd worden.
Is er bijvoorbeeld geen orakelevaluatie, dan wordt de waarde uit het testplan gebruikt.
Elke stap krijgt als argument dan een evaluatiepakket, dat bestaat uit onder andere het resultaat van de vorige evaluatie en de geproduceerde waarde.

\section{Extra functionaliteit}\label{sec:kleinere-functies}

Er is in de loop van de thesis extra functionaliteit naar boven gekomen waarvoor ook ondersteuning zou toegevoegd kunnen worden:

\begin{itemize}
    \item Ondersteuning voor de Python Tutor.
    Dit is een visuele debugger, die origineel geschreven is voor Python \autocite{10.1145/2445196.2445368}.
    Ondertussen worden ook andere talen ondersteund, zoals Java, C++, JavaScript en Ruby.
    In Dodona ondersteunt momenteel enkel de Python-judge de Python Tutor.
    Het zou een mooie toevoeging zijn indien dit bij \tested{} voor meerdere programmeertalen kan toegevoegd worden.
    \item Interpreteren van output van de compiler.
    Momenteel ondersteunt \tested{} linting (zie \cref{sec:andere-taken}), maar bij sommige talen worden ook veel nuttige waarschuwingen getoond bij het compileren van de code.
    Deze waarschuwingen worden momenteel getoond in \tested{}, en er is ondersteuning om deze waarschuwingen om te zetten naar annotaties op de code van de oplossing.
    Dit laatste is momenteel geïmplementeerd voor de compilatiestap in Python.
    De ondersteuning hiervoor zou uitgebreid kunnen worden naar andere talen en flexibeler gemaakt kunnen worden.
    Zo wordt de uitvoer van de compiler altijd getoond, ook al is die uitvoer omgezet naar annotaties (dezelfde informatie wordt dus dubbel getoond).
    \item Foutboodschappen beter afhandelen.
    Als momenteel een fout optreedt, wordt de stacktrace zonder verdere verwerking getoond aan de gebruiker.
    Het is echter nuttig hier een verwerkingsstap toe te voegen, zodat:
    \begin{itemize}
        \item De code gegenereerd door \tested{} weggefilterd kan worden.
        Het kan verwarrend zijn voor studenten als de stacktrace verwijst naar code die zij niet ingediend hebben.
        \item Het aanbrengen van links naar specifieke plaatsen in de ingediende code.
        Als er bijvoorbeeld in de foutboodschap staat "fout op regel 5, kolom 10", dan zou een snelkoppeling naar die plaats in de code nuttig zijn.
        Dodona biedt hier ondersteuning voor en dit wordt ook gebruikt in de Python-judge.
    \end{itemize}
    \item Lange uitvoer beperkt tonen.
    \tested{} bevat momenteel een eenvoudige limiet: de uitvoer wordt beperkt tot tienduizend tekens.
    Deze limiet houdt echter geen rekening met de aard van de uitvoer (bijvoorbeeld een boodschap van \tested{} of de uitvoer van de compiler).
    Zo zou een stacktrace anders ingekort kunnen worden.
    \item Datastructuren mooier weergeven (\english{pretty printing}).
    Enerzijds gaat het om grote datastructuren verkort weer te geven.
    Is de uitvoer bijvoorbeeld een lijst van duizend elementen, dan zouden enkel de eerste en laatste tien getoond kunnen worden.
    Momenteel worden alle waarden verbatim aan de studenten getoond.
    Anderzijds gaat het ook om het beter weergeven van waarden om het vergelijken met de verwachte uitvoer gemakkelijker te maken.
    Een voorbeeld is verzamelingen gesorteerd tonen, zodat het vinden van de verschillen eenvoudiger wordt.
    Ook het tonen van complexe datastructuren op meerdere regels is hier een voorbeeld van.
    \item Tijdslimieten op het niveau van de contexten.
    Momenteel is het enkel mogelijk om op het niveau van een oefening een tijdslimiet in te stellen.
    De implementatie hiervan zou eenvoudig moeten zijn: intern werkt \tested{} al met een tijdslimiet per context, namelijk de totale tijdslimiet van de oefening minus de verstreken tijd.
    \item Gerelateerd aan de tijdslimieten is er binnen Dodona nog geen status voor een test die niet uitgevoerd is\footnote{Zie de issue: \url{https://github.com/dodona-edu/dodona/issues/1785}}.
    Momenteel toont \tested{} de niet-uitgevoerde testen met de status "fout".
    Het is echter wenselijk deze oefening als "niet-uitgevoerd" aan te duiden.
    Ook visueel zou er een verschil kunnen zijn, door bijvoorbeeld een grijze achtergrond te gebruiken in plaats van een rode achtergrond.
\end{itemize}